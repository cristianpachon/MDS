\documentclass[10pt,compress]{beamer}
%\documentclass[10pt,compress,draft]{beamer}
\mode<presentation>
\usepackage{graphicx}
%\usepackage{beamerthemesplit}
\usepackage[ansinew]{inputenc}
%\usepackage[spanish]{babel}
\usepackage{amsmath,amssymb}
\usepackage{eqlist}
\usepackage{amsfonts} % Simbolos matematicos
\usepackage{amsthm} %Enunciados predefinidos y demostraciones
\usepackage{bbm}
\usepackage{enumerate}
\usepackage[dvips]{epsfig} %Incluir figuras .eps
\usepackage{epsf}%Incluir figuras .eps
\usepackage{eurosym}
\usepackage{subfigure}


\useoutertheme[footline=authortitle]{miniframes}
\useinnertheme{circles}

\input{slides_English.sty}

%\usetheme{Singapore}
%\usetheme{Berlin}
%\usetheme{Frankfurt}
%\usepackage{beamerthemeclassic}

%\graphicspath{ {../_Toulouse/Seminar/} }
\graphicspath{ 
{./Figures/}
%{../_images/}
%{../../practicas_y_scripts/}
%{../Summer_School_2014/_images/}
%{../Summer_School_2014/__2_Statistical_Issues/lasso_glmnet/}
}

%\beamersetuncovermixins{\opaqueness<1>{25}}{\opaqueness<2->{15}}

\usepackage{multirow}
\usepackage{animate}

\usepackage{chicago}%,e:/latex/psfrag
% option "harvard"
%\usepackage{natbib} % for using \bibliographystyle{plainnat} 
%\bibpunct{(}{)}{;}{a}{,}{,} % for writing "(year)" instead of "[year]"

\newcommand{\single}{\renewcommand{\baselinestretch}{1.2}\normalsize}
\newcommand{\double}{\renewcommand{\baselinestretch}{1.63}\normalsize}

\def\noframe#1{}
\newcommand{\mb}{\mbox}
\newcommand{\ts}{\thickspace}
\newcommand{\xq}{\overline{\mb{x}}}
\newcommand{\yq}{\overline{\mb{y}}}
\renewcommand{\baselinestretch}{1,2}

%%%%%%%%%%%%
\newcommand{\bea}{\begin{eqnarray*}}
\newcommand{\eea}{\end{eqnarray*}}
\newcommand{\btab}{\begin{tabular}}
\newcommand{\etab}{\end{tabular}}
\newcommand{\np}{\newpage}
\newcommand{\la}{\label}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\bay}{\begin{array}}
\newcommand{\eay}{\end{array}}
\newcommand{\bs}{\boldsymbol}
%\newcommand{\mb}{\boldsymbol}
\def\bX{\bs{X}}
\def\by{\bs{y}}

\def\vsp{\vspace{0.3cm}}
\def\vs{\vspace{.1cm}}
\def\red{\textcolor{red}}


\def\bco{\iffalse}
\def\cov{{\rm cov}}
\def\var{{\rm var}}
\def\corr{{\rm corr}}
\def\diag{{\rm diag}}
\def\trace{{\rm trace}}
\def\logit{\hbox{logit}}
\def\expit{\hbox{expit}}
\def\ci{\cite}
\def\cp{\citep}
\def\citep{\cite}
\def\eps{\varepsilon}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

%\bibpunct{(}{)}{;}{a}{}{,}

% Pedro: new commands
\def\E{\mathbb{E}\,}
%\def\E{E}

\def\R{\mathbb{R}}
\newcommand{\mT}{\mathcal{T}}
\newcommand{\mS}{\mathcal{S}}
\newcommand{\mQ}{\mathcal{Q}}


%%%%%%%%%%%%

\def\red#1{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
%\def\green#1{\textcolor[rgb]{0.00,1.00,0.00}{#1}}
\def\green#1{\textcolor[rgb]{0.20,0.70,0.10}{#1}}
\def\blue#1{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\def\grey#1{\textcolor[rgb]{0.80,0.80,0.80}{#1}}

\def\citet{\citeN}    
%\def\citeN{\cite}    
\def\F{\mathcal{F}}
\def\then{\Rightarrow}
\def\tiende{\longrightarrow}

\def\MSE{\mbox{MSE}}
\def\PMSE{\mbox{PMSE}}
\def\ISE{\mbox{ISE}}
\def\MISE{\mbox{MISE}}
\def\IMSE{\mbox{IMSE}}
\def\AMISE{\mbox{AMISE}}
\def\AMSE{\mbox{AMSE}}
\def\Sesgo{\mbox{Sesgo}}
\def\Var{\mbox{Var}}%given that we observe $p_i<p$ is

\def\IQR{\mbox{IQR}}
\def\LSCV{\mbox{LSCV}}
\def\CV{\mbox{CV}}
\def\GCV{\mbox{GCV}}
\def\PI{\mbox{PI}}
\def\ECMP{\mbox{ECMP}}
\def\test{\mbox{\scriptsize test}}
\def\val{\mbox{\scriptsize val}}

\def\diag{\mbox{Diag}}
\def\Trace{\mbox{Trace}}
\def\Traza{\mbox{Traza}}
\def\traza{\mbox{Traza}}
\def\tr{^{\mbox{\scriptsize T}}}

\def\comment#1{}
\def\comentario#1{}
\def\practice#1{\frame{\Large {\bf \green{Practice:}} \\ #1}}

%%%%%%%%%%
%    1   %
%%%%%%%%%%
\title[Regularized estimation of LM and GLM\qquad{} Advanced Statistical Models, MIRI  \qquad{} \insertframenumber %/54]
%/\inserttotalframenumber]
/\pageref{last_page}]
{\LARGE Regularized estimation of LM and GLM}
\author[Pedro Delicado]{{\large Pedro Delicado}
\\
{\small Departament d'Estadística i Investigació Operativa \\
Universitat Politècnica de Catalunya}
}
\date{\today}

\begin{document}

\frame{\titlepage}

%\section{}
\frame{\scriptsize{\tableofcontents} }
\AtBeginSubsection[]
{
  \begin{frame}<beamer>
    \frametitle{}
    \scriptsize{\tableofcontents[currentsection,currentsubsection]}
  \end{frame}
}

\AtBeginSection[]
{
  \begin{frame}<beamer>
   \frametitle{}
    \scriptsize{\tableofcontents[currentsection,currentsubsection]}
  \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\refers{
% Bibliografía utilizada para elaborar estas notas:\\
\ \\
Sections 3.4 and 4.4.4 in \citeN{HasTibFri:2009}\\
Chapters 1, 2 and 3, section 5.4 in \citeN{HasTibWai:2015}\\
Section 6.2 3 in \citeN{JamWitHasTib:2013}\\
\citeN{tibshirani:2011:lasso_retosp}\\
\citeN{glmnet_vignette_2014}
}
}


\section{Introduction}

\frame{\frametitle{Introduction}
\bi 
\item In the multiple linear regression model (with $n$ observations and $p$ predictors, $p$ possibly greater than $n$) we consider the penalized least squares coefficients estimator where the penalization is given by the 
$L_1$ or the $L_2$ norms of the estimator.

\item This procedures leads to \blue{ridge regression} ($L_2$ penalization) and to 
\blue{lasso} (least absolute shrinkage and selection operator) estimation ($L_1$ penalization).

%\item The presentation is based on the following references:  
%\bi
%\item \citeN{HasTibFri:2009} {\em The Elements of Statistical Learning}, Chapter 3 (and particularly Section 3.4).
%\item \citeN{HasTibWai:2015} {\em Statistical Learning with Sparsity}, Chapters 1 to 5.
%\item \citeN{tibshirani:2011:lasso_retosp}
%\item \citeN{glmnet_vignette_2014} {\em Glmnet vignette}.
%\ei 

\item In the pathway, we will learn:
\bi 
%\item Ridge regression.
\item Linear estimators of a regression function.
\item Effective number of parameters (or effective degrees of freedom) of a regression estimator.
\item Tuning parameters choice based on leave-one-out cross-validation, $k$-fold cross-validation or generalized cross-validation.
\item Efficient computation of leave-one-out cross-validation for linear estimators.
\ei 
\ei 
}

\subsection{Multiple linear regression model}

\frame{\frametitle{Multiple linear regression model}
\bi 
\item Consider that $n$ pairs $(\bx_i,y_i)$, $i=1,\ldots,n$ of data, $y_i\in \R$ and $\bx_i\in \R^p$, are observed from the \blue{multiple linear regression model} 
$$
y_i = \beta_0 + \sum_{j=1}^p x_{ij}  \beta_j + \varepsilon_i,
$$
where $\varepsilon_1,\ldots,\varepsilon_n$ are i.i.d. r.v. with zero mean and variance $\sigma^2$, and $\bbeta=(\beta_0,\ldots,\beta_p)\tr \in \R^{p+1}$ is a vector of unknown coefficients.

\item Fitting the model consists in providing estimators for $\bbeta$ and $\sigma^2$. 

\ei
}

\frame{\frametitle{Ordinary Least Squares (OLS)}
\bi 
\item Ordinary Least Squares (OLS) estimator:
$$
\hat{\bbeta}_{\mbox{\tiny OLS}} = 
\arg\min_{\bbeta} \sum_{i=1}^n 
\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}  \beta_j  \right)^2 
$$
\item In matrix notation: 
$\hat{\bbeta}_{\mbox{\tiny OLS}} = (\bX\tr \bX)^{-1} \bX\tr \by$.

\item $\hat{\bbeta}_{\mbox{\tiny OLS}}$ is an unbiased estimator of $\bbeta$.

\item The computation of $\hat{\bbeta}_{\mbox{\tiny OLS}}$ is numerically unstable 
when $\bX\tr \bX$ is close to be singular: 

\ei 
}

\frame{\frametitle{Multicolinearity and bad conditioned matrices}
\bi 
\item \blue{Condition number} of a symmetric matrix $\bA$:
$\kappa(\bA)=\sqrt{\frac{\gamma_{\max}}{\gamma_{\min}}}$, 
where $\gamma_{\max}$ and $\gamma_{\min}$ are, respectively, the largest and lowest eigenvalue absolute values of $\bA$.
\item $\bA$ is not invertible if and only if $\kappa(\bA)=\infty$.
\item A large value of $\kappa(\bA)$ (in practice, larger than 30), indicates that numerical problems may appear when inverting $\bA$. 
\item In these cases we say that \blue{$\bA$ is bad conditioned}. 
\item If $\bX\tr \bX$ is bad conditioned then the computation of $\hat{\bbeta}_{\mbox{\tiny OLS}}$ is numerically unstable.
\item A large condition number indicates that $\bX$ is close to be singular, that is, close that some columns of $\bX$ can be written as linear combinations of the other.
\item We talk about \blue{multicolinearity} between columns of $\bX$. 
\ei 
}

\frame{\frametitle{Regularized regression}
\bi 
\item Beyond numerical problems, $\hat{\bbeta}_{\mbox{\tiny OLS}}$ can not be computed when the rank of $\bX$ is lower than $p$ (an extreme case of multicolinearity).
\item This is the case when $p>n$ (or $p\gg n$, as it can happen in applications with large scale data).
\item In practical terms, what happens is that $\by$ can be written as a linear combination of the predictors using infinitely many coefficient vectors, for which the objective OLS objective function is equal to 0, its minimum. So there is no way to select {\em the best} among those coefficient vectors.
\item \blue{Shrinkage (or regularized) methods:} They add a penalty (depending on $\bbeta$) to the objective function in such a way that the new optimum is attained at a unique vector $\hat{\bbeta}$.
\item The unbiasedness of OLS estimation is lost, 
but the new estimators may have lower Mean Square Error (and they are numerically stable).
\ei 
}


\section{Ridge regression}
\frame{\frametitle{Ridge regression}
\bi 
\item The ridge coefficients minimize a penalized residual sum of squares:
$$
\hat{\bbeta}_{\mbox{\tiny ridge}} = 
\arg\min_{\bbeta} \left\{ 
\sum_{i=1}^n 
\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}  \beta_j  \right)^2 + \lambda \sum_{j=1}^p \beta_j^2 
\right\} 
$$
$$
=\arg\min_{\bbeta} \left\{ 
(\by - \bX \bbeta)\tr (\by - \bX \bbeta) + \lambda \|\bbeta\|_2^2,
\right\} 
$$
\item There is a closed form expression: 
$\hat{\bbeta}_{\mbox{\tiny ridge}} = 
 (\bX\tr \bX + \lambda \bI_p)^{-1} \bX\tr \by$.

\item Here $\lambda\ge 0$ is a complexity parameter that controls the amount of shrinkage:
the larger the value of $\lambda$, the greater the amount of shrinkage of $\bbeta$ toward zero.
\ei 
}

\frame{
	\bi 
	\item Alternative expression:
{\footnotesize 
	$$
	\hat{\bbeta}_{\mbox{\tiny ridge}} = 
	\arg\min_{\bbeta} \sum_{i=1}^n 
	\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}  \beta_j  \right)^2 
	$$
	$$
	\mbox{subject to } \sum_{j=1}^p \beta_j^2 \le t, 
	$$}
	for $t\ge 0$. 
	There is a one-to-one decreasing correspondence between parameters $\lambda\in [0,\infty)$ and 
	$t \in (0, \|\hat{\bbeta}_{\mbox{\tiny OLS}}\|^2]$.

\item Observe that changes in scale of the explanatory variables affect the constraint effects (or, equivalently, the effects of penalization term).

\item For this reason, from now on we assume that the predictor variables have zero mean and unit variance:
$$
\sum_{i=1}^n x_{ij}=0,
\frac{1}{n}\sum_{i=1}^n x_{ij}^2=1,\; \; j=1,\ldots,p.
$$
\item Moreover, the response variable is assumed to have zero mean ($\sum_{i=1}^n y_i=0$), that is, $\beta_0=0$.
	\ei 
}

\frame{\frametitle{Prostate cancer example}
	\bi 
	\item Goal: To examine the correlation between the level of 
	log of prostate-specific antigen (lpsa)  and a number of clinical measures in 97 men who were about to receive a radical prostatectomy. 
	
	\item The predictor variables are 
	\bi 
	\item log cancer volume (lcavol), 
	\item log prostate weight (lweight), 
	\item age, 
	\item log of the amount of benign prostatic hyperplasia (lbph), 
	\item seminal vesicle invasion (svi), 
	\item log of capsular penetration (lcp),
	\item Gleason score (gleason), and 
	\item percent of Gleason scores 4 or 5 (pgg45).
	\ei 
	\ei 
}

\frame{\frametitle{Prostate cancer example. Ridge regression}
	\begin{center}
		%\fbox{
		\includegraphics[scale=.55,viewport=50 190 400 520,clip]{ESL_page_65.pdf}
		%}
	\end{center}
	{\tiny Source: \citeN{HasTibFri:2009}}
}

\frame{{Explicit solution for the ridge regression}
	\bi 
	\item The ridge regression estimators are the solution of the penalized least squares problem
	$$
\min_{\bbeta\in \R^p} \left\{ 
\sum_{i=1}^n 
\left(y_i - \sum_{j=1}^p x_{ij}  \beta_j  \right)^2 + \lambda \sum_{j=1}^p \beta_j^2 
\right\} 
	$$
	 that can be expressed as 
	$$
	\min_{\bbeta\in \R^p} \Psi(\bbeta)=(\by- \bX\bbeta)\tr (\by-\bX\bbeta) + 
		\lambda \bbeta\tr \bbeta,
	$$
	that has an explicit solution, as we show now.

	\item Taking the gradient
	$$
	\nabla \Psi(\bbeta) = -2\bX\tr (\by-\bX \bbeta) + 2\lambda \bbeta,
	$$
	and solving in $\bbeta$ the equation $\nabla \Psi(\bbeta) = \bf{0}$, we obtain
	%we have that the optimal value of $\bbeta$ is
	$$
	\hat{\bbeta}=\left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \bX\tr \by.
	$$
	\ei
}

\frame{
	\bi 
\item Ridge regression estimator: $\hat{\bbeta}_{\mbox{\tiny ridge}}=\left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \bX\tr \by$.
	
	\item Therefore, for any $\bx\in \R^p$, the corresponding predicted value is
	$$
	\hat{y} = \bx\tr \hat{\bbeta}_{\mbox{\tiny ridge}} = \bx\tr \left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \bX\tr \by.
	$$
	\item The vector of fitted values is 
	$$
	\hat{\by}= \bX \left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \bX\tr \by = \bH_{\lambda} \by.
	$$
	\item Compare with the OLS solution: $\hat{\bbeta}_{\mbox{\tiny OLS}}=\left(\bX\tr \bX\right)^{-1} \bX\tr \by$.
	$$
	\hat{\by}_{\mbox{\tiny OLS}}= \bX \left(\bX\tr \bX \right)^{-1} \bX\tr \by = \bH \by,
	$$
	where $\bH=\bX \left(\bX\tr \bX \right)^{-1} \bX\tr$ is called the \blue{hat matrix}.
	\item $\lim_{\lambda\tiende 0} \hat{\bbeta}_{\mbox{\tiny ridge}}= \hat{\bbeta}_{\mbox{\tiny OLS}}$, 
	$\lim_{\lambda\tiende \infty} \hat{\bbeta}_{\mbox{\tiny ridge}}={\bf 0}$. 
	\ei 
}

\practice{
\begin{itemize}
	\item Prostate data: Ridge regression estimation and {\em coefficients path}.
	\item Use the R script {\tt prostate.ridge.regression.R}.
\end{itemize}
}


\frame{{Singular Value Decomposition of $\bX$}
\bi 
\item Let $\bX = \bU \bD \bV\tr$ be the Singular Value Decomposition of $\bX$. That is:
{\scriptsize 
\bi 
\item $\bU$, $n\!\!\times\!\! p$ orthonormal matrix whose columns span the $\bX$ column space.
\item $\bD$, $p\!\!\times\!\! p$ diagonal matrix with elements $d_1\ge\ldots\ge d_p\ge 0$ in the diagonal, that are called \blue{singular values of $\bX$.}
\item $\bV$, $p\!\!\times\!\! p$ orthonormal matrix whose columns span the row space of $\bX$. 
\ei 
}
\item Observe that $\bX\tr \bX = \bV \bD \bU\tr \bU \bD \bV\tr = \bV \bD^2  \bV\tr$ and it follows that the eigenvalues of $\bX\tr\bX$ are the squared singular values of $\bX$: 
$$
\gamma_j=d_j^2,\,\, j=1,\ldots,p.
$$
\item As we are assuming that the explanatory variables have zero mean, we have that $\bX\tr \bX$ is the sample covariance matrix. Then the columns of $\bV$ are the \blue{principal components} of $\bX$. Moreover the columns of $\bU$ are the scores of the observed data in the principal components.
\ei 
}

\frame{{Numerical stability of ridge regression}
\bi 
\item  $\hat{\bbeta}_{\mbox{\tiny ridge}}=\left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \bX\tr \by$
\item Let us compute the condition number of $\bX\tr \bX + \lambda \bI_p$, 
%using the s.v.d. of $\bX$:
$$
\bX\tr \bX + \lambda \bI_p = \bV \bD^2  \bV\tr + \lambda \bV \bV\tr=
\bV \left(\bD^2 + \lambda \bI_p\right)  \bV\tr.
$$
\item $\left(\bD^2 + \lambda \bI_p\right)$ is a diagonal matrix whose elements in the diagonal are
$$
d_j^2 + \lambda = \gamma_j + \lambda,\, \, j=1,\ldots,p.
$$ 
\item Therefore the condition number of $\bX\tr \bX + \lambda \bI_p$ is
$$
\kappa\left(\bX\tr \bX + \lambda \bI_p\right)=\sqrt{\frac{\gamma_1+ \lambda}{\gamma_p+ \lambda}}
$$
lower than $\kappa\left(\bX\tr \bX\right)=\sqrt{\gamma_1/\gamma_p}$ for all $\lambda>0$.
\item By the way, $\left(\bX\tr \bX + \lambda \bI_p \right)^{-1}=\bV \left(\bD^2 + \lambda \bI_p\right)^{-1}  \bV\tr$, and 
$\left(\bD^2 + \lambda \bI_p\right)^{-1} =\text{Diagonal}(1/(d_j^2 + \lambda),\, j=1,\ldots,p)$. 
\ei 
}

\subsection{Linear estimators of a regression function}
\frame{ {Linear estimators of a regression function}
	\bi 
	\item Let $(\bx_i,y_i)$, $i=1,\ldots,n$, be $n$ i.i.d. observs. from the r.v. $(\bX,Y)$. 
	\item Let $m(\bx)=\E(Y|\bX=\bx)$ be the regression function of $Y$ over $\bX$.
	\item Let $\hat{m}(\bx)$ an estimator (parametric, non-parametric, ...) of the regression function $m(\bx)$. 
	\item We say that $\hat{m}(\bx)$ is a \blue{linear estimator} when for any fix $\bx$, $\hat{m}(\bx)$ is a linear function of $y_1,\ldots,y_n$:
	$$
	\hat{m}(\bx)= \sum_{i=1}^n w_i(\bx) y_i,
	$$
where in fact $w_i(\bx) = w_i(\bx; \bx_1,\ldots, \bx_n)$.
	\item For the $n$ observed values $\bx_i$ of the explanatory variable, let
	$$
	\hat{y}_i=\hat{m}(\bx_i)=\sum_{j=1}^n w_j(\bx_i) y_j
	$$
	be the fitted values.
	\ei 
}

\frame{
	\bi 
	\item In matrix format, 
	$$
	\blue{\hat{\by}=\bW \by,} 
	$$
	where the column vectors $\by$ and $\hat{\by}$ have elements $y_{i}$ and $\hat{y}_{i}$, respectively, and the matrix $\bW$ has generic $(i,j)$ element
	$$ 
	w_{ij}=w_j(\bx_{i}).
	$$
	
	\item The matrix $\bW$ is analogous to the \blue{hat matrix} $\bH=\bX(\bX\tr \bX)^{-1} \bX\tr$ in OLS estimation of the multiple linear regression:
	$$
	\blue{
		\hat{\by}_{\mbox{\tiny OLS}}=\bX(\bX\tr \bX)^{-1} \bX\tr \by = \bH \by.
	}
	$$
	
	\item Observe that ridge regression is a linear estimation method:
	$$
	\blue{
	\hat{\by}_{\mbox{\tiny ridge}}= \bX \left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \bX\tr \by = \bH_{\lambda} \by.
}	$$
	\ei 
}

\frame{{Effective number of parameters for linear estimators}
\bi 
	\item Consider the multiple linear regression with $p$ regressors (including the constant term, if it appears in the model):
	$$
	\by=\bX\bbeta+\varepsilon,
	$$
	$\bX$ being a $n\times p$ matrix, $\bbeta\in\R^p$.
	
	\item It is known that  
	$$
	\hspace*{-.5cm} 
	\Trace(\bH)=\Trace(\bX(\bX\tr \bX)^{-1} \bX\tr)=
	\Trace((\bX\tr \bX)^{-1} \bX\tr \bX)=\Trace(\bI_p)=p,
	$$
	that is the number of parameters in the model.
	
	\item For a linear estimator with matrix $\bW$ (\blue{$\hat{\by}=\bW \by$}) we define  
	$$
	\nu = \Trace(\bW)= \sum_{i=1}^n w_{ii},
	$$
	the sum of diagonal elements of $\bW$.
	\ei 
}

\frame{
	\bi
	
	\item $\nu=\Trace(\bW)$ is called the \blue{effective number of parameters} 
	of the linear estimator corresponding to matrix $\bW$.
	
	\item In some books (and softwares) $\nu$ is called \blue{effective degrees of freedom} ($\df$) of the regression estimator. 
	This is the terminology used by \citeN{HasTibFri:2009} and \citeN{HasTibWai:2015}, and related packages.
	
	\item The interpretation of $\nu$ as the effective number of parameters is valid for any linear estimator of the regression function (parametric, nonparametric, ...).
	
	\item Then we can compare the degree of complexity of two linear estimators of a regression function just comparing their effective numbers of parameters.
	
	\item Usually a good estimator of $\sigma^2$, the residual variance, is
	$$
	\hat{\sigma}^2=\frac{1}{n-\nu} \sum_{i=1}^n (y_i - \hat{y}_i)^2.
	$$
	\ei 
}

\frame{{Effective number of parameters in ridge regression}
In the case of ridge regression $\nu=\nu(\lambda)=\df(\lambda)$ has an explicit expression: 
	$$
	\bW = \bH_{\lambda} = 
	\bX \left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \bX =
	\bU\bD\bV\tr \, \bV \left(\bD^2 + \lambda \bI_p\right)^{-1}  \bV\tr \, \bV\bD\bU\tr =
	$$
	$$
	\bU\bD \left(\bD^2 + \lambda \bI_p\right)^{-1}  \bD\bU\tr = 
	\bU \left(\text{Diagonal}(dj^2/(d_j^2+\lambda),\, j=1,\ldots,p\right) \bU\tr
	$$
$$
\then \nu(\lambda)=\df(\lambda)=\Trace(\bH_\lambda)=
$$
$$
\trace(\bU \left(\text{Diagonal}(dj^2/(d_j^2+\lambda),\, j=1,\ldots,p\right) \bU\tr)=
$$
$$
\trace(\left(\text{Diagonal}(dj^2/(d_j^2+\lambda),\, j=1,\ldots,p\right) \bU\tr \bU)=
$$
$$
\trace(\left(\text{Diagonal}(dj^2/(d_j^2+\lambda),\, j=1,\ldots,p\right))
=\sum_{j=1}^p \frac{dj^2}{d_j^2+\lambda}.
$$
}

\frame{
$$
\nu(\lambda)=\df(\lambda)=\sum_{j=1}^p \frac{dj^2}{d_j^2+\lambda}
$$	
\bi
\item 
$\lim_{\lambda\tiende \infty} \df(\lambda)=0$,  \ \ 
$\lim_{\lambda\tiende 0} \df(\lambda)=\text{rank}(\bX)$.

	\item The effective number of parameters $\nu(\lambda)=\df(\lambda)$ is a decreasing 
	function of penalizing parameter $\lambda$:
	\bi 
	\item Small values of $\lambda$ correspond to large numbers $\nu$ of effective parameters, close to the number of linearly independent explanatory variables (usually $\min\{n,p\}$), allowing complex and flexible estimators. 
	\item Large values of $\lambda$ correspond to small numbers $\nu$ of effective parameters, that is, to regression estimators with low complexity and flexibility. 
	\ei 
	
	\ei 
}

\practice{
\begin{itemize}
	\item Prostate data: Effective number of parameters in ridge regression.
	\item Use the R script {\tt prostate.ridge.regression.R}.
\end{itemize}
}


\frame{{Effective degrees of freedom for non-linear estimators}
\bi 
%	\item Let $(\bX_i,Y_i)$, $i=1,\ldots,n$, be $n$ i.i.d.r.v. distributed as $(\bX,Y)$. 
%	\item Conditioning to $\bX_i=\bx_i$, $i=1,\ldots,n$, it is equivalent to say
%	$$
%	Y_i=m(\bx_i) + \varepsilon_i,\, i=1,\ldots,n,
%	$$
%	with $m(\bx)=\E(Y|\bX=\bx)$ and $\varepsilon_i=Y_i-m(\bx_i)$, having $\E(\varepsilon)=0$ and $\text{Var}(\varepsilon)=\sigma^2$ for all $i$. 
	\item Let $\hat{m}(\bx)$ an estimator of the regression function $m(\bx)$ (a random function because it is based on $(Y_1,\ldots,Y_n)$).
	%\item 
	Let $\hat{Y}_i=\hat{m}(\bx_i)$.
\item The effective degrees of freedom of $\hat{m}(\bx)$ is defined as 
\blue{
$$
\df(\hat{m}) = \frac{1}{\sigma^2} \sum_{i=1}^{n} \text{Cov}(\hat{Y}_i, Y_i).
$$
}
\item Interpretation: 
\bi 
{\footnotesize 
\item A very flexible regression estimator $\hat{m}(\bx)$ will be able to interpolate the observed data, and then 
$$
\hat{Y}_i=Y_i, \, \text{Var}(\hat{Y}_i)=\text{Var}(Y_i)=\sigma^2,\, 
\text{Cov}(\hat{Y}_i, Y_i)/\sigma^2 = \text{Cor}(\hat{Y}_i, Y_i)=1,
$$
so $\df(\hat{m}) =n$: $\hat{m}(\bx)$ has as many degrees of freedom as the number of observed data.
\item The constant function equal to the sample mean of $Y_1,\ldots,Y_n$ for all $\bx$ has 1 degree of freedom.
\item A function that is constant in $\bx$ has 0 degrees of freedom if this constant does not depend on the data.
}
\ei 
\ei 
}


\frame{{Both definitions of $\df$ coincide in linear estimators}
Assume that $\hat{m}(\bx)$ is a linear estimator with matrix $\bW$. Assume also that $\E(Y)=0$. 
Then
$$
\df(\hat{m}) = 
\frac{1}{\sigma^2} \sum_{i=1}^{n} \text{Cov}(\hat{Y}_i, Y_i) =
\frac{1}{\sigma^2} \Trace\left( \text{Cov}(\hat{\bY},\bY) \right) =
$$
$$
\frac{1}{\sigma^2} \Trace\left( \E(\hat{\bY} \bY\tr) \right) =
\frac{1}{\sigma^2} \Trace\left( \E(\bW \bY \bY\tr ) \right) =
$$
$$
\frac{1}{\sigma^2} \Trace\left( \bW  \E(\bY \bY\tr) \right) =
	\frac{1}{\sigma^2} \Trace\left( \bW \sigma^2 \bI_p \right) =
 \Trace(\bW).
$$
}

\subsection{Choosing the tunning parameter by CV and GCV}
\frame{{Choosing the tunning parameter $\lambda$}
\bi
\item The tunning parameter $\lambda$ can be chosen by cross-validation (CV), 
$k$-fold cross-validation ($k$-fold CV)
or by generalized cross-validation (GCV).

\item Given the expression of $\hat{\bbeta}_{\mbox{\tiny ridge}}$ (linear in $\by$) CV and GCV are not computationally expensive.

\item We will first introduce these concepts before talking about efficient computation. 
\ei 
}


\frame{
	\bi 
	\item \blue{Predictive Mean Square Error (PMSE).}
	It is the expected squared error made when predicting
	$$
	Y = m(\bx) + \varepsilon
	$$
	by $\hat{m}(\bx)$, where $\bx$ is an observation of the random variable $\bX$, distributed as the observed explanatory variable, when $\bX$ and $\varepsilon$ are independent from the sample $\mathcal{Z}=\{(\bX_i,Y_i): i=1,\ldots, n\}$ used to compute $\hat{m}$:
	$$
	\PMSE(\hat{m}) = 
	E_{\mathcal{Z},\bX,\varepsilon}\left[ (Y-\hat{m}(\bX))^2 \right].
	$$
	%\item PMSE is a particular case of \blue{expected loss}:
	%$E_{\mathcal{Z},\bX,\varepsilon}\left[ L(Y,\hat{m}(\bX)) \right]$,
	%where $L(y,\hat{y})$ is the loss of predicting the value $y$ by $\hat{y}$.
	%\item Other examples of loss functions: $|y-\hat{y}|$, $\one\{y\ne\hat{y}\}$. 
	\ei 
}

\frame{\frametitle{Prediction error in a validation set}
	{\small 
		\bi 
		\item When the number of available data is large (as it usually happens in data mining or in Big Data problems) the sample is randomly divided in three sets:
		\bi 
		\item The \blue{training set}: it is used to fit the model.
		\item The \blue{validation set}: it is used to compute feasible versions of the above criteria for model selection and/or parameter tuning.
		\item The \blue{test set}: it is used to evaluate the generalization (or prediction) error of the final chosen model in independent data.
		\ei 
		
		\item Assuming that at least a validation set has been preserved, an estimation of PMSE is the \blue{Predictive Mean Squared Error in the validation set}:
		$$
		\PMSE_{\val}(\hat{m}) = {1\over n_{V}} \sum_{i=1}^{n_{V}}
		(y_i^{V} - \hat{m}(\bx_i^{V}))^2,
		$$
		where $(\bx_i^V,y_i^V)$, $i=1,\ldots,n_{V}$, is the validation set
		and $\hat{m}(\bx)$ is the estimator computed using the training set.
		\ei 
	}
}

\frame{\frametitle{Leave-one-out cross-validation}
\bi 
\item When the sample size does not allow us to set a validation set aside, 
	\blue{leave-one-out cross-validation} is an attractive alternative:
	\begin{enumerate}
	\item Remove the observation $(\bx_{i},y_{i})$ from the sample and fit the regression using the other $(n-1)$ data. 
	Let $\hat{m}_{(i)}(\bx)$ be the resulting estimator.
	\item Now use  $\hat{m}_{(i)}(\bx_i)$ to predict $y_i$.
	\item Repeat the previous steps for $i=1,\ldots,n$.
	\item Compute 
	$$
	\PMSE_{\CV}(\hat{m}) = {1\over n} \sum_{i=1}^{n} (y_i - \hat{m}_{(i)}(\bx_i))^2.
	$$
	\end{enumerate}
\item In ridge regression:
	$$
	\lambda_{CV}  = \arg \min_{\lambda\ge 0} \PMSE_{\CV}(\lambda)=
	{1\over n} \sum_{i=1}^{n} (y_i -  \bx_i\tr \hat{\bbeta}_{\mbox{\tiny ridge},\lambda}^{(i)})^2.
	$$ 
\ei 
}

\practice{
\begin{itemize}
	\item Prostate data: Leave-one-out cross-validation in ridge regression.
	\item Use the R script {\tt prostate.ridge.regression.R}.
\end{itemize}
}

\frame{{$k$-fold cross validation}
	\bi 
	\item $\PMSE_{\CV}(\hat{m})$ is an approximately unbiased estimator of $\PMSE(\hat{m})$, but has a considerable variance.
	\item The variance can be reduced doing $k$-fold cross-validation:
	The sample is randomly divided in $k$ subsets, each of them is removed by turns from the sample, the model is estimated with the other $(k-1)$ subsamples and the removed subsample is used to compute prediction errors.
	\item $n$-fold cross-validation is leave-one-out cross-validation.
	\item $k$-fold cross-validation has lower variance than leave-one-out cross-validation but larger bias.
	\item General recommendation: Use 5-fold or 10-fold cross-validation.
	\ei 
}



\frame{\frametitle{Efficient computation of $\PMSE_{\CV}$}
	\bi 
	\item Consider a \blue{linear estimator} of the regression function with matrix $\bW=(w_{ij})_{i,j}$: \blue{$\hat{\by}=\bW \by$}.
	\item That is   
	$$
	\hat{y}_i=\sum_{j=1}^n w_{ij} y_j, \, \, i=1,\ldots,n,
	$$
	where $w_{ij}=w_j(\bx_i)= w_j(\bx_i; \bx_1,\ldots, \bx_n)$.
	\item In these cases $\PMSE_{\CV}$ can be calculated avoiding the computational cost of fitting  $n$ different regression models.
	\item For most linear estimators it can be proved that
	\blue{
		$$
		\PMSE_{\CV} = {1\over n} \sum_{i=1}^{n} \left({ y_i - \hat{y}_i
			\over 1 - w_{ii} } \right)^2.
		$$}
%	See \citeN{Wood:2006}, pages 169-170.
	\ei 
}

\frame{{Proof for the ridge regression estimation}
{\scriptsize 
\bi 
\item Let $\hat{\bbeta}_{\mbox{\tiny ridge},\lambda}^{(i)}$ be the estimation of $\bbeta=(\beta_1,\ldots,\beta_p)$ when leaving out the $i$-th observation:
$$
\hat{\bbeta}_{\mbox{\tiny ridge},\lambda}^{(i)} = 
\arg\min_{\bbeta} \left\{
\sum_{l=1,l\ne i}^n \left( y_l - \sum_{j=1}^{p} x_{lj}\beta_j\right)^2 + 
\lambda \|\bbeta\|_2^2
\right\}
$$
\item Let us define 
$$
\tilde{y}_l^{(i)} =
\left\{
\begin{array}{lrc}
y_l & \text{ if } & l\ne i, \\
\hat{y}_i^{(i)}=\sum_{j=1}^{p} x_{ij}\hat{\beta}_{\mbox{\tiny ridge},\lambda,j}^{(i)}
& \text{ if } & l=i.
\end{array} 
\right.
$$
\item It follows that for all $\bbeta\in \R^p$,
$$
\sum_{l=1}^n \left( \tilde{y}_l^{(i)} - \sum_{j=1}^{p} x_{lj}\beta_j\right)^2 + 
\lambda \|\bbeta\|_2^2
=
\left\{ 
\sum_{l=1,l\ne i}^n \left( y_l - \sum_{j=1}^{p} x_{lj}\beta_j\right)^2 + 
\lambda \|\bbeta\|_2^2 
\right\}
+
\left(\sum_{j=1}^{p} x_{lj}(\hat{\beta}_{\mbox{\tiny ridge},\lambda,j}^{(i)} - \beta_j) \right)^2
$$
\item Observe that $\hat{\bbeta}_{\mbox{\tiny ridge},\lambda}^{(i)}$ minimizes both terms in the right hand side. 
Then it is also
$$
\hat{\bbeta}_{\mbox{\tiny ridge},\lambda}^{(i)} = 
\arg\min_{\bbeta} \left\{
\sum_{l=1}^n \left( \tilde{y}_l^{(i)} - \sum_{j=1}^{p} x_{lj}\beta_j\right)^2 + 
\lambda \|\bbeta\|_2^2
\right\}
$$
\item This is the ridge regression estimator corresponding to a data set with matrix of explanatory variables $\bX$ and vector of responses $\tilde{\by}^{(i)}=(\tilde{y}_1^{(i)},\ldots,\tilde{y}_n^{(i)})\tr$. 
\ei 
}
}

\frame{ 
{\scriptsize 
\vspace*{-.5cm}
\bi 
\item Then
$$
\hat{\bbeta}_{\mbox{\tiny ridge},\lambda}^{(i)} = 
(\bX\tr \bX + \lambda \bI_p)^{-1} \bX\tr \tilde{\by}^{(i)},
$$
$$
\hat{\tilde{\by}}^{(i)} = 
\bX \hat{\bbeta}_{\mbox{\tiny ridge},\lambda}^{(i)} = 
\bX (\bX\tr \bX + \lambda \bI_p)^{-1} \bX\tr \tilde{\by}^{(i)} =
\bH_{\lambda} \tilde{\by}^{(i)}.
$$
\item Observe that the $i$-th element of $\hat{\tilde{\by}}^{(i)}$ is just $\hat{y}_i^{(i)}=\sum_{j=1}^{p} x_{ij}\hat{\beta}_{\mbox{\tiny ridge},\lambda,j}^{(i)}$.
\item Let $\be_i$ be the $n$-dimensional vector whose $i$-th element is $1$ and the others are equal to 0. 
\item Then $\tilde{\by}^{(i)}=\by - (y_i - \hat{y}_i^{(i)})\be_i$ and, consequently,
$$
\hat{\tilde{\by}}^{(i)} = \bH_{\lambda} \tilde{\by}^{(i)} = 
\bH_{\lambda} \left(\by - (y_i - \hat{y}_i^{(i)})\be_i\right) =
\bH_{\lambda} \by - (y_i - \hat{y}_i^{(i)})\bH_{\lambda}\be_i =
\hat{\by} - (y_i - \hat{y}_i^{(i)})\bh_i^{\lambda},
$$
where $\bh_{i}^{\lambda}$ is the $i$-th column of $\bH_{\lambda}$. 
\item Looking just at the $i$-th component, 
$\hat{y}_i^{(i)}= \hat{y}_i - (y_i - \hat{y}_i^{(i)}) h_{ii}^{\lambda}$, 
where $h_{ii}^{\lambda}$ is the element $(i,i)$ of $\bH_{\lambda}$, or the $i$-th element in the diagonal of $\bH_{\lambda}$. 
\item Then $y_i - \hat{y}_i^{(i)}= y_i - \hat{y}_i + (y_i - \hat{y}_i^{(i)}) h_{ii}^{\lambda}$ and we conclude that
$$
y_i - \hat{y}_i^{(i)}= \frac{y_i - \hat{y}_i}{1-h_{ii}^{\lambda}}.
$$
So the loo-CV errors $(y_i - \hat{y}_i^{(i)})$ can be computed if we know the 
errors $(y_i - \hat{y}_i)$ when fitting the ridge regression with all the data, 
and the diagonal of $\bH_{\lambda}$, and the proof concludes. 
\ei 
}
}

\practice{
\begin{itemize}
	\item Prostate data: Efficient computation of $\PMSE_{\CV}$ in ridge regression.
	\item Use the R script {\tt prostate.ridge.regression.R}.
\end{itemize}
}


\frame{\frametitle{Generalized cross-validation}
	\bi 
	\item For \blue{linear estimators} of the regression function, a modification can be done in the measure of 
	$\PMSE_{\CV}$.
	\item It is known as \blue{generalized cross-validation (GCV)}.
	\item It consists in replacing in the expression of $\PMSE_{\CV}$ 
	the values $w_{ii}$, coming from the diagonal of $\bW$, by their average value:
	$$
	\PMSE_{\GCV} = {1\over n} \sum_{i=1}^{n} \left({ y_i - \hat{y}_i
		\over 1 - \nu/n } \right)^2,
	$$
	$\nu = \Trace(\bW)= \sum_{i=1}^n w_{ii}$ is the effective number of parameters.
	\item In ridge regression, \blue{$\lambda_{\GCV} = \arg \min_{\lambda} \PMSE_{\GCV}(\lambda)$.}
	\item Manipulating the expression of $\PMSE_{\GCV}$ it follows that
	$$
	\PMSE_{\GCV}=  {n \hat{\sigma}^2_{\varepsilon} \over n- \nu},
	$$
	where
	$\hat{\sigma}^2_{\varepsilon}={1\over n- \nu}\sum_{i=1}^{n} ( y_i - \hat{y}_i)^2$ estimates the residual variance.
	\ei 
}

\practice{
\begin{itemize}
	\item Prostate data: $\PMSE_{\GCV}$ in ridge regression.
	\item Use the R script {\tt prostate.ridge.regression.R}.
\end{itemize}
}

\frame{{Variance of the ridge regression estimator}
Remember that 
$\hat{\bbeta}_{\mbox{\tiny ridge}}=\left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \bX\tr \by$.
Then
\begin{align*}
\mbox{Var}(\hat{\bbeta}_{\mbox{\tiny ridge}}) &= 
\left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \bX\tr
\mbox{Var}(\by) 
\bX \left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \\
&=\sigma^2 
\left(\bX\tr \bX + \lambda \bI_p \right)^{-1} \bX\tr
\bX \left(\bX\tr \bX + \lambda \bI_p \right)^{-1}.
\end{align*}
From the s.v.d. of $\bX$, $\bX = \bU \bD \bV\tr$, we have deduced that
$\bX\tr \bX = \bV \bD^2  \bV\tr$ and that
$\left(\bX\tr \bX + \lambda \bI_p \right)^{-1}=\bV \left(\bD^2 + \lambda \bI_p\right)^{-1}  \bV\tr$
Therefore,
\begin{align*}
\mbox{Var}(\hat{\bbeta}_{\mbox{\tiny ridge}})
&=\sigma^2 \bV \left(\bD^2 + \lambda \bI_p\right)^{-1} 
\bD^2 
\left(\bD^2 + \lambda \bI_p\right)^{-1} \bV\tr \\
&= \sigma^2 \bV \, \mbox{Diagonal}\left(d_j^2/(d_j^2+\lambda)^2,\, j=1,\ldots,p \right) \bV\tr.
\end{align*}
}

\section{The Lasso estimation}

\frame{\frametitle{The Lasso estimation \cite{tibshirani:1996:lasso}}
\bi 
\item Lasso: Least absolute shrinkage and selection operator.
\item The Lasso, also a shrinkage method, uses the norm $L_1$ as penalty term: 
$$
\hat{\bbeta}_{\mbox{\tiny Lasso}} = 
\arg\min_{\bbeta} \left\{ 
\frac{1}{2n} 
\sum_{i=1}^n 
\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}  \beta_j  \right)^2 + \lambda \sum_{j=1}^p |\beta_j| 
\right\} 
$$
\item Alternative expression: 
$$
\hat{\bbeta}_{\mbox{\tiny Lasso}} = 
\arg\min_{\bbeta} 
\frac{1}{2n} 
\sum_{i=1}^n 
\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}  \beta_j  \right)^2 
$$
$$
\mbox{subject to } \sum_{j=1}^p |\beta_j| \le t.
$$
\item $t=s \|\hat{\bbeta}_{\tiny{OLS}}\|_{\ell_1}$, $s\in[0,1]$. 
\blue{$s$: shrinkage factor}.
\ei  
}

\frame{\frametitle{Lasso gives sparse solutions}
\begin{center}
%\fbox{
\includegraphics[scale=.7,viewport=55 80 390 320,clip]{ESL_page_91.pdf}
%}
\end{center}
{\tiny Source: \citeN{HasTibFri:2009}}
}

\frame{\frametitle{Prostate cancer example. Lasso}
\vspace*{-.1cm}
\begin{center}
%\fbox{
\includegraphics[scale=.6,viewport=50 190 400 520,clip]{ESL_page_90.pdf}
%}
\end{center}
\vspace*{-.5cm}
{\tiny Source: \citeN{HasTibFri:2009}}
}


\frame{\frametitle{Lasso: Properties}
\bi 
\item Lasso provides sparse solutions.
\item Lasso enables estimation and variable selection simultaneously
in one stage.
\item No closed expression for the Lasso estimator.
\item Lasso involves a convex optimization problem (convex quadratic objective function, convex feasible region) 
$$
\arraycolsep=1pt
\begin{array}{rl}
\min_{\beta\in \R^p} & \frac{1}{2n} \sum_{i=1}^n (y_i - \bx_i\tr \bbeta)^2 \\
\mbox{s.t.}& \|\bbeta\|_{\ell_1}\le t
\end{array}
$$
that can be efficiently solved.
\ei 
} 

\frame{\frametitle{Lasso and $\ell_q$ norms}
\bi 
\item For $q>0$, 
$\ell_q$ norm of $\bbeta\in \R^p$: $\|\bbeta\|_{\ell_q} = \left( \sum_{j=1}^p |\bbeta_j|^q \right)^{1/q}$. 
\item $\|\bbeta\|_{\ell_{\infty}} = \lim_{q\tiende \infty}\|\bbeta\|_{\ell_q}=\max_{j=1,\ldots,p}|\beta_j|$. 
\item Defining $0^0=0$, $\|\bbeta\|_{\ell_0} = \sum_{j=1}^p |\beta_j|^0$, the $\ell_0$ ``norm'' of $\bbeta$ is the number of non-zero entries of $\bbeta$. This is not a real norm ($\|a \bbeta\|_{\ell_0}\ne |a| \|\bbeta\|_{\ell_0}$ for scalars $a\not\in\{-1,0,1\}$). 
\ei 
\begin{center}
%\fbox{
\includegraphics[scale=.7,viewport=130 270 500 380,clip]{SLS_page_22_Fig_2_6.pdf}
%}
\end{center}
\vspace*{-.5cm}
{\tiny Source: \citeN{HasTibWai:2015}} 

}

\frame{ 
\bi 
\item Lasso is between the best subset selection (a combinatorial problem) and the ridge regression:
$$
{\scriptsize 
\arraycolsep=1pt
\begin{array}{rl}
\blue{\mbox{Best subset}}&\blue{\mbox{selection}}\\
\min_{\bbeta\in \R^p}&\frac{1}{2n}\sum_{i=1}^n (y_i - \bx_i\tr \bbeta)^2 \\
\mbox{s.t.}&\|\bbeta\|_{\ell_0}\le t
\end{array}
\,\,\,
\begin{array}{rl}
\blue{\mbox{Lasso}}&\\
\min_{\bbeta\in \R^p} &\frac{1}{2n}\sum_{i=1}^n (y_i - \bx_i\tr \bbeta)^2 \\
\mbox{s.t.}& \|\bbeta\|_{\ell_1}\le t
\end{array}
\,\,\,
\begin{array}{rl}
\blue{\mbox{Ridge}}&  \blue{\mbox{regression}}\\
\min_{\bbeta\in \R^p} &\frac{1}{2n}\sum_{i=1}^n (y_i - \bx_i\tr \bbeta)^2 \\
\mbox{s.t.}& \|\bbeta\|_{\ell_2}\le t
\end{array}
}
$$
\item The Lasso problem ($\ell_1$-penalty) uses the smallest value of $q$ that leads to a convex constraint region. 
\item In this sense, it is the closest convex relaxation of the best subset selection problem ($\ell_0$), among those based on $\ell_q$-penalties, $q\ge 0$. 
\ei  
\vspace*{-.5cm}
\begin{center}
%\fbox{
\includegraphics[scale=.7,viewport=130 270 500 380,clip]{SLS_page_22_Fig_2_6.pdf}
%}
\end{center}
\vspace*{-.5cm}
{\tiny Source: \citeN{HasTibWai:2015}} 
}

\frame{\frametitle{Lasso: A retrospective \cite{tibshirani:2011:lasso_retosp}}
\bi 
\item After publication, \citeN{tibshirani:1996:lasso} did
not receive much attention until years later. 
\item Why? In 2011, Tibshirani's guesses were that
\bi 
\item[(a)] the computation in 1996 was slow compared with today,
\item[(b)] the algorithms for the Lasso were black boxes and not statistically motivated (until the LARS (least angle regression) algorithm in 2002),
\item[(c)] the statistical and numerical advantages of sparsity were not immediately appreciated (by Tibshirani or the community),
\item[(d)] large data problems (in $N$, $p$ or both) were rare and
\item[(e)] the community did not have the R language for fast, easy sharing of new software tools.
\ei 
\ei 
}


\noframe{
\vspace*{-1cm}
\begin{center}
%\fbox{
\includegraphics[scale=.8,viewport=
40 360 440 646,clip]{Tibshirani_2011_JRSS_lasso_retr_page_6.pdf}
%}
\end{center}
\vspace*{-.5cm}
{\tiny Source: Peter Bülhmann's comments to \citeN{tibshirani:2011:lasso_retosp}}
}

\noframe{ \frametitle{From Peter Bülhmann's comments to \citeN{tibshirani:2011:lasso_retosp}}
\begin{quotation}
[The previous Figure] shows that [\blue{Lasso}] frequency of
citation continues to be in the exponential growth regime, together with the \blue{false discovery rate} paper from Benjamini and Hochberg (1995): 
\blue{both of these works are crucial for high dimensional statistical inference}.
\end{quotation}
}

\subsection{Computation of Lasso}
\frame{ \frametitle{Computation of Lasso}
\bi 
\item The original Lasso paper used a standard quadratic program solver. 
\item This does not scale well and is not transparent. 
\item The LARS algorithm \cite{efron:2004:lars} gives an efficient way of
solving the Lasso and connects the Lasso to forward stagewise regression. 
\item Later on, a \blue{cyclic coordinate descent algorithm} replaced LARS and,
since \citeN{friedman2010regularization} the {\tt glmnet} R package implements 
this algorithm.
\ei 
}


%\section{Cyclic coordinate optimization}
\frame{\frametitle{Cyclic coordinate optimization} 
	\bi 
	\item Consider the problem
	$$
	\min_{\bx \in \R^q} f(\bx)\equiv 
	\min_{(x_1,\ldots,x_q)\in \R^q} f(x_1,\ldots,x_q).
	$$
	\item The \blue{cyclic coordinate descent algorithm} works as follows:
	\begin{enumerate}
		\item Let $k=0$ and choose an arbitrary initial point $\bx^0=(x_1^0,\ldots,x_q^0)\in \R^q$.
		\item Iterate until convergence:
		\bi 
		\item For $i=1,\ldots,q$, 
		$$
		x^{k+1}_i = \arg\min_{y\in\mathbb R}
		f(x^{k+1}_1, \dots, x^{k+1}_{i-1}, y, x^k_{i+1}, \dots, x^k_q).
		$$
		\item STOP if $\|\bx^{k+1}-\bx^k\|$ or $|f(\bx^{k+1})-f(\bx^k)|$ are small.
		\ei 
	\end{enumerate}
	\item This algorithm is specially useful when the one-dimensional optimization problems have closed form solution. 
	\item This is the case for the \blue{LASSO estimation in regression}.
	\ei 
}

\noframe{
\practice{
	\begin{itemize}
		\item Maximum likelihood estimation of 2 parameters,
		\centerline{$X\sim \mbox{logistic}(m,s)$,}
		using the coordinate ascent algorithm.
		\item Use the last sentences of the R script {\tt illustr.numer.coord.R}.
	\end{itemize}
}
}

\frame{\frametitle{Cyclic coordinate optimization. Properties} 
	{\scriptsize 
		Consider the problem $\min_{\bx \in \R^q} f(\bx)$.
		\bi
		\item The cyclic coordinate descent algorithm has the descent property: $f(\bx^{k+1})\le f(\bx^k)$ far all $k$. 
		\item Sufficient conditions for the algorithm convergence.
		\bi 
		{\scriptsize 
			\item Assuming that $f$ is twice differentiable, that $\bx^*\in \R^q$ is a local minimum of $f$ and that the Hessian matrix of $f$ at $\bx^*$ is positive definite, then the cyclic coordinate descent algorithm converges \blue{locally} to  $\bx^*$:
			if $\bx^0$ is \blue{close} to $\bx^*$ then $\lim_k \bx^k = \bx^*$.
			\\
			\cite[Section 13.3, page 165]{Lange:1999}
			\item If $f$ is continuously differentiable and strictly convex in each coordinate, then the cyclic coordinate descent algorithm converges to the global minimum of $f$. 
			\item If $f$ has the additive decomposition 
			$$
			f(x_1,\ldots,x_q) = g(x_1,\ldots,x_q) + \sum_{i=1}^q h_i(x_i)
			$$
			where $g$ is differentiable and convex, and the univariate functions $h_i$ are convex (but not necessarily differentiable), then the cyclic coordinate descent algorithm converges to the global minimum of $f$. 
			\\
			\cite[Section 5.4.1, for references]{HasTibWai:2015} 
			\item The \blue{LASSO estimation in regression} has this separability property.
		}
		\ei 
		\ei 
	}
}


\frame{{Failure of the coordinate descent algorithm}
	\bi 
	\item When the non-differentiable part of $f(x_1,\ldots,x_q)$ is not separable, the coordinate descent algorithm may fail to converge.
	
	\item \blue{Example:} $\min_{(x,y)\in\R^2} f(x,y)=5|x-2y|+|2x+y|$
	
	The global minimum is $(x,y)=(0,0)$ but any point over the line $y=x/2$ is a fixed point of the algorithm.
	\ei 
	\centerline{\includegraphics[scale=.5]{grad_desc_bad.pdf}}
}


\frame{\frametitle{Cyclic coordinate descent algorithm for LASSO:}
\bi 
\item First we will see that Lasso has a closed form solution when $p=1$ (single predictor case).
\item Them we will give the co-ordinate descent algorithm for a generic $p$.
\ei 
}


\frame{{Single predictor. Soft thresholding function}
\bi 
\item We observe $(x_i,y_i)$, $i=1,\ldots,n$, $x_i\in \R$, $y_i\in \R$, and assume 
$$
\sum_{i=1}^n x_i=0,\, 
\frac{1}{n}\sum_{i=1}^n x_i^2=1,\, 
\sum_{i=1}^n y_i=0
\Rightarrow
\hat{\beta}_{\tiny{\mbox{OLS}}}=\frac{1}{n}\sum_{i=1}^n x_i y_i= \frac{1}{n}\langle \bx, \by \rangle. 
$$
\item Consider the Lasso problem  
\blue{$\min_{\beta\in \R} \left\{ \frac{1}{2n}\sum_{i=1}^n (y_i - x_i \beta)^2 + \lambda |\beta| \right\}$}.
\ei
\vspace*{-.5cm}
\centerline{\includegraphics[scale=.3]{Lasso_single_predictor.pdf}}
}

\frame{
Let $f(\beta)=\frac{1}{2n}\sum_{i=1}^n (y_i - x_i \beta)^2 + \lambda |\beta|$. Then,
$$
f'(\beta)=
\left\{
\begin{array}{rcl}
-\frac{1}{n} \sum_{i=1}^n (y_i - x_i \beta)x_i + \lambda = 
-\frac{1}{n} \langle \bx, \by \rangle + \beta +\lambda & \mbox{if} & \beta> 0, 
\\
-\frac{1}{n} \sum_{i=1}^n (y_i - x_i \beta)x_i - \lambda = 
-\frac{1}{n} \langle \bx, \by \rangle + \beta -\lambda & \mbox{if} & \beta< 0.
\end{array} 
\right.
$$
\bi 
\item If $\hat{\beta}_{\tiny{\mbox{OLS}}}=\frac{1}{n}\langle \bx, \by \rangle \ge 0$ then:
\bi
\item $f'(\beta)<0$ for all $\beta<0$,
\item $f'(\beta)<0$ for $\beta \in (0, \max\{0,\hat{\beta}_{\tiny{\mbox{OLS}}} - \lambda\})$,
\item $f'(\beta)>0$ for $\beta > \max\{0,\hat{\beta}_{\tiny{\mbox{OLS}}} - \lambda\}$.
\item Therefore $\hat{\beta}_{\tiny{\mbox{Lasso}}}= \max\{0,\hat{\beta}_{\tiny{\mbox{OLS}}} - \lambda\}$.
\ei 
\item If $\hat{\beta}_{\tiny{\mbox{OLS}}}=\frac{1}{n}\langle \bx, \by \rangle < 0$ then: $f'(\beta)>0$ for all $\beta>0$. 
\bi
\item $f'(\beta)>0$ for all $\beta>0$,
\item $f'(\beta)>0$ for $\beta \in (\min\{0,\hat{\beta}_{\tiny{\mbox{OLS}}} + \lambda\},0)$,
\item $f'(\beta)<0$ for $\beta < \min\{0,\hat{\beta}_{\tiny{\mbox{OLS}}} + \lambda\}$.
\item Therefore $\hat{\beta}_{\tiny{\mbox{Lasso}}}= 
	\min\{0,\hat{\beta}_{\tiny{\mbox{OLS}}} + \lambda\} = 
	-\max\{0,-\hat{\beta}_{\tiny{\mbox{OLS}}} - \lambda\}.
	$
	\ei 
\item $\hat{\beta}_{\tiny{\mbox{Lasso}}}= \sign(\hat{\beta}_{\tiny{\mbox{OLS}}})\max\{0,|\hat{\beta}_{\tiny{\mbox{OLS}}}| - \lambda\}$.
\ei 
}

\frame{{Soft-thresholding operator}
\begin{columns}
	\begin{column}{0.4\textwidth}
\bi 
\item For $x\in\R$ let $x_+=\max\{0,x\}$ its positive part. 
\item For $\lambda>0$ we define the 
\blue{Soft-thresholding operator $\S_{\lambda}(x)=\sign(x)\left(|x|-\lambda \right)_+$}.
\item Then, in the single predictor case, 
\blue{$\hat{\beta}_{\tiny{\mbox{Lasso}}}=\S_{\lambda}(\hat{\beta}_{\tiny{\mbox{OLS}}})$}.
\ei 
	\end{column}
	\begin{column}{0.6\textwidth}
\hspace*{-.5cm}
\includegraphics[width=1.2\textwidth]{SoftThresholding.pdf}      
	\end{column}
\end{columns}
}
 
\frame{{Multiple predictors: Cyclic coordinate descent}
\bi 
\item When there are $p$ predictors, the Lasso objective function, to be minimized in $\beta\in\R^p$, is
$$
\frac{1}{2n}\sum_{i=1}^n \left(y_i - \sum_{j=1}^p x_{ij} \beta_j \right)^2 + \lambda \sum_{j=1}^p|\beta_j|
$$
\item It has the additive decomposition 
$$
f(\beta_1,\ldots,\beta_p) = g(\beta_1,\ldots,\beta_p) + \sum_{j=1}^p h_j(\beta_j)
$$
where $g$ is differentiable and convex, and the univariate functions $h_j$ are convex (but not differentiable), then the cyclic coordinate descent algorithm converges to the global minimum of $f$. \cite[Section 5.4.1, for references]{HasTibWai:2015} 
\ei 
}

\frame{ 
\bi 
\item The cyclic coordinate descent algorithm repeatedly cycle through the predictors in fixed  order (say $1,\ldots,p$) the minimization in one coordinate (say the $j$-th) fixing the others in the last available values for them (say $\hat{\beta}_k$, $k\ne j$):
$$
\min_{\beta_j\in \R} \left\{ 
\frac{1}{2n}\sum_{i=1}^n \left(y_i - \sum_{k\ne j} x_{ik} \hat{\beta}_k - x_{ij} \beta_j \right)^2 + \lambda \sum_{k\ne j}|\hat{\beta}_k| + \lambda |\beta_j|
\right\}.
$$
\item Define the \blue{partial residuals $r_i^{(j)} = y_i - \sum_{k\ne j} x_{ik} \hat{\beta}_k$}.
\item Then, the optimal value for $\beta_j$ is (with obvious notation)
\blue{$\hat{\beta}_j^{\text{new}} = 
\S_{\lambda}\left(\frac{1}{n} \langle\bx_j,\br^{(j)}\rangle\right)$}.
\item Let $\hat{\beta}_j$ be the last available estimation for $\beta_j$ before computing $\hat{\beta}_j^{\text{new}}$ and let 
$r_i = y_i - \sum_{k=1}^n x_{ik} \hat{\beta}_k$ be the previous \blue{full residuals}. Then 
$\br^{(j)}=\br + \hat{\beta}_j \bx_j$ and 
$\frac{1}{n}\langle \bx_j,\br^{(j)}\rangle = \frac{1}{n}\langle \bx_j,\br\rangle +
\hat{\beta}_j\frac{1}{n}\langle \bx_j,\bx_j\rangle = 
\frac{1}{n}\langle \bx_j,\br\rangle +\hat{\beta}_j$. 
\item Then 
\blue{$\hat{\beta}_j^{\text{new}} = 
\S_{\lambda}\left(\hat{\beta}_j + \frac{1}{n}\langle \bx_j,\br\rangle\right)$}.
\item And the new full residuals are 
\blue{$\br^{\text{new}}=\br - \left(\hat{\beta}_j^{\text{new}} -\hat{\beta}_j \right) \bx_j$}.
\ei 
}

\practice{
	\begin{itemize}
		\item Prostate data: Lasso estimation for a given $\lambda$.
		\item Use the R script {\tt prostate.lasso.R}.
	\end{itemize}
}



\frame{{Solutions path and warm starts}
\bi 
\item Typically one want a sequence of Lasso solutions, corresponding to $\lambda_0,\dots,\lambda_L=0$.
\item The largest value of $\lambda$ given a non-zeros solution is
$$
\lambda_0=\frac{1}{n}\max_{j}|\langle\by,\bx_j\rangle|,
$$
because for $\lambda>\lambda_0$ the cyclic coordinate descent algorithm has $\bbeta=\bs{0}$ as the only fixed point.
\item \blue{Warm start:} The solution $\hat{\bbeta}(\lambda_l)$ is the initial value (warm start) for the algorithm when looking for the solution $\hat{\bbeta}(\lambda_{l+1})$, $l=1,\ldots,L-1$.
\item Usually $L=100$ is enough and $\lambda_0,\dots,\lambda_{L-1}$ are evenly spaced.
\item \blue{Active set for $\lambda$:} The set of coefficients $\beta_1,\ldots,\beta_p$ that are non-zero for a given value of $\lambda$.
\item Monitoring the active sets when going from $\lambda_l$ to $\lambda_{l+1}$ allows to improve algorithmic efficiency.
\ei 
}

\practice{
	\begin{itemize}
		\item Prostate data: Lasso estimation and {\em coefficients path}.
		\item Use the R script {\tt prostate.lasso.R}.
	\end{itemize}
}



\subsection{Statistical properties of Lasso}

\frame{ {Effective degrees of freedom for Lasso (I)}
\bi 
\item Lasso is not a linear estimator of the regression function. 
\item Let $(x_{i1},\ldots,x_{ip},Y_i)$, $i=1,\ldots,n$, be $n$ data following a multiple linear  regression model with residual variance $\sigma^2$. 
\item For $\lambda>0$, let $\hat{Y}_i^{\lambda}$, $i=1,\ldots,n$, be the fitted values resulting from the Lasso estimation using penalization parameter $\lambda$.
\item The effective degrees of freedom of the Lasso estimator when using penalization parameter $\lambda$ is defined as 
$$
\df(\lambda) = \frac{1}{\sigma^2} \sum_{i=1}^{n} \text{Cov}(\hat{Y}_i^\lambda, Y_i).
$$
\ei 
}
\frame{ {Effective degrees of freedom for Lasso (II)}
\bi 
\item Let $k_{\lambda} = \| \hat{\beta}^{\lambda} \|_{\ell_0}$ be the number of non-zero estimated coefficients when using $\lambda$.
\item Observe that $k_{\lambda}$ is a random variable.
\item It can be proved that $k_{\lambda}$ is an unbiased estimator of $\df(\lambda)$.
\item A flexibility trade-off in Lasso:
\bi
\item A Lasso estimator with $k$ non-zero coefficients should have more flexibility than a OLS estimator using just $k$ variables fixed in advance, because Lasso select the {\em best} (in some sense) subset of $k$ variables.
\item But the Lasso estimation of these $k$ coefficient is less flexible than the OLS estimation because the penalization term shrinks the estimated coefficient toward zero, relative to the usual OLS estimates. 
\item Both terms compensate each other and, in average, the number of nonzero  coefficients estimates $\df(\lambda)$ with no bias. 
\ei
\ei 	
}

\frame{ \frametitle{Lasso: Statistical properties}
Consider a potentially high dimensional linear model:
$$
\by =\bX \bbeta + \varepsilon,\; \bX_{n\times p}, \; p=p_n\gg n 
\mbox{ as } n\tiende \infty.
$$

Four problems have received much attention: 
\bi 
\item Prediction and estimation of the regression surface $\bX \bbeta$.
\item Estimation of parameters $\bbeta$.
\item Variable screening or {\em Sparsistency}.
\item P-values for high-dimensional linear models.
\ei 


\bigskip

%\blue{\scriptsize (More information and references in the Appendix of these slides.)}

}


\subsection{{\tt glmnet} package in R} 
\frame{\frametitle{{\tt glmnet} package in R}
(See the {\em Glmnet vignette}, \citeN{glmnet_vignette_2014})
 
\bi 
\item Glmnet is a package that fits a generalized linear model via 
penalized maximum likelihood, using the Lasso or elasticnet penalty. 

\item The authors of glmnet are Jerome Friedman, Trevor Hastie, Rob 
Tibshirani and Noah Simon. 

\item The algorithm is extremely fast, and can exploit sparsity in the input matrix $\bX$. 

\item It fits linear, logistic and multinomial, Poisson, and Cox regression models. 

\item It can also fit multi-response linear regression.
\ei 
}

\frame{\frametitle{{\tt glmnet} package in R (II)}
\bi 
\item {\tt glmnet} solves the following problem 
$$
\min_{\beta_0,\bbeta} \frac{1}{N} \sum_{i=1}^{N} w_i \ell(y_i,\beta_0+\bbeta^T x_i) + \lambda\left[(1-\alpha)||\bbeta||_2^2/2 + \alpha ||\bbeta||_1\right],
$$
over a grid of values of $\lambda$ covering the entire range. 

\item Here $\ell(y,\eta)$ is the negative log-likelihood contribution for observation $i$; e.g. for the Gaussian case it is $(1/2)(y-\eta)^2$. 

\item The elastic-net penalty is controlled by $\alpha$, and bridges the gap between Lasso ($\alpha=1$, the default) and ridge ($\alpha=0$). 

\item The tuning parameter $\lambda$ controls the overall strength of the penalty.
\ei 
}

\frame{\frametitle{{\tt glmnet} package in R (III)}
\bi 
\item It is known that the ridge penalty shrinks the coefficients of 
correlated predictors towards each other while the Lasso tends to pick 
one of them and discard the others. 

\item The elastic-net penalty mixes these 
two; if predictors are correlated in groups, an $\alpha=0.5$ tends to select the groups in or out together. 

\item One use of $\alpha$ is for numerical stability; for example, the elastic net with $\alpha=1-\epsilon$ for some $\epsilon>0$ performs much like the Lasso, but removes any degeneracies and wild behavior caused by extreme correlations.
\ei 
}

\frame{\frametitle{{\tt glmnet} package in R (IV)}
\bi 
\item The {\tt glmnet} algorithms use cyclical coordinate descent, 
which successively optimizes the objective function over each parameter 
with others fixed, and cycles repeatedly until convergence. 

\item Due to highly efficient updates and techniques such as warm 
starts, the algorithms can compute the 
solution path very fast.

\item The code can handle sparse input-matrix formats, as well as range constraints on coefficients. 

\item The core of {\tt glmnet} is a set of Fortran subroutines, which make for very fast execution.

\item The package also includes methods for prediction and plotting, and a function that performs $k$-fold cross-validation.
\ei 
}

\practice{
	\begin{itemize}
		\item Prostate data: Lasso with {\tt glmnet}.
		\item To scale or not to scale?
		\item Use the R script {\tt prostate.lasso.R}.		
		\item See the {\em Glmnet vignette}, \citeN{glmnet_vignette_2014}.
	\end{itemize}
}

\subsection{Conclusions}
\frame{\frametitle{Concluding remarks on Lasso}

\bi 
\item Lasso ($L_1$ penalty) offers a way to simultaneously 
select variables and estimate the coefficients in generalized linear models (and more).
\item Newly developed computational algorithms allow application of these models to large data sets, with both $n$ and $p$ large, particularly when $p\gg n$.
\item There is a very active research on the statistical properties of Lasso.
\item The package {\tt glmnet} in R is an efficient implementation of Lasso.
\ei 
}


\section[Lasso in the GLM]{Lasso estimation in the GLM}

\frame{\frametitle{Lasso estimation in the GLM}
	\bi
	\item Let $(Y_i,\bx_i)$, $i=1,\ldots,n$, following a GLM corresponding to the parametric model $f(y,\theta)$. 
	\item Let $\ell(\theta_i,y_i) = \log f(y_i,\theta_i)$ be log-likelihood contribution for one observation.
	\item Assume that there is a one-to-one relationship between $\theta_i$ and the linear term $\beta_0 + \bx_i\tr \beta$:
	\centerline{$\theta_i=k(\beta_0 + \bx_i\tr \beta)$.} 
	\item Let $\ell(\beta_0,\beta,y_i,x_i)=\ell(k(\beta_0 + x_i\tr \beta),y_i)$.
	\item The Lasso estimation of the GLM solves the problem 
	$$
	\min_{\beta_0,\beta} \frac{1}{n} \sum_{i=1}^{n} -2\ell(\beta_0,\beta,y_i,x_i) + 
	\lambda ||\beta||_1.
	$$
	%\item The tuning parameter $\lambda$ controls the overall strength of the penalty.
	\ei 
}

\subsection{Preliminaries on maximum likelihood estimation (MLE)}
\frame{\frametitle{Preliminaries on maximum likelihood estimation}
\bi 	
\item Let $\bX=(X_1,\ldots,X_n)$ be i.i.d. distributed as $X$, a random variable with density (probability) function $f(\bx|\theta)$,
	$\theta=(\theta_1,\ldots,\theta_k)\in
	\Theta \subseteq \R^k$. 
\item Let $\calX$ be the sampling space, that is, the set of possible values of $\bX$.
	
\item The \blue{\bf likelihood function} for $\bx=(x_1,\ldots,x_n)\in \calX$ is defined as 
	$$
	\begin{array}{rccl}
	L(\cdot|\bx):& \Theta & \flecha & \R^+ \\
	& \theta & \flecha & L(\theta|\bx)=f(\bx|\theta)=\prodn
	f(x_i|\theta) 
	\end{array}
	$$
\item \blue{\bf Score vector}, gradient of the log-likelihood:  
$S(\bx,\btheta)=\grad_{\btheta} \log L(\btheta|\bx)$.
\item \blue{\bf Observed Information matrix}, minus the Hessian matrix of the log-likelihood:
$O(\bx, \btheta)=-H_{\btheta}\log L(\btheta|\bx)$.
\item \blue{\bf Fisher Information matrix}, the expected value of the observed information:
$I(\btheta)=-\E(H_{\btheta}\log L(\btheta|\bX))$.
\ei 
}

\frame{
\bi 
\item Under regularity conditions,
\bi 
\item $\E[S(\bX,\btheta)]={\bf 0}$.
\item $\mbox{Var}[S(\bX,\btheta)] =  \E[S(\bX,\btheta) S(\bX,\btheta)\tr]=I(\btheta)$.
\ei
\item Maximum likelihood estimator (MLE): 
$\hat{\btheta}_{\mathrm{MLE}} = \arg \max_{\btheta\in\Theta} \log L(\btheta|\bx)$. 
\item \blue{\bf Asymptotic properties of MLE:}
Let $\btheta_0$ be the true parameter value. Under regularity conditions,
\bi 
\item $\hat{\btheta}_{\mathrm {MLE} } {\xrightarrow {\text{p}}}\ \btheta _{0}$,
\item $\sqrt{n} \big( \hat{\btheta}_{\mathrm {MLE} }-\btheta _{0} \big) \ \xrightarrow{d} \ N(0,\,I(\btheta)^{-1})$,
\ei 
as $n$ goes to infinity.
\ei 
}



\subsection{Preliminaries on Newton-Raphson method}
\frame{\frametitle{Preliminaries on Newton-Raphson method}
\bi 
\item The Newton-Raphson method is an iterative procedure providing a sequence $\{\bx^k\}_{k\ge 1}$ that, under quite general assumptions, converges to the minimum $\bx^* = \arg\min_{\bx\in \R^q} f(\bx)$.

\item When starting the step $k+1$ of the algorithm, the last available value is $\bx^k$.

\item In order to obtain the next value $\bx^{k+1}$, the Newton-Raphson algorithm uses the second order Taylor's approximation of $f(\bx)$ around $\bx^k$:
$$
f(\bx)\approx f(\bx^k)+ (\bx-\bx^k)\tr \grad f(\bx^k) + \frac{1}{2}(\bx-\bx^k)\tr H f(\bx^k) (\bx-\bx^k).
$$
\ei
}
\frame{ 
\bi 
\item Let $\tilde{f}^k(\bx)$ be the right hand side approximating quadratic function:
$$
\tilde{f}^k(\bx) = f(\bx^k)+ (\bx-\bx^k)\tr \grad f(\bx^k) + \frac{1}{2}(\bx-\bx^k)\tr H f(\bx^k) (\bx-\bx^k).
$$
\item It is possible to minimize $\tilde{f}^k(\bx)$ analytically: 
\bi 
\item Its gradient is $\grad \tilde{f}^k(\bx)=\grad f(\bx^k) + H f(\bx^k) (\bx-\bx^k)$.
\item We solve the equation $\grad \tilde{f}^k(\bx)={\bf 0}$ and call the solution $\bx^{k+1}$:
$$
\grad f(\bx^k) + H f(\bx^k) (\bx^{k+1}-\bx^k)={\bf 0}\then
\bx^{k+1}=\bx^k-\left(H f(\bx^k) \right)^{-1}\grad f(\bx^k).
$$
\ei 
\item This point $\bx^{k+1}$ is the minimum of $\tilde{f}^k(\bx)$ if 
$H f(\bx^k)$ is positive definite because 
$$
H\tilde{f}^k(\bx^k)=H f(\bx^k).
$$
\item This will be the case when $\bx^k$ is close to the global minimum of $f$ and this function has continuous second derivatives. 
\ei 
}

\frame{{Newton-Raphson Method}
\bi 
\item The recursive formula giving $\bx^{k+1}$ from $\bx^k$ is 
$$
\bx^{k+1}=\bx^k-\left(H f(\bx^k) \right)^{-1}\grad f(\bx^k).
$$	

\item The algorithm iterates until convergence:
it stops if $\|\bx^{k+1}-\bx^k\|$ or $|f(\bx^{k+1})-f(\bx^k)|$ are small.

\item A sufficient condition for convergence of $\{\bx^k\}_{k\ge 1}$ to the global minimum 
$\bx^*=\arg\min_{\bx} f(\bx)$ is that $f$ is a convex function. 
%\item We will use the Newton-Raphson algorithm (and some variants of it) to \blue{maximum likelihood estimation}.
\item {\bf Maximum likelihood estimation:}
\begin{description}
\item[Newton-Raphson algorithm:] $\btheta^{k+1} = \btheta^k + O(\bx, \btheta)^{-1} S(\bx_i, \btheta^k)$.  
\item[Fisher Scoring algorithm:] \hspace{1em} $\btheta^{k+1} = \btheta^k + I(\btheta^k)^{-1} S(\bx_i, \btheta^k)$.
\end{description}
\item Both methods coincide for exponential families, because the observed information matrix does not depend on the observed data.
\ei 
}



\subsection{IRWLS for MLE in logistic regression}
\frame{\frametitle{IRWLS for MLE in logistic regression}

Let $(Y_i,x_i)$, $i=1,\ldots,n$, $x_i\in \R$ known constant values and  
$Y_1,\ldots,Y_n$ independent random variables 
$$
Y_i\sim \mbox{Bernoulli}(p_i), \, i=1,\ldots,n.
$$
Assume that for all $i=1,\ldots,n$ 
$$
p_i=\frac{e^{\beta_0+\beta_1 x_i}}{1+e^{\beta_0+\beta_1 x_i}} 
\Leftrightarrow \log\frac{p_i}{1-p_i}=\beta_0+\beta_1 x_i.
$$
Remember that $E(Y_i)=p_i$, $\mbox{Var}(Y_i)=p_i(1-p_i)$.

So we have a \blue{generalized linear model}.

The link function is the logistic function:
$$ 
g(p)=\log\frac{p_i}{1-p_i} \Leftrightarrow 
g^{-1}(v)=\frac{e^{v}}{1+e^{v}}.
$$
}

\frame{\frametitle{Logistic regression: Likelihood function}

When $(y_i,x_i)$, $i=1,\ldots,n$, are observed the likelihood function is 
$$
L(\beta_0,\beta_1)=\Pr(Y_1=y_1,\ldots,Y_n=y_n) = \prod_{i=1}^n \Pr(Y_i=y_i)
= \prod_{i=1}^n p_i^{y_i} (1-p_i)^{(1-y_i)}
$$
with logarithm
$$
\ll(\beta_0,\beta_1) = \sum_{i=1}^n \left(y_i \log \frac{p_i}{1-p_i}  + \log (1-p_i)\right)
$$
$$
=
\sum_{i=1}^n y_i(\beta_0+\beta_1 x_i) - \sum_{i=1}^n \log\left(1+e^{\beta_0+\beta_1 x_i}\right).
$$
}

\frame{\frametitle{Logistic regression: Score function}
$$
\ll(\beta_0,\beta_1)=
\sum_{i=1}^n y_i(\beta_0+\beta_1 x_i) - \sum_{i=1}^n \log\left(1+e^{\beta_0+\beta_1 x_i}\right).
$$
$$
\frac{\partial \ll(\beta_0,\beta_1)}{\partial \beta_0}= \sum_{i=1}^n y_i - 
\sum_{i=1}^{n} \frac{e^{\beta_0+\beta_1 x_i}}{1+e^{\beta_0+\beta_1 x_i}} =
\sum_{i=1}^{n} (y_i-p_i)
$$
$$
\frac{\partial \ll(\beta_0,\beta_1)}{\partial \beta_1}= \sum_{i=1}^n y_i x_i - 
\sum_{i=1}^{n} \frac{e^{\beta_0+\beta_1 x_i}}{1+e^{\beta_0+\beta_1 x_i}} x_i =
\sum_{i=1}^{n} (y_i-p_i)x_i
$$
So,
$$
S(\beta_0,\beta_1,\by) =\grad \ll(\beta_0,\beta_1) = \bX^t (\by - \bp)
$$
where 
$$
\bX^t= \left( \begin{array}{ccc}
1 & \cdots & 1 \\
x_1 & \cdots & x_n
\end{array}\right),
$$
$\by$ and $\bp$ are column vectors.
}

\frame{\frametitle{Logistic regression: Fisher's Information; Fisher Scoring}
$$
S(\beta_0,\beta_1,\bY)=\bX^t (\bY - \bp)
$$
$$
I(\beta_0,\beta_1) = \mbox{Var}(S(\beta_0,\beta_1,\bY)) = \bX^t \mbox{Var}(\bY) \bX 
= \bX^t \bW \bX
$$
where
$$
\bW= \mbox{diag}(p_1(1-p_1),\ldots, p_n(1-p_n)).
$$
Generic iteration in the Fisher Scoring algorithm:
$$
{\beta_0^{m+1} \choose \beta_1^{m+1}} =
{\beta_0^{m} \choose \beta_1^{m}} + I(\beta_0^m,\beta_1^m)^{-1} 
\grad \ll(\beta_0^m,\beta_1^m)
= {\beta_0^{m} \choose \beta_1^{m}} + (\bX^t \bW \bX)^{-1}  \bX^t (\by - \bp)
$$
$$
= {\beta_0^{m} \choose \beta_1^{m}} + 
(\bX^t \bW \bX)^{-1}  \bX^t \bW \left(\bW^{-1}(\by - \bp)\right)
$$
}

\frame{
$$
{\beta_0^{m+1} \choose \beta_1^{m+1}} 
= {\beta_0^{m} \choose \beta_1^{m}} + 
(\bX^t \bW \bX)^{-1}  \bX^t \bW \left(\bW^{-1}(\by - \bp)\right)
$$
$$
= (\bX^t \bW \bX)^{-1} (\bX^t \bW \bX) {\beta_0^{m} \choose \beta_1^{m}} + 
(\bX^t \bW \bX)^{-1}  \bX^t \bW \left(\bW^{-1}(\by - \bp)\right)
$$
$$
= (\bX^t \bW \bX)^{-1} \bX^t \bW \left( \bX{\beta_0^{m} \choose \beta_1^{m}} + 
\left(\bW^{-1}(\by - \bp)\right)\right)
$$
$$
= (\bX^t \bW \bX)^{-1} \bX^t \bW \bZ,
$$
where $\bZ$ is the $n\times 1$ vector of pseudo-observations, with $i$-th element
$$
z_i = \beta_0^m + \beta_1^m x_ i + \frac{y_i-p_i}{p_i(1-p_i)}.
$$
}

\frame{
$$
{\beta_0^{m+1} \choose \beta_1^{m+1}} 
= (\bX^t \bW \bX)^{-1} \bX^t \bW \bZ.
$$
Observe that ${\beta_0^{m+1} \choose \beta_1^{m+1}}$ is the weighted least squares (WLS) coefficients estimator in the simple linear regression  

\centerline{\blue{with data
$(x_i,z_i)$ and weights $p_i(1-p_i)$, $i=1,\ldots,n$.}
}

Taking into account that these data come from the model
$$
Z_i = \beta_0^m + \beta_1^m x_ i + \frac{Y_i-p_i}{p_i(1-p_i)},
$$
and calling $\varepsilon_i=\frac{Y_i-p_i}{p_i(1-p_i)}$, it follows that 
$$
E(\varepsilon_i)=0,\, \mbox{Var}(\varepsilon_i)=\frac{1}{p_i(1-p_i)}.
$$
So the weight of each case $(x_i,z_i)$ is equal to the inverse of the error $\varepsilon_i$ variance.
}

%\subsection{IRWLS for logistic regression}
\frame{%\frametitle{IRWLS for logistic regression} 
	{\small \bf Iteratively re-weighted least squares algorithm (IRWLS) for logistic regression.}
	\begin{itemize}
		\item Choose starting values $\beta^0=(\beta_0^0,\beta_1^0)$ 
		(the choice $\beta_0^0=\beta_1^0=0$ is usually appropriate; choosing the OLS estimates is also possible).
		\item Set $m=0$ and iterate the following steps until convergence.
		\begin{enumerate}
			\item Set 
			$$
			p^m_i={e^{\beta_0^m + \beta_1^m x_i} \over 1+ e^{\beta_0^m + \beta_1^m x_i}},
			$$
			$$
			z^m_i= \beta_0^m + \beta_1^m x_i + {y_i -  p^m_i \over p^m_i(1-p^m_i)}, \; i=1,\ldots,n.
			$$
			\item Let $(\nu^m_{1},\dots,\nu^m_{n})$ be the weight vector 
			with $\nu^m_i = p^m_i(1-p^m_i)$.
			\item Fit the linear regression with responses $z^m_i$ and explanatory variable values $x_i$, (plus the constant term) by 
			weighted least squares 
			using the weights $\nu^m_i$,  $i=1,\ldots,n$. 
			\\
			Let $\beta^{m+1}=(\beta_0^{m+1},\beta_1^{m+1})$ be the estimated regression coefficients.
			\item Set $m=m+1$ and go back to the step 1.
		\end{enumerate}
		\ei 
	}

\frame{\frametitle{Logistic regression: Variance of the MLE}
We know that 
$$
\mbox{Var}\left( {\hat{\beta}_0^{ML} \choose \hat{\beta}_1^{ML}} \right) 
\approx I(\hat{\beta}_0^{ML}, \hat{\beta}_1^{ML})^{-1} =
\left( \bX^t \bW_{\hat{\beta}_0^{ML}, \hat{\beta}_1^{ML}} \bX\right)^{-1}.
$$
On the other hand,
$$
\mbox{Var}\left({\beta_0^{m+1} \choose \beta_1^{m+1}}\right) 
= \mbox{Var}\left((\bX^t \bW \bX)^{-1} \bX^t \bW \bZ\right)
$$
$$
=
(\bX^t \bW \bX)^{-1} \bX^t \bW \, \mbox{Var}(\bZ) \, \bW \bX  (\bX^t \bW \bX)^{-1}
$$
$$
=
(\bX^t \bW \bX)^{-1} \bX^t \bW \bW^{-1} \bW \bX  (\bX^t \bW \bX)^{-1}
$$
$$
=
(\bX^t \bW \bX)^{-1} \bX^t \bW \bX  (\bX^t \bW \bX)^{-1}=
(\bX^t \bW \bX)^{-1}.
$$
\blue{Conclusion:} We can estimate the variance of the $(\beta_0,\beta_1)$ MLE by the coefficients variance of the last WLS estimator.
}

\practice{
Follow the R Markdown file {\tt IRWLS\_logistic.Rmd}.
}

\subsection{Revisiting the IRWLS version of Newton-Raphson for GLM}
\frame{{\small Revisiting the IRWLS version of Newton-Raphson (or Fisher scoring) for GLM}
\bi
\item The key-point in the IRWLS version of Newton-Raphson (or Fisher scoring) for GLM is that at each step $m$ of the algorithm, the following two elements coincide:
\bi 
\item The point $\theta_{m+1}^{\mbox{\tiny NR}}$ maximizing the quadratic function $\tilde{\ell}_m(\theta)$ that approximates the log-likelihood function $\ell(\theta)$ around $\theta_m$ 
\\ (or minimizing \red{$-2\tilde{\ell}_m(\theta)$}). 
\item The point $\theta_{m+1}^{\mbox{\tiny WLS}}$ minimizing the \blue{weighted sum of squared residuals} in the $m$-th regression problem.
\ei 
\item Do these two functions share just the location of their minimums? Or are they equal up to an additive constant?
\ei
\vspace*{-.75cm} 
\centerline{
\includegraphics[scale=.5]{graf_IRWLS_GLM_Lasso.pdf}
}

\vspace*{-1.35cm} 
We will see that the latter happens.
}

\frame{{\small Revisiting the IRWLS version of Newton-Raphson (or Fisher scoring) for GLM}
\bi 
\item The GLM assumes that $f(y,\theta)$ is in the exponential family:
$$
f(y,\theta)=h(y)c(\theta)\exp\left(\eta(\theta) t(y)\right).
$$
\item Let us assume additionally that this family is parameterized in natural form ($\eta(\theta)=\theta$), and that the function $t(y_i)=y_i$
(that is, the sample mean is a sufficient statistic): 
$$
f(y,\theta)=h(y)c(\theta)\exp\left(\theta y\right),
\,\,
\ell(\theta,y)=\log h(y) + \log c(\theta) + \theta y.
$$
\item The score function is
$$
S(\theta,Y)=\parcial{\ell(\theta,y)}{\theta}{}=\parcial{}{\theta}{}\log c(\theta) + Y
$$
\item As $E_{\theta}(S(\theta,Y))=0$, it follows that $\mu = E_{\theta}(Y)=-\parcial{}{\theta}{}\log c(\theta)$.
\item Then $S(\theta,Y)= Y - \mu$.
\item Then $V_{\theta}(Y)=V_{\theta}(S(\theta,Y))=-\parcial{}{\theta}{2}\log c(\theta)$.
\ei 		
}

\frame{
	\bi 
	\item Consider again $(Y_i,\bx_i)$, $i=1,\ldots,n$, from this GLM model, with parameters  
	$\theta_i=k(\beta_0 + \bx_i\tr \beta)$, respectively, and 
	$\ell(\beta_0, \beta,y_i,x_i)=\ell(k(\beta_0 + x_i\tr \beta),y_i)$.
	\item Then, applying the chain rule, 
	$$
	\grad_{\beta_0,\beta} \ell(\beta_0, \beta,y_i,x_i)=
	\left.\parcial{\ell(\theta,y_i)}{\theta}{}\right|_{\theta=\theta_i} 
	\grad_{\beta_0,\beta} k(\beta_0 + \bx_i\tr \beta) =
	$$
	$$
	S(\theta_i,y_i) k'(\beta_0 + \bx_i\tr \beta) \tilde{\bx}_i=
	(y_i-\mu_i)k'(\beta_0 + \bx_i\tr \beta) \tilde{\bx}_i ,
	$$
	where $\tilde{\bx}_i\tr = (1,\bx_i\tr)$. 
	\item Let $\ell(\beta_0,\beta,\by,\bX) = \sum_{i=1}^n \ell(\beta_0, \beta,y_i,x_i)$ be the full likelihood. Then
	$$
	\grad_{\beta_0,\beta} \ell(\beta_0, \beta,\by,\bX)=
	\sum_{i=1}^n (y_i-\mu_i)k'(\beta_0 + \bx_i\tr \beta) \tilde{\bx}_i = 
	\bX\tr \bK (\by - \bmu).
	$$
	where $\bK=\diag(k'(\beta_0 + \bx_i\tr \beta),i=1,\ldots,n)$, and $\bX$ is the matrix with rows $\tilde{\bx}_i\tr$.
	\ei 
}

\frame{ 
	\bi 
	\item The Fisher's Information matrix is
	$$
	I(\beta_0,\beta) = V(\grad_{\beta_0,\beta} \ell(\beta_0, \beta,\by,\bX))=
	V(\bX\tr K (\by - \bmu)) =  \bX\tr \bK \, V(\by) \bK \bX. 
	$$ 
	\item Observe that $ V(\by)=\bD=
	\diag\left(-\left. \parcial{\log c(\theta)}{\theta}{2}\right|_{\theta=\theta_i},i=1,\ldots,n \right)$.
	\item Then
	$I(\beta_0,\beta) = \bX\tr \bW \bX$ with 
	$$
	\bW=\bK\bD\bK=\diag
	\left(-\left. k'(\beta_0 + \bx_i\tr \beta)^2 \parcial{\log c(\theta)}{\theta}{2}\right|_{\theta=\theta_i},i=1,\ldots,n \right).
	$$
	\item Remember that, in exponential families, the Hessian matrix of $\ell(\beta_0, \beta,\by,\bX)$ with respect to $(\beta_0,\beta)$ is $-I(\beta_0,\beta)$.
	\ei 
}

\frame{ 
	\bi 
	\item Let $\bbeta$ be the column vector with components $\beta_0$ and $\beta$. 
	\item In the iteration $m$ of the Newton-Raphson algorithm to minimize 
	$-2\ell(\bbeta)$, the second order Taylor expansion around $\bbeta^m$ is
	$$\!\!\!\!\!\!\!\!
	-2 \tilde{\ell}_m(\bbeta)=
	-2\ell(\bbeta^m) -2 (\bbeta-\bbeta^m)\tr \bX\tr \bK (\by - \bmu) + 
	(\bbeta-\bbeta^m)\tr\bX\tr \bW \bX(\bbeta-\bbeta^m)=
	$$
	$$
	-2\ell(\bbeta^m) -2 (\bbeta-\bbeta^m)\tr \bX\tr \bW\,  \bW^{-1} \bK (\by - \bmu) + 
	(\bbeta-\bbeta^m)\tr\bX\tr \bW \bX(\bbeta-\bbeta^m)=
	$$
	$$
	-2\ell(\bbeta^m) -2 (\bbeta-\bbeta^m)\tr \bX\tr \bW  \bepsilon + 
	(\bbeta-\bbeta^m)\tr\bX\tr \bW \bX(\bbeta-\bbeta^m)=
	$$
	$$
	\gamma - 2 \bbeta\tr \bX\tr \bW \bZ +  \bbeta\tr \bX\tr \bW \bX \bbeta,
	$$
where $\gamma$ is a constant term that does not depend on $\bbeta$,  
$$
\bepsilon =\bW^{-1} \bK (\by - \bmu)= \bK^{-1} \bD^{-1} \bK^{-1} \bK(\by - \bmu)= 
\bK^{-1} \bD^{-1}(\by - \bmu)
$$
and $\bZ = \bX\bbeta^m + \bepsilon$. 
	\ei 
}

\frame{ 
	\bi 
	\item Consider now the Weighted Least Square (WLS) problem where the response variable is contained in the vector $\bZ$ defined before, the matrix of explanatory variables is $\bX$ and the weights are given by the diagonal matrix $\bW$. 
	\item The objective function to be minimized in the WLS estimation is 
	$$
	Q_m(\bbeta)=(\bZ-\bX\bbeta)\tr \bW (\bZ-\bX\bbeta) = 
	\bZ\tr \bZ - 2 \bbeta\tr \bX\tr \bW \bZ +  \bbeta\tr \bX\tr \bW \bX \bbeta.
	$$ 
	\item Observe that $\kappa=-2\tilde{\ell}_m(\bbeta) - Q_m(\bbeta)$ is constant in $\bbeta$. 
	So the value $\bbeta^{m+1}$ minimizing  $Q_m(\bbeta)$ also minimizes  $-2\tilde{\ell}_m(\bbeta)$. 
	\item This is the key point in the IRWLS version of the Newton-Raphson algorithm.
	\item \blue{When we introduced the IRWLS for the first time, we only shown that 
		the value $\bbeta^{m+1}$ minimizing  $Q_m(\bbeta)$ also minimizes  $-2\tilde{\ell}_m(\bbeta)$.}
	\item \blue{Now we have seen that the functions  $-2\tilde{\ell}_m(\bbeta)$ and $Q_m(\bbeta)$ are equal up to an additive constant.}
	\item \blue{This fact will be crucial for the Lasso estimation of the GLM.}
	\ei 
}

\frame{{IRWLS version of Newton-Raphson for GLM}
\centerline{\blue{Problem: $\min_{\bbeta} -2\ell(\bbeta)$}}
\begin{itemize}
\item[Step 0:] Take $\bbeta^0$ arbitrarily (at random, with all components equal to zero, as the OLS estimator, ...). Let $m=0$. 
\item[Step 1:] Approximate the objective function using the second order Taylor approximation of $\ell(\bbeta)$ around $\bbeta^m$ and express the approximation in terms of a weighted sum of squares regression errors:
$$
\blue{
-2\ell(\bbeta) \approx -2\tilde{\ell}_m(\bbeta) = \kappa + Q_m(\bbeta).}
$$
\item[Step 2:] Solve the problem \blue{$\min_{\bbeta} Q_m(\bbeta)$} by WLS. 
Let  $\bbeta_{m+1}$ be the optimum.
\item[Step 3:] Stop if $\|\bbeta^{m+1}-\bbeta^m\|$ or $|\ell(\bbeta^{m+1})-\ell(\bbeta^m)|$ are small, or if the maximum number of iterations is reached. 
\\
Otherwise let $m=m+1$ and go to \blue{Step 1}.
\end{itemize} 
}



\subsection[IRW Lasso]{Iterative Re-Weighted Lasso estimation in the GLM}

\frame{\frametitle{Lasso estimation in the GLM}
\bi
	\item The Lasso estimation of the GLM solves the problem 
	$$
\blue{ 
	\min_{\beta_0,\beta} \frac{1}{n} \sum_{i=1}^{n} -2\ell(\beta_0,\beta,y_i,x_i) + 
	\lambda ||\beta||_1
}
	\equiv 
\blue{\min_{\bbeta} -\frac{2}{n}\ell(\bbeta) 
	+ \lambda \|\beta\|_1
}
	$$
where $\bbeta\tr=(\beta_0,\beta\tr)$.

\item The way this problem is solved in the R library {\tt glmnet} 
(see \citeNP[Chapter 5]{HasTibWai:2015})
is a modified version of the IRWLS.

\item The same approximation of $-2\ell(\bbeta)$ by a quadratic function is done.

\item Strictly speaking, the proposal is not a Newton-Raphson algorithm.
\ei 
}

\frame{{Iterative Re-Weighted Lasso estimation in the GLM}
	\centerline{
\blue{Problem: $\min_{\bbeta} -\frac{2}{n}\ell(\bbeta) 
	+ \lambda \|\beta\|_1$}}
	\begin{itemize}
		\item[Step 0:] Take $\bbeta^0$ arbitrarily (at random, with all components equal to zero, as the OLS estimator, ...). Let $m=0$. 
		\item[Step 1:] Approximate the objective function using the second order Taylor approximation of $\ell(\bbeta)$ around $\bbeta^m$ and express the approximation in terms of a weighted sum of squares regression errors:
		$$%\hspace*{-1cm}
		\blue{-\frac{2}{n}\ell(\bbeta) + \lambda \|\beta\|_1 \approx -\frac{2}{n}\tilde{\ell}_m(\bbeta) + \lambda \|\beta\|_1 =
		 \frac{\kappa}{n} + \frac{1}{n}Q_m(\bbeta) + \lambda \|\beta\|_1}.
		$$
		\item[Step 2:] Solve the problem 
		\blue{$\min_{\bbeta} \frac{1}{n} Q_m(\bbeta)+ \lambda \|\beta\|_1$} 
		by weighted Lasso (standard coordinate descend). 
		Let  $\bbeta_{m+1}$ be the optimum.
		\item[Step 3:] Stop if $\|\bbeta^{m+1}-\bbeta^m\|$ or $|\ell(\bbeta^{m+1})-\ell(\bbeta^m)|$ are small, or if the maximum number of iterations is reached. 
		\\
		Otherwise let $m=m+1$ and go to \blue{Step 1}.
	\end{itemize} 
\label{last_page}
}

%  Bibliografía


\bibliographystyle{chicago}
%\bibliographystyle{e:/latex/chicago_esp}
%\bibliographystyle{g:/latex/chicago_esp}
%\bibliographystyle{c:/pedro/latex/chicago_esp}
{\footnotesize 
%\bibliography{BD_lasso,StatisticalLearning,DataMining}
\bibliography{StatisticalLearning} %,DataMining}
}


%\end{document}

\section*{Appendix: Statistical properties of Lasso}
\frame{ \frametitle{Appendix: Statistical properties of Lasso}
	{\scriptsize \blue{(Based on Bülhmann's comments to \citeNP{tibshirani:2011:lasso_retosp}. 
			See also Chapters 6 and 11 of \citeNP{HasTibWai:2015} or the book
			\citeNP{BulGer:2011})}
	}
	
	Consider a potentially high dimensional linear model:
	$$
	\by =\bX \bbeta + \varepsilon,\; \bX_{n\times p}, \; p=p_n\gg n 
	\mbox{ as } n\tiende \infty.
	$$
	
	Four problems have received much attention: 
	\bi 
	\item Prediction and estimation of the regression surface $\bX \bbeta$.
	\item Estimation of parameters $\bbeta$.
	\item Variable screening or {\em Sparsistency}.
	\item P-values for high-dimensional linear models.
	\ei 
}

\frame{{Prediction and estimation of the regression surface}
	\bi 
	\item For fixed design, under no assumptions on $\bX$ and mild conditions on $\varepsilon$, it can be proved that 
	$$
	{1\over n} \|\bX (\hat{\bbeta}_{\mbox{\tiny Lasso}} - \bbeta)\|_2^2 \le 
	\|\bbeta\|_1 O_P (\sqrt{\log p/n} ).
	$$
	
	\item Achieving a faster rate of convergence for prediction 
	%and estimation of the parameter vector, 
	requires a design condition such as the \blue{restricted $\ell_1$-eigenvalue assumption}:
	$$
	\frac{\frac{1}{n} \nu \bX\tr \bX \nu\tr}{\|\nu\|^2_{\ell_2}} \ge \gamma \text{ for all nonzero }\nu \in \mathcal{C}(S_0,3),
	$$
	for $\gamma>0$, where $S_0=\{j:\beta_j\ne 0\}$ is the \blue{active variables set} and 
	$$
	\mathcal{C}(S_0,\alpha)=\{\nu\in \R^p: \|\nu_{S_0^c}\|_{\ell_1} \le \alpha \|\nu_{S_0}\|_{\ell_1}\}.
	$$
	\ei 
}

\frame{ \frametitle{Estimation of parameters $\bbeta$}
	\bi 
	\item Active variables set: $S_0=\{j:\beta_j\ne 0\}$, $s_0 = |S_0|$.
	
	\item Under the \blue{restricted $\ell_1$-eigenvalue assumption},
	\citeN{BulGer:2011} prove that, with high probability, 
	$$
	\|\hat{\bbeta} -\bbeta\|_1 \le O_P(s_0 \sqrt{\log p/n}).
	$$
	
	\item Then $\bbeta$ is identifiable if $s_0 \le \sqrt{n/\log p}$, 
	that is, if the true model is \blue{sparse}. 
	\ei 
}


\frame{ \frametitle{Variable screening or Sparsistency} 
	\bi 
	\item Active variables set: $S_0=\{j:\beta_j\ne 0\}$. 
	Let $\hat{S}=\{j:\hat{\beta}^{\mbox{\tiny Lasso}}_j\ne 0\}$. 
	
	\item In order to have asymptotically perfect variable selection, 
	$$
	\lim_n \Pr(\hat{S}=S_0)=1,
	$$
	some restrictive (and rather unlikely to hold in practice!) assumptions must be made, that are sufficient and (essentially) necessary.
	
	\item What happens with high probability under no such restrictive conditions is that 
	$$
	\lim_n \Pr(\hat{S} \supseteq S_{\mbox{\tiny relev}})=1,
	$$
	where $S_{\mbox{\tiny relev}}$ is the set of coefficients that are {\em relevant} in the sense that they are far from 0.
	
	\item This result is still valid when the $\lambda$ (or $t$) is chosen by CV. 
	\ei 
}


\frame{ \frametitle{P-values for high-dimensional linear models}	
	
	\bi 
	\item Asymptotic distribution of Lasso estimators has a point mass at zero.
	
	\item Standard bootstrap cannot be used.
	
	\item Peter Bülhmann and co-authors propose \blue{de-sparsifying} the Lasso estimator.  They prove the asymptotic normality of the de-sparsified estiamtors.
	
	\item Finally, \citeN{lockhart2014significance} test the significance
	of the predictor variable that enters the current Lasso model, in the sequence
	of models visited along the Lasso solution path. 
	\ei 
}


%Extensions and other problems
\frame{{Lasso: A very active research area}
	\hspace*{-1cm}
	%\begin{center}
	%\fbox{
	\includegraphics[scale=.9,viewport=
	45 464 445 643,clip]{Tibshirani_2011_JRSS_lasso_retr_page_3.pdf}
	%}
	%\end{center}
	
	\vspace*{-.1cm}
	{\tiny Source: \citeN{tibshirani:2011:lasso_retosp}}
}


\end{document}
