\documentclass[11pt]{report}
\usepackage{amsbsy}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{times}
\usepackage{appendix}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{fancyvrb} 
\usepackage{subfig}
\usepackage[a4paper]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc} 
\usepackage[colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black]{hyperref}
\usepackage{tikz}
\usepackage{placeins}
\usepackage{listings}
\usepackage{bm} 
\usepackage{xcolor} 
%\usepackage[square,numbers]{natbib}
\usepackage{chicago}
\usepackage{caption}
\usepackage{float}
\usepackage{cleveref}
\usepackage[nottoc]{tocbibind}
\usepackage{algorithm2e}
\SetKwComment{Comment}{$\triangleright$\ }{}
\usepackage{longtable}
\usetikzlibrary{shapes,arrows}
\DeclareMathOperator{\Tr}{Trace}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\cummds}{cum-mds}
\DeclareMathOperator{\eigen}{eigen}

\xdefinecolor{gray}{rgb}{0.4,0.4,0.4} 
\xdefinecolor{blue}{RGB}{58,95,205}% R's royalblue3; #3A5FCD 

\lstset{% setup listings 
        language=R,% set programming language 
        basicstyle=\ttfamily\small,% basic font style 
        keywordstyle=\color{blue},% keyword style 
        commentstyle=\color{gray},% comment style 
        numbers=left,% display line numbers on the left side 
        numberstyle=\scriptsize,% use small line numbers 
        numbersep=10pt,% space between line numbers and code 
        tabsize=3,% sizes of tabs 
        showstringspaces=false,% do not replace spaces in strings by a certain character 
        captionpos=b,% positioning of the caption below 
        breaklines=true,% automatic line breaking 
        escapeinside={(*}{*)},% escaping to LaTeX 
        fancyvrb=true,% verbatim code is typset by listings 
        extendedchars=false,% prohibit extended chars (chars of codes 128--255) 
        literate={"}{{\texttt{"}}}1{<-}{{$\bm\leftarrow$}}1{<<-}{{$\bm\twoheadleftarrow$}}1 
        {~}{{$\bm\sim$}}1{<=}{{$\bm\le$}}1{>=}{{$\bm\ge$}}1{!=}{{$\bm\neq$}}1{^}{{$^{\bm\wedge}$}}1,% item to replace, text, length of chars 
        alsoletter={.<-},% becomes a letter 
        alsoother={$},% becomes other 
        otherkeywords={!=, ~, $, \&, \%/\%, \%*\%, \%\%, <-, <<-, /},% other keywords 
        deletekeywords={c}% remove keywords 
} 


\begin{document}
\SweaveOpts{concordance=TRUE}



%%%   PORTADA  %%%

\begin{titlepage}
  \begin{center}
	  \textsf{\scshape\LARGE Universitat Politècnica de Catalunya\\[0.5em]
	          Facultat de Matemàtiques i Estadística}\vskip 8em
	  {\LARGE Master's thesis \par} \vskip 8em
	  {\bfseries \huge Multidimensional Scaling for Big Data} \vskip 1.5em 
	  {\LARGE \lineskip .5em Cristian Pachón García \par} 
    \vskip 12em {\large Advisor: Pedro Delicado Useros \hskip 0.3em}
    \vfill {\large Dept. d'Estadística i Investigació Operativa }
  \end{center}
  \par
  \vskip 3.5em 
\end{titlepage}

\thispagestyle{empty}

%\begin{titlepage}
%\begin{center}
%\emph{}\\ [3cm]
%\rule{\textwidth}{2.5pt}
%\huge{\textbf{This is the tittle}}
%\rule{\textwidth}{2.5pt} \\ [6 cm]
%\large{Student: Cristian Pachon Garcia}\\[0.5 cm]
%\large{Director: Pedro Delicado}\\ [2.2 cm]
%\large{Master Thesis} \\ [0.3 cm]
%\large{(MESIO UPC-UB)} \\ [1.5 cm]
%\large{January 2019}
%\end{center}
%\end{titlepage}

\textwidth=6in
\textheight=9.2in
\oddsidemargin=0.3in
\evensidemargin=0.2in
\headheight=0.1in
\topmargin=-0.1in

\newcommand{\Robject}[1]{\texttt{#1}}
\newcommand{\Rpackage}[1]{\textsf{#1}}
\newcommand{\Rclass}[1]{\textit{#1}}
\newcommand{\R}{\textsf{R}}
\newcommand*{\h}{\hspace{5pt}}
\newcommand*{\hh}{\h\h}




%%%%%%%%%%%%%%%%%%%%%%%%


\pagebreak

\setcounter{page}{1}



%\title{This is the tittle}
%\vspace{1cm}

%\author{Student: 
%Cristian Pachon Garcia\\
%\texttt{cc.pachon@gmail.com}
%\and
%Director: Pedro Delicado\\
%\texttt{pedroemail@upc.edu}
%}





%\date{January 2019} 
%\title{How hard would it be to build a spaceship from scrap}
%\author{Carl Capybara\thanks{I never procrastinate} \and Walter Wombat}
%\subtitle{A closer look at the expenses}
%\subject{a funny paper}




\thispagestyle{empty}




%\maketitle

\clearpage\mbox{}\clearpage

\addcontentsline{toc}{chapter}{Abstract}
\noindent {\LARGE{\textbf{Abstract}}} \\
\indent We present a set of algorithms for \textit{Multidimensional Scaling} 
(MDS) to be used with large datasets. MDS is a statistic tool for reduction of 
dimensionality, using as input a distance matrix of dimensions 
$n \times n$. When $n$ is large, classical algorithms suffer from computational 
problems and MDS configuration can not be obtained.

\indent In this thesis we address these problems by means of three algorithms:
\textit{Divide and Conquer MDS, Fast MDS} and 
\textit{MDS based on Gower interpolation}. The main idea of these methods is
based on partitioning the dataset into small pieces, where classical 
methods can work.

\indent In order to check the performance of the algorithms as well as to 
compare them, we do a simulation study. This study
points out that \textit{Fast MDS} and \textit{MDS based on Gower interpolation} 
are appropriated to use when $n$ is large. Although 
\textit{Divide and Conquer MDS} is not as fast as the other two algorithms,
it is the best method that captures the variance of the original data. 


\clearpage\mbox{}\clearpage

\tableofcontents
\thispagestyle{empty}


\addtocontents{toc}{\protect\thispagestyle{empty}}
\chapter{Classical Multidimensional Scaling}
\section{Introduction to Multidimensional Scaling}
Multidimensional Scaling (MDS) is a family of methods that represents 
measurements of dissimilarity (or similarity) among pairs of objects as 
Euclidean distances between points of a low-dimensional space. The data, 
for example, may be correlations among intelligence tests and the MDS 
representation is a plane that shows the tests as points. The graphical display 
of the correlations provided by MDS enables the data analyst to literally 
``look" at the data and to explore their structure visually. This often shows 
regularities that remain hidden when studying arrays of numbers. 

\indent Given a square matrix \textbf{D} $n\times n$, the goal of MDS is to 
obtain a configuration matrix \textbf{X} $n \times p$ with orthogonal columns
that can be interpreted as the matrix of $p$ variables for the $n$ 
observations, where the Euclidean distance between the rows of \textbf{X} 
is approximately equal to \textbf{D}. The columns of \textbf{X} are called 
\textit{principal coordinates}.

\indent This approach arises two questions: is it (always) possible to find this
low-dimensional configuration \textbf{X}? How is it obtained? In general, it 
is not possible to find a set of $p$ variables that reproduces 
\textit{exactly} the initial distance. However, it is possible to find a set 
of variables which distance is approximately the initial distance matrix 
\textbf{D}.


\indent As a classical example, consider the distances between European cities 
as in Table \ref{european_distances}. One would like to get a 
representation in a 2-dimensional space such that the distances would be 
almost the same as in  Table \ref{european_distances}. The representation 
of these coordinates are displayed in Figure \ref{europ_cities}.

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
 & Athens & Barcelona & Brussels & Calais & Cherbourg & $\dotsi$ \\ 
  \hline
Athens & 0 & 3313 & 2963 & 3175 & 3339 & $\dotsi$ \\ 
  Barcelona & 3313 & 0& 1318 & 1326 & 1294 & $\dotsi$ \\ 
  Brussels & 2963 & 1318 & 0 & 204 & 583 & $\dotsi$ \\ 
  Calais & 3175 & 1326 & 204 & 0 & 460 & $\dotsi$ \\ 
  Cherbourg & 3339 & 1294 & 583 & 460 & 0 & $\dotsi$ \\
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ \\
   \hline
\end{tabular}
\caption{Distances between European cities (just 5 of them are shown).} 
\label{european_distances}
\end{table}


\begin{figure}[!ht]
\centering
    \includegraphics[scale = 0.5]{./images/europ_cities.png}
    \caption{MDS on the European cities.}
    \label{europ_cities}
\end{figure}


\section{Principal coordinates}
Given a matrix \textbf{X} $n \times p$, the matrix of $n$ 
individuals over $p$ variables, it is possible to obtain a new one with 
mean equal to 0 by column from the previous one:

\[
\mathbf{\widetilde{X}} = \Big( \mathbf{I} - \frac{1}{n} \mathbf{1}\mathbf{1'}\Big) \mathbf{X} = \mathbf{P}\mathbf{X},
\]
where 

\[
\mathbf{P} = \Big( \mathbf{I} - \frac{1}{n} \mathbf{1}\mathbf{1'}\Big).
\]

\indent This new matrix, $\mathbf{\widetilde{X}}$, has the same dimensions as 
the original one but its columns mean is \textbf{0}. From this matrix, it is 
possible to build two square semi-positive definite matrices: the covariance 
matrix \textbf{S}, defined as $\mathbf{\widetilde{X}'}\mathbf{\widetilde{X}}/n$ 
and the cross-products matrix $Q = \mathbf{\widetilde{X}}\mathbf{\widetilde{X}'}$. 
The last matrix can be interpreted as a similarity matrix between the $n$ elements. 
The term $ij$ is obtained as follows:

\begin{equation} \label{qij}
q_{ij} = \sum_{s=1}^{p} x_{is}x_{js} = \mathbf{x_i'} \mathbf{x_j}
\end{equation}
where $\mathbf{x_i}'$ is the $i-th$ row from $\mathbf{\widetilde{X}}$. 

\indent Given the scalar product formula, 
${\mathbf{x_i'}\mathbf{x_j} =  \mid \mathbf{x_i} \mid \mid \mathbf{x_i} \mid \cos\theta_{ij}}$,
if the elements $i$ and $j$ have similar coordinates, then $\cos\theta_{ij} \simeq 1$
and $q_{ij}$ will be large. On the contrary, if the elements are very different,
then $\cos \theta_{ij} \simeq 0$ and $q_{ij}$ will be small. So, 
$\mathbf{\widetilde{X}}\mathbf{\widetilde{X}'}$ can be interpreted as the similarity
matrix between the elements.

\indent The distances between elements can be deduced from the similarity 
matrix. The Euclidean distance between two elements is calculated in the 
following way:

\begin{equation} \label{dij}
d^2_{ij} =  \sum_{s=1}^{p} (x_{is}- x_{js} )^2  = \sum_{s=1}^{p}x_{is}^2 + \sum_{s=1}^p x_{js}^2 - 2\sum_{s=1}^{p} x_{is}x_{js}.
\end{equation}

\indent This expression can be obtained directly from the matrix \textbf{Q}:

\begin{equation} \label{dfromq}
d^2_{ij} = q_{ii} + q_{jj} - 2q_{ij}.
\end{equation}

\indent We have just seen that, given the matrix $\mathbf{\widetilde{X}}$, 
it is possible to get the similarity matrix 
$\mathbf{Q} = \mathbf{\widetilde{X}}\mathbf{\widetilde{X}'}$ and from it, 
to get the distance matrix \textbf{D}. Let $\diag(\mathbf{Q})$ be the
vector that contains the diagonal terms of \textbf{Q} and \textbf{1} be the vector
of ones, the matrix \textbf{D} is given by

\[
\mathbf{D} = \diag(\mathbf{Q}) \mathbf{1}' + \mathbf{1}\diag(\mathbf{Q})' - 2\mathbf{Q}.
\]

\indent The problem we are dealing with goes in the opposite direction. We want 
to rebuild $\mathbf{\widetilde{X}}$ from a square distance matrix \textbf{D}, 
with elements $d_{ij}^2$. The first step is to obtain \textbf{Q} and afterwards, 
to get $\mathbf{\widetilde{X}}$. The theory needed to get the solution can be 
found in \cite{pena_libro}. Here, we summarise it.

\indent The first step is to find out a way to obtain the matrix \textbf{Q} 
given \textbf{D}. We can assume without loss of generality that the mean of 
the variables is equal to 0. This is a consequence of the fact that the distance 
between two points remains the same if the variables are expressed in terms 
of the mean:


\begin{equation} \label{dtraslated}
d_{ij}^2 = \sum_{s = 1}^p (x_{is} - x_{js})^2 = \sum_{s=1} ^p [(x_{is} - \overline{x_s})- (x_{js} - \overline{x_s})]^2.
\end{equation}

\indent The previous condition means that we are looking for a matrix  
$\mathbf{\widetilde{X}}$ such that $\mathbf{\widetilde{X}'}\mathbf{1} = 0$. 
It also means that $\mathbf{Q}\mathbf{1} = 0$, i.e, the sum of all the elements 
of a column of \textbf{Q} is 0. Since the matrix is symmetric, the previous 
condition should state for the rows as well. 

\indent To establish these constrains, we sum up (\ref{dfromq}) at row level:

\begin{equation} \label{sumrows}
\sum_{i = 1}^n d_{ij}^2 = \sum_{i = 1}^n q_{ii} + nq_{jj} = t + nq_{jj}
\end{equation}
where $t = \sum_{i = 1}^n q_{ii} = \Tr(\mathbf{Q})$, and we have used that the
condition \textbf{Q}\textbf{1} = 0 implies $\sum_{i = 1}^n q_{ij} = 0$. Summing 
up (\ref{dfromq}) at column level:

\begin{equation} \label{sumcols}
\sum_{j = 1}^n d_{ij}^2 = t + nq_{ii}.
\end{equation}

\indent Summing up (\ref{sumrows}) we obtain:

\begin{equation} \label{doublesum}
\sum_{i = 1}^n\sum_{j = 1}^n d_{ij}^2 = 2nt
\end{equation}

\indent Replacing in (\ref{dfromq}) $q_{jj}$ obtained in (\ref{sumrows}) and $q_{ii}$
obtained in (\ref{sumcols}), we have the following expression:

\begin{equation} \label{generaldij}
d_{ij}^2 = \frac{1}{n}\sum_{i = 1}^n d_{ij}^2 - \frac{t}{n} + \frac{1}{n} \sum_{j = 1}^n d_{ij}^2 -\frac{t}{n} -2q_{ij}
\end{equation}

\indent Let $d_{i.}^2 = \frac{1}{n}\sum_{j = 1}^n d_{ij}^2$ and $d_{.j}^2 = \frac{1}{n}\sum_{i=1}^n d_{ij}^2$ 
be the row-mean and column-mean of the elements of \textbf{D}. Using 
(\ref{doublesum}), we have that

\begin{equation} \label{dmeans}
d_{ij}^2 = d_{i.}^2 + d_{.j}^2 - d_{..}^2-2q_{ij}
\end{equation}
where $d_{..}$ is the mean of all the elements of \textbf{D}, given by

\[
d_{..}^2 = \frac{1}{n^2}\sum \sum d_{ij}^2.
\]

\indent Finally, from (\ref{dmeans}) we get the expression:

\begin{equation} \label{qij2}
q_{ij} = -\frac{1}{2}(d_{ij}^2 - d_{i.}^2 - d_{.j}^2 + d_{..}^2).
\end{equation}

\indent The previous expression shows how to build the matrix of similarities 
\textbf{Q} from the distance matrix \textbf{D}.

\indent The next step is to obtain the matrix \textbf{X} given the matrix 
\textbf{Q}. Let's assume that the similarity matrix is positive definite of 
range $p$. Therefore, it can be represented as

\[
\mathbf{Q} = \mathbf{V}\mathbf{\Lambda}\mathbf{V'},
\]
where $\mathbf{V}$ is a $n \times p$ matrix that contains the eigenvectors with
not nulls eigenvalues of \textbf{Q}. $\mathbf{\Lambda}$ is a diagonal matrix 
$p \times p$ that contains the eigenvalues.

\indent Re-writing the previous expression, we obtain

\begin{equation} \label{generalQ}
\mathbf{Q} = (\mathbf{V}\mathbf{\Lambda}^{1/2})(\mathbf{\Lambda}^{1/2}\mathbf{V'}).
\end{equation}

Getting

\[
\mathbf{Y} = \mathbf{V}\mathbf{\Lambda}^{1/2}.
\]

We have obtained a matrix with dimensions $n \times p$ with $p$ uncorrelated
variables that reproduce the initial metric. It is important to notice that if 
one starts from \textbf{X} (i.e \textbf{X} is known) and calculates from these
variables the distance matrix in (\ref{dij}) and after that it is applied
the method explained, the matrix obtained is not the same as \textbf{X}, but
its principal components. This happens since the distance between the rows in
a matrix does not change if:

\begin{itemize}
\item The row-mean values are modified by adding the same row vector to all
the rows in \textbf{X}.

\item Rows are rotated, i.e, \textbf{X} is postmultiplied by an orthogonal 
matrix.
\end{itemize}

\indent By (\ref{dfromq}), the distance is a function of the terms of the 
similarity matrix \textbf{Q} and this matrix is invariant given any rotation,
reflection or translation of the variables

\[
\mathbf{Q} = \mathbf{\widetilde{X}} \mathbf{\widetilde{X'}} = \mathbf{\widetilde{X}} \mathbf{A} \mathbf{A'}\mathbf{\widetilde{X'}}
\]
for any orthogonal \textbf{A} matrix. The matrix \textbf{Q} only contains 
information about the space generated by the variables \textbf{X}. Any rotation,
reflection or translation preserves the distance. Therefore, the solution
is not unique.


\section{Building principal coordinates}
Let \textbf{D} be a square distance matrix. The process to obtain 
the \textit{principal coordinates} is as follows:

\begin{enumerate}
\item Build the matrix $\mathbf{Q = - \frac{1}{2} PDP}$ of cross-products.
\item Obtain the eigenvalues of \textbf{Q}. Take the $r$ greatest eigenvalues. 
Since $\mathbf{P1}=0$, where \textbf{1} is a vector of ones, 
$\range(\mathbf{Q})=n-1$, being the vector \textbf{1} an eigenvector with 
eigenvalue 0. 
\item Obtain the coordinates of the rows in the variables 
$\mathbf{v_i}\sqrt{\lambda_i}$,
where $\lambda_i$ is an eigenvalue of \textbf{Q} and $\mathbf{v_i}$ is the
associated unitary eigenvector. This implies that \textbf{Q} is approximated by

\[
\mathbf{Q} \approx (\mathbf{V_r \Lambda}^{1/2})(\mathbf{\Lambda_r}^{1/2} \mathbf{V_r'}).
\]

\item Take as coordinates of the points the following variables:
\[
\mathbf{Y_r} = \mathbf{V_r}\mathbf{\Lambda_r}^{1/2}.
\]
\end{enumerate}

\indent The method can also be applied if the initial information is not a 
distance matrix but a similarity matrix. A \textit{similarity function} 
is characterized by the following properties ($s_{ij}$ denotes the
similarity between element $i$ and $j$):


\begin{itemize}
\item $s_{ii} = 1$.
\item $0 \leq s_{ij} \leq 1$.
\item $s_{ij} = s_{ji}$.
\end{itemize}

If the initial information is \textbf{Q}, a similarity matrix, then $q_{ii} = 1$,
$q_{ij} = q_{ji}$ and $0 \leq q_{ij} \leq 1$. The associated distance matrix 
is

\[
d_{ij}^2 = q_{ii} + q_{jj} - 2q_{qij} = 2(1-q_{ij}),
\]
and it is easy to see that $\sqrt{2(1-q_{ij})}$ is a distance and it verifies
the triangle inequality.


\section{Procrustes transformation}
As we have mentioned before, the MDS solution is not unique. One example of it 
is shown in Figure \ref{twosol}.

\begin{figure}[!ht]
    \centering
    \subfloat{{\includegraphics[scale=1.5]{./images/europ_cities.png} }}%
    \qquad
    \subfloat{{\includegraphics[scale=1.5]{./images/europ_cities_rot.png} }}%
    \caption{Two different MDS configurations for Figure \ref{european_distances}.}%
    \label{twosol}%
\end{figure}


\indent Since rotations, translations and reflections are distance-preserving 
functions, one can find two different MDS configurations 
for the same set of data. How is it possible to align both solutions? 
\textit{Align both solutions} (or multiple ones) means to find a common 
coordinate system for all the solutions, i.e, let \textbf{MDS$_1$} and 
\textbf{MDS$_2$} be two MDS solutions of dimensions $n \times r$. We say they 
are aligned if the coordinates of row $i$ are the same in both solutions:

\[
mds_{i1}^1 = mds_{i1}^2, \dots, mds_{ir}^1 = mds_{ir}^2
\]
where $mds_{ij}^k$ is the coordinates $j$ for the individual $i$ given the 
solution $k$, $j \in \{1, \dots r\}$, $i \in \{1, \dots, n\}$ and 
$k \in \{1,2\}$.


\indent This problem is solved by means of \textit{Procrustes transformations}.  
The Procrustes problem is concerned with fitting a configuration (testee)
to another (target) as closely as possible. In the simple case, both 
configurations have the same dimensionality and the same number of points, which
can be brought into 1-1 correspondence. Under orthogonal transformations, 
the testee can be fitted to the target. In addition to such rigid motions, 
one may also allow for dilations and for shifts.

\indent All the details are developed in  \citeN{BorgGroenen2005}. This is out 
of the scope of this thesis. However, since it has been a repeatedly used tool, 
we briefly summarise it. 

\indent Let \textbf{A} and \textbf{B} be two different MDS configurations 
of dimensions $n \times t$ for the same set of data. Without loss of generality, 
let's assume that the target is \textbf{A} and the testee is \textbf{B}. 
One wants to obtain $s \in {\rm I\!R}$, 
$\mathbf{T} \in \mbox{M}_{r\times r}({\rm I\!R})$ and 
$\mathbf{t} \in {\rm I\!R}^r$ such that

\[
\mathbf{A} = s \mathbf{B} \mathbf{T} + \mathbf{1t}'
\]
where \textbf{T} is an orthogonal matrix. As mentioned before, 
in \citeN{BorgGroenen2005} are all the details needed to estimate 
these parameters.

\section{Multidimensional Scaling with \textsf{R}}

All the algorithms are coded in \textsf{R}. We use two 
packages for developing our MDS approaches:

\begin{itemize}
\item Package: \textsf{stats}. From this one we use the function 
\textsf{cmdscale} to do the MDS. The output of this function is a list of 
two elements:
\begin{itemize}
\item The first $r$ principal coordinates for the individuals, i.e,
the low-dimensional configuration for the data.
\item All the eigenvalues found. If the dimensions of the initial dataset 
are $n \times k$, then there are $n$ eigenvalues.
\end{itemize}
\item Package: \textsf{MCMCpack}. From this one we use the function 
\textsf{procrustes} to do Procrustes transformation. The output of 
this function is a list of three elements:
\begin{itemize}
\item The dilation coefficient $s$.
\item The orthogonal matrix \textbf{T}.
\item The translation vector \textbf{t}.
\end{itemize}
\end{itemize}

\chapter{Algorithms for Multidimensional Scaling with Big Data}
\label{alg_mds}

\section{Why is it needed?}
In this chapter we present the algorithms developed so that MDS can be applied
when we are dealing with large datasets. The first question one might ask is 
why we need them if there are already some implementations to obtain a
MDS configuration. To answer this question, let's take a look at 
Figure \ref{elapsed_time_mds} and Figure \ref{memory_distance}.

 
\begin{figure}[!ht]
\centering
    \includegraphics[scale = 0.5]{./images/elapsed_time_mds.png}
    \caption{Elapsed time to compute MDS.}
    \label{elapsed_time_mds}
\end{figure}



\begin{figure}[!ht]
\centering
\includegraphics[scale = 0.5]{./images/memory_distance.png}
\caption{Memory consumed to compute the distance matrix.}
\label{memory_distance}
\end{figure}


\indent Figure \ref{elapsed_time_mds} shows the time needed to compute MDS
as a function of the sample size. As we can see, the time grows 
considerably as the sample size increases when using \textsf{cmdscale} function. 
Apart of the time issue, there is another one related to the memory needed to 
compute the distance matrix. Figure \ref{memory_distance} points out 
that it is required at least 400MB of RAM memory to obtain the distance matrix 
when the dataset is close to $10^4$ observations.

\indent In order to solve these problems, we have considered to work on 
three algorithms:


\begin{itemize}
\item \textit{Divide and Conquer MDS:} Before this thesis, 
\textit{Pedro Delicado} did some work about  MDS with large
datasets and he already created a first approach, which is this one. The 
algorithm is based on the idea of dividing and conquering. Given a large 
dataset, it is divided into $p$ partitions. After that, MDS is performed 
over all the partitions and, finally, the $p$ solutions are stitched so that 
all the points lie on the same coordinate system.

\item \textit{Fast MDS:} during the phase of gathering information, we found an 
article that solved the problem of scalability \cite{Yang06afast}. The 
authors  use a divide and conquer idea together with recursive programming. 

\item \textit{MDS based on Gower interpolation:} this algorithm uses
Gower interpolation formula, which allows to add a new set of points
to an existing MDS configuration. For further details see, for instance, the 
Appendix of \cite{gowerformula}. 

\end{itemize}

In the next sections we provide a description of the algorithms. 
If further details about the implementation are needed, the code is provided in 
\Cref{chap:code}.

\section{Divide and Conquer MDS}
\subsection{Algorithm}

\begin{itemize}

\item The first step is to divide the original dataset into $p$ partitions: 
$\mathbf{X_1},\dots, \mathbf{X_p}$. The number of partitions, $p$, is also
the number of steps needed to compute the algorithm. Each partition has the
same number of rows.

\item Calculate the MDS for the first partition: \textbf{MDS(1)}. This solution
will be used as a guide to align the MDS configuration for the remaining 
partitions. We use a new variable, \textbf{cum-mds}, that will be growing 
as long as new partitions are used. Before adding a new MDS configuration, 
it is aligned and, after that, added. 

\item Define \textbf{cum-mds} equals to \textbf{MDS(1)} and start iterating 
until the last partition is reached.

\item Given a step $k$, $1 < k \leq p$, partitions $k$ and \textit{k-1} are 
joint, i.e, $\mathbf{X_k} \cup \mathbf{X_{k-1}}$. MDS is calculated on 
this union, obtaining $\mathbf{MDS_{k, k-1}}$. In order to add the
rows of the \textit{k-th} partition to \textbf{cum-mds}, the following steps 
are performed:

\begin{itemize}
\item Take the rows of the partition \textit{k-1} from $\mathbf{MDS_{k, k-1}}$: 
$\mathbf{MDS_{k, k-1}} \Bigr|_{k-1}$.
\item Take the rows of the partition \textit{k-1} from \textbf{cum-mds}: 
\textbf{cum-mds} $\Bigr|_{k-1}$.
\item Apply Procrustes to align both solutions. It means that a scalar number
\textit{s}, a vector \textbf{t} and an orthogonal matrix \textbf{T} are obtained
so that
\[
\textbf{\mbox{cum-mds}} \Bigr|_{k-1} \approx s \mathbf{MDS_{k, k-1}} \Bigr|_{k-1} \mathbf{T} + \mathbf{1t'}.
\]
\item Take the rows of the partition $k$ from $\mathbf{MDS_{k, k-1}}: \mathbf{MDS_{k, k-1}} \Bigr|_{k}$.
\item Use the previous Procrustes parameters to add the rows of 
$\mathbf{MDS_{k, k-1}} \Bigr|_{k}$ to \textbf{cum-mds}:
\[
\textbf{\mbox{cum-mds}}_\textbf{k} := s \mathbf{MDS_{k, k-1}} \Bigr|_{k} \mathbf{T} + \mathbf{1t}'.
\]
\item Add the previous dataset to \textbf{cum-mds}, i.e:
\[
\textbf{\mbox{cum-mds}} = \textbf{\mbox{cum-mds}} \cup \textbf{\mbox{cum-mds}}_\textbf{k}
\]

\end{itemize}
\end{itemize}

As we have seen, the algorithm depends on $p$, the number of partitions. How 
many of them are needed? To answer this question, let $l \times l$ be the 
size of the largest matrix that allows to run MDS efficiently, i.e, in a 
reasonable amount of time. If $n$ is the number of rows 
of \textbf{X}, then $p$ is $\frac{2n}{l}$. 

\indent Note that if the number of rows of the original dataset, \textbf{X}, 
is such that allows to run classical MDS over \textbf{X} without partitioning 
it, then $p=1$ and \textit{Divide and Conquer MDS} is just classical MDS.


\subsection{Some indicators about the performance of the algorithm}
\label{chap:ind_div}
The aim of this section is to show some indicators about the performance of the
algorithm. A deeper analysis is done in \Cref{chap:sim}, where more details are
provided.

\indent We generate a matrix \textbf{X} with 3 independent \textit{Normal} 
distributions ($\mu = 0$ and $\sigma = 1$) and $10^3$ rows with $l$ equals to 500. 
Afterwards, we run the algorithm. We require the algorithm to 
return 3 columns. So, a new matrix with 3 columns and $10^3$ rows 
($\mathbf{MDS_{Div}}$) has been obtained. Both matrices should be ``equal" 
with an exception of either a rotation, translation or reflection, but 
not a dilation. We do not allow dilations to see that the distance is 
preserved.

\indent To align the matrices we perform a Procrustes transformation, but 
setting the the dilation parameter (\textit{s}) equals to 1. 
After that, we compare the three columns (we refer to the columns as 
\textit{dimensions}). Figure \ref{divide_example1}, Figure \ref{divide_example2} 
and Figure \ref{divide_example3}  show the dimension
$i$ of \textbf{X} against the dimension $i$ of $\mathbf{MDS_{Div}}$, 
$i \in \{1,2,3\}$. 

\indent As we can see, the algorithm is able to capture the dimensions of the 
original matrix. We do not show cross-dimensions (i.e dimension $i$ of \textbf{X}
against dimension $j$ of $\mathbf{MDS_{Div}}$ $ i \neq j$), but Table 
\ref{corr_mds} contains the cross-correlation matrix. The results 
show that dimension  $i$ of  $\mathbf{MDS_{Div}}$ captures dimension 
$i$ of \textbf{X} and just dimension $i$. So, it seems that the algorithm, for 
this particular case, has a good performance in terms of quality.



\begin{figure}
    \centering
    \includegraphics[scale = 1]{./images/first_div.pdf}
    \caption{Dimension 1 \textbf{X} against dimension 1 of $\mathbf{MDS_{Div}}$. In red, the line $x=y$.}
    \label{divide_example1}
\end{figure}

\FloatBarrier

\begin{figure}
    \centering
    \includegraphics[scale = 1]{./images/second_div.pdf}
    \caption{Dimension 2 \textbf{X} against dimension 2 of $\mathbf{MDS_{Div}}$. In red, the line $x=y$.}
    \label{divide_example2}
\end{figure}

\FloatBarrier

\begin{figure}
    \centering
    \includegraphics[scale = 1]{./images/third_div.pdf} 
    \caption{Dimension 3 of \textbf{X} against dimension 3 of $\mathbf{MDS_{Div}}$. In red, the line $x=y$.}
    \label{divide_example3}
\end{figure}


\FloatBarrier

\begin{table}[ht]
\centering
\begin{tabular}{r|rrr}
 & $X_1$ & $X_2$ & $X_3$ \\ 
  \hline
  $\mbox{MDS}_{Div1}$ & 1 & 0.02 & -0.04 \\ 
  $\mbox{MDS}_{Div2}$ & 0.02 & 1 & 0.02 \\ 
  $\mbox{MDS}_{Div3}$ & -0.04 & 0.02 & 1 \\ 
   \hline
\end{tabular}
\caption{Cross-correlation of \textbf{X} and $\mathbf{MDS_{Div}}$.} 
\label{corr_mds}
\end{table}



\section{Fast MDS}
During the process of gathering information about work previously done around
MDS with large datasets, we found that \citeN{Yang06afast} already proposed an 
algorithm, they named it \textit{Fast Multidimensional Scaling}. 


\subsection{Algorithm}

\begin{itemize}

\item Divide \textbf{X} into $\mathbf{X_1},\dots, \mathbf{X_p}$. Each of them 
of the same dimensions.

\item Obtain MDS configuration in the following way:

\begin{itemize}

\item If $\mathbf{X}_i$ has such dimensions that allow to run classical MDS in
a reasonable amount of time, compute MDS for each $\mathbf{X_i}$: 
$\mathbf{MDS_1}, \dots, \mathbf{MDS_p}$. 

\item Otherwise, for each $\mathbf{X_i}$ call recursively \textit{Fast MDS} 
(at the end of this section we explain detailedly how the stop condition for
the recursion is found).

\end{itemize}

\item These individuals MDS solutions are stitched together by sampling 
$s$ points (rows) from each submatrix $\mathbf{X_i}$ and putting them into 
an alignment matrix $\mathbf{M_{align}}$ of size $sp \times sp$. In principle, 
$s$ should be at least 1 plus the estimated dimensionality of the dataset. 
In practice, it is oversampled by a factor of 2 or more. 

\item MDS is run on $\mathbf{M_{align}}$. After this, it is obtained
$\mathbf{mMDS}$. Given a sampled point, there are two solutions of MDS: 
one from $\mathbf{X_i}$ and another one from $\mathbf{M_{align}}$.

\item The next step is to compute Procrustes transformation to  line  
these two sets of solutions up in a common coordinate system:

\[
\mathbf{mMDS_i} = s_i \mathbf{dMDS_i} \mathbf{T_i} + \mathbf{1t_i}'
\]
where:

\begin{itemize}

\item $\mathbf{dMDS_i}$ is $\mathbf{MDS_i}$ but taking into account just
the subset of the sample points that belongs to partition $i$.

\item $\mathbf{mMDS_i}$ is $\mathbf{mMDS}$ but taking into account just
the subset of the sample points that belongs to partition $i$.
\end{itemize}

\item After doing the previous part, we obtain a set of $p$ Procrustes 
parameters $(s_i, \mathbf{T_i},  \mathbf{t_i})$. So, the next step is to 
apply this set of parameters to each $\mathbf{MDS_i}$, i.e, 

\[
\mathbf{MDS_i}^a := s_i \mathbf{MDS_i} \mathbf{T_i} + \mathbf{1t_i'}.
\]

\item The last step is to join $\mathbf{MDS_1}^a, \dots,  \mathbf{MDS_p}^a$ 
all together, i.e, 
\[
\mathbf{MDS_X}:= \mathbf{MDS_1}^a \cup \cdots \cup \mathbf{MDS_p}^a.
\]

\end{itemize}

\indent The process of splitting the dataset is applied recursively 
until the size of $\mathbf{X_i}$ is optimal to run MDS on. The stop condition 
is found as follows: let $l \times l$ be the size of the largest matrix that 
allow MDS to be executed efficiently, i.e, in a reasonable amount of time. 
There are two issues that impact the performance of the algorithm: the size 
of each submatrix after subdivision and the number of submatrices, $p$, that 
are stitched together at each step. Ideally, the size of each submatrix after 
division should be as large as possible without exceeding $l$. By the same 
token, the size of $\mathbf{M_{align}}$ should be bounded by $l$. The number of 
submatrices to be stitched together, $p$, should be the largest number such 
that $sp \leq l$.

\indent As in the previous algorithm, if the number of rows of the original 
dataset, \textbf{X}, is such that allows to run classical MDS over \textbf{X} 
without partitioning it, then $p=1$ and \textit{Fast MDS} is just the 
classical MDS.


\subsection{Some indicators about the performance of the algorithm}
As we have done in \Cref{chap:ind_div}, we present some visual results of
this algorithm. The data used are the same as in \Cref{chap:ind_div}. We
call $\mathbf{MDS_{Fast}}$ the result that provides the previous algorithm.

\indent Figure \ref{fast_example1}, Figure \ref{fast_example2} and
Figure \ref{fast_example3} show that, for this particular case,
the algorithm captures quite well the dimensions of the original data, 
providing a good performance. In addition, dimension $i$ of 
$\mathbf{MDS_{Fast}}$ fits perfectly the same dimensions $i$ of \textbf{X} and 
just this one, as we can see in Table \ref{corr_fast}.

\begin{figure}
    \centering
    \includegraphics[scale = 1]{./images/first_fast.pdf} 
    \caption{Dimension 1 of \textbf{X} against dimensions 1 of  $\mathbf{MDS_{Fast}}$. In red, the line $x=y$.}
    \label{fast_example1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale = 1]{./images/second_fast.pdf}
    \caption{Dimension 2 of \textbf{X} against dimensions 2 of  $\mathbf{MDS_{Fast}}$. In red, the line $x=y$.}
    \label{fast_example2}
\end{figure}

\FloatBarrier

\begin{figure}
    \centering
    \includegraphics[scale = 1]{./images/third_fast.pdf}
    \caption{Dimension 3 of \textbf{X} against dimensions 3 of  $\mathbf{MDS_{Fast}}$. In red, the line $x=y$.}
    \label{fast_example3}
\end{figure}


\FloatBarrier


\begin{table}[ht]
\centering
\begin{tabular}{r|rrr}
 & $X_1$ & $X_2$ & $X_3$ \\ 
  \hline
  $\mbox{MDS}_{Fast1}$ & 1 & 0.02 & 0 \\ 
  $\mbox{MDS}_{Fast2}$ & 0.02 & 1 & 0.02 \\ 
  $\mbox{MDS}_{Fast3}$ & 0 & 0.02 & 1 \\ 
   \hline
\end{tabular}
\caption{Cross-correlation of \textbf{X} and $\mathbf{MDS_{Fast}}$.} 
\label{corr_fast}
\end{table}

\section{MDS based on Gower interpolation}

Gower interpolation formula (see Appendix of \cite{gowerformula}) allows to add 
a new set of points to a given MDS configuration. Given a matrix 
\textbf{X} $n \times p$, a MDS configuration for this matrix of dimension 
$n \times c$ and a matrix $\mathbf{X_{new}}$ $m \times p$, one wants to 
add these new $m$ rows to the existing MDS configuration. So, 
after adding this new rows, the MDS configuration will have $n+m$ rows and 
$c$ columns. We briefly summarise how to do so:

\begin{itemize}

\item Obtain $\mathbf{J} = \mathbf{I_n} - \frac{1}{n}\mathbf{1}\mathbf{1}'$,
where $\mathbf{I_n}$ is the identity matrix $n \times n$.

\item Given the distance matrix \textbf{D} of the rows of \textbf{X},
calculate $\mathbf{\Delta} = (\delta_{ij}^2)$. Note that $\mathbf{\Delta}$ is 
of size $n \times n$.

\item Calculate $\mathbf{G} = - \frac{1}{2} \mathbf{J} \mathbf{\Delta} \mathbf{J}'$.

\item Let \textbf{g} be the diagonal of \textbf{G}, i.e, 
$\mathbf{g} = \diag({\mathbf{G}})$. We treat \textbf{g} as a vector. 

\item Let \textbf{A} be the distance matrix between the rows of 
\textbf{X} and the rows of $\mathbf{X_{new}}$. \textbf{A} has dimensions 
$m \times n$. Let $\mathbf{A}^2$ be the matrix of the square elements 
of \textbf{A}, i.e, $\mathbf{A}^2 = (a_{ij}^2)$.

\item Let \textbf{M} and \textbf{S} be the MDS for \textbf{X} and the 
variance-covariance matrix of the $c$ columns of \textbf{M}.

\item The interpolated coordinates for the new $m$ observations are given by

\begin{equation} \label{gower_f}
\frac{1}{2n} (\mathbf{1}\mathbf{g}' - \mathbf{A}^2) \mathbf{M}\mathbf{S}^{-1}.
\end{equation}

\end{itemize}

The resulting MDS for the $m$ observations of $\mathbf{X_{new}}$ is in the same
coordinate system as \textbf{M}. So, here it is not needed to do any 
Procrustes transformation.

\subsection{Algorithm}

\begin{itemize}
\item Divide \textbf{X} into $p$ partitions $\mathbf{X_1},\dots, \mathbf{X_p}$
of the same dimensions.

\item If $p=1$, classical MDS is run over \textbf{X}.

\item Otherwise, we use the procedure explained above, 
being $\mathbf{X} := \mathbf{X_1}$ and 
$\mathbf{X_{new}} := \mathbf{X_k}$, $k \in \{2, \dots, p\}$.

\item Calculate \textbf{J}, $\mathbf{\Delta}$, \textbf{G}, \textbf{g},
\textbf{A}, \textbf{M} and \textbf{S} according to the above formulas.

\item Obtain MDS for the first partition $\mathbf{X_1}$. 

\item Given a partition $1 < k \leq p$, do the following steps to get the 
related MDS:

\begin{itemize}

\item Calculate the distance matrix between the rows of $\mathbf{X_1}$ and
$\mathbf{X_k}$ and calculate the square of each element of this matrix. Let
$\mathbf{A}^2$ be this matrix (same as above).

\item Use Gower interpolation formula (\ref{gower_f}) to obtain MDS for 
partition $k$. 

\item Accumulate this solution.


\end{itemize}

\end{itemize}


As in the previous two algorithms, there is a key parameter to choose: $p$,
the number of partitions. For this algorithm, $p$ is set in the following way.
Let $l \times l$ be the size of the largest distance matrix that a computer can 
calculate efficiently, i.e, in a reasonable amount of time. The value of 
$p$ is set as $n/p$.


\subsection{Some indicators about the performance of the algorithm}
We repeat the same as in \Cref{chap:ind_div}. Figure \ref{gower_example1},
Figure \ref{gower_example2}, Figure \ref{gower_example3} and 
Table \ref{corr_gower} show that the algorithm, for this particular case, 
captures quite well the dimensions of the original data, providing a 
good performance.


\begin{figure}[h]
    \centering
    \includegraphics[scale = 1]{./images/first_gower.pdf} 
    \caption{Dimension 1 of \textbf{X} against dimension 1 of  $\mathbf{MDS_{Gower}}$. In red, the line $x=y$.}
    \label{gower_example1}
\end{figure}

\FloatBarrier

\begin{figure}[h]
    \centering
    \includegraphics[scale = 1]{./images/second_gower.pdf}
    \caption{Dimension 2 of \textbf{X} against dimension 2 of  $\mathbf{MDS_{Gower}}$. In red, the line $x=y$.}
    \label{gower_example2}
\end{figure}
\FloatBarrier


\begin{figure}[h]
    \centering
    \includegraphics[scale = 1]{./images/third_gower.pdf}
    \caption{Dimension 3 of \textbf{X} against dimension 3 of  $\mathbf{MDS_{Gower}}$. In red, the line $x=y$.}
    \label{gower_example3}
\end{figure}


\FloatBarrier


\begin{table}[ht]
\centering
\begin{tabular}{r|rrr}
 & $X_1$ & $X_2$ & $X_3$ \\ 
  \hline
  $\mbox{MDS}_{Gower1}$ & 1 & 0 & -0.04 \\ 
  $\mbox{MDS}_{Gower2}$ & 0 & 1 & -0.0 \\ 
  $\mbox{MDS}_{Gower3}$ & -0.04 & -0.03 & 1 \\ 
   \hline
\end{tabular}
\caption{Cross-correlation of \textbf{X} and $\mathbf{MDS_{Gower}}$.} 
\label{corr_gower}
\end{table}



\section{Output of the algorithms}
The three algorithms have the same type of output. It consists on a list of 
two parameters. 

\indent The first parameter is the MDS configuration calculated by the 
algorithm. It is a matrix of $n$ rows and $c$ columns, where $n$ is the number 
of rows of the input data and $c$ is the number of dimensions the user has 
required.

\indent The second parameter is a list of eigenvalues. This list is built
as follows: 

\begin{itemize}
\item All the algorithms divide the initial data into a set of $p$ partitions.

\item Given a partition $i$, a distance matrix of dimensions $m_i \times m_i$
is calculated: $\mathbf{D_i}$. 

\item Over $\mathbf{D_i}$ a singular value decomposition is performed, providing 
a list of length $m_i$ that contains all the eigenvalues of the previous 
decomposition: $\mbox{list}_i$.

\item Let $\mbox{norm\_eigenvalues}_i$ be $\mbox{list}_i/m_i$, i.e, each 
eigenvalue is divided by the number of rows of $\mathbf{D_i}$.

\item The algorithms return 
$\mbox{norm\_eigenvalues}_1 \cup \cdots \cup \mbox{norm\_eigenvalues}_p$. We 
refer to this union as the \textit{normalized eigenvalues}. 

\end{itemize}


\section{Comparison of the algorithms}
\indent The three previous algorithms share the same goal: obtaining a MDS 
configuration for a given large dataset. However, there are some differences 
between the approaches that impact the performance of the algorithms. 
The main differences between them are:

\begin{itemize}
\item \textit{Divide and Conquer MDS} uses a guide 
(the first subset, $\mathbf{X_1}$) to align the solutions as well as it uses the whole partition $\mathbf{X_i}$ to find Procrustes parameters. However, 
\textit{Fast MDS} does not use a guide an it uses a set of subsamples to 
find Procrustes parameters.

\item \textit{Fast MDS} is based on recursive programming. It divides until 
a manageable dimensionality is found. However, \textit{Divide and Conquer MDS} 
finds the number of partitions without applying recursive programming.

\item \textit{MDS based on Gower interpolation} does not need any Procrustes
transformation. 

\end{itemize}

The fact that we found three algorithms to compute MDS possesses some questions 
that need to be answered:

\begin{itemize}
\item Are these algorithms able to capture the data dimensionality as good as 
classical MDS does?
\item Which is the fastest method?
\item Can they deal with large datasets in a reasonable amount of time?
\item How are they performing when dealing with large data sets?
\end{itemize}

All these questions are answered in \Cref{chap:sim}.



\chapter{Simulation study}
\label{chap:sim}

\section{Design of the simulation}


Given the three algorithms, we would like to explore their performance.  There 
are two issues to study:

\begin{itemize}
\item Performance in terms of results quality: are they able to capture 
the right data dimensionality?
\item Performance in terms of time: are they `` fast" 
enough? Which one is the fastest?
\end{itemize}

To test the algorithms under different conditions, a simulation study has been
carried out. The scenarios are obtained as combinations of:

\begin{itemize}
\item \textit{Sample sizes}: we use different sample sizes, combining small
datasets and large ones. A total of six sample sizes are used, which are:

\begin{itemize}
\item Small sample sizes: $10^3, 3\cdot 10^3, 5\cdot 10^3$ and $10^4$.

\item Large sample sizes: $10^5$ and $10^6$.
\end{itemize}



\item \textit{Data dimensions}: we generate a matrix with two different number 
of columns: 10 and 100.

\item \textit{Main dimensions}: given \textbf{X} $n \times k$,
where $n \in \{10^3, 3\cdot 10^3, 5\cdot10^3, 10^4, 10^5, 10^6 \}$ and 
$k \in \{10, 100\}$, it is postmultiplied by a diagonal matrix that contains 
$k$ values,  $\lambda_1, \dots, \lambda_k$. The first values are much 
higher than the rest. The idea of this is to see if the algorithms are able 
to capture the main dimensions of the original dataset, i.e, the columns 
with the highest variance. We set 5 combinations for this variable, which are:

\begin{itemize}
\item All the columns with the same values of $\lambda$: 
$\lambda_1 = \cdots = \lambda_k = 1$.

\item One main dimension with $\lambda_1 = 15$ and 
$\lambda_2 = \cdots = \lambda_k = 1$.

\item  Two main dimensions of the same value $\lambda$: 
$\lambda_1  = \lambda_2 = 15$ and $\lambda_3 = \cdots = \lambda_k = 1$.

\item  Two main dimensions of different values $\lambda$: 
$\lambda_1  = 15$, $\lambda_2 =10$ and $\lambda_3 = \cdots = \lambda_k = 1$.

\item  Four main dimensions of the same value $\lambda$: 
$\lambda_1  = \lambda_2 = \lambda_3 = \lambda_4 = 15$ and $\lambda_5 = \cdots = \lambda_k = 1$.


\end{itemize}

\item As a probabilistic model, we use a Normal distribution with $\mu = 0$ and 
$\sigma = 1$. With this distribution, we generate a matrix of $n$ observations
and $k$ columns, being the $k$ columns independent. After generating the 
dataset \textbf{X}, it is postmultiplied by the diagonal matrix that contains
the values of $\lambda$'s.


\end{itemize}

There is a total of 60 scenarios to simulate. Given a scenario, it is 
replicated 100 times. For every simulation, it is generated a dataset 
(according to the scenario), and all the algorithms are run using this dataset.
So, a total of 6000 simulations are carried out.

\indent Tables \ref{scenarios_sim} and \ref{scenarios_sim_big} show the 
configuration of each scenario, being Table \ref{scenarios_sim} the 
configurations for small sample sizes and Table \ref{scenarios_sim_big} 
the configurations for  for large sample sizes. Given a scenario, 
\textit{scenario\_id} identifies it. We refer to a scenario by its 
\textit{scenario\_id}. 

\newpage

\begin{longtable}{rrrl} 
scenario\_id & sample\_size & n\_dimensions & value\_primary\_dimensions \\ 
\hline
1 & $10^3$ & 10 &  - \\ 
2 & $10^3$ & 100 & - \\ 
3 & $10^3$ & 10 & 15 \\ 
4 & $10^3$ & 100 & 15 \\ 
5 & $10^3$ & 10 & 15, 15 \\ 
6 & $10^3$ & 100 & 15, 15 \\ 
7 & $10^3$ & 10 & 15, 10 \\ 
8 & $10^3$ & 100 & 15, 10 \\ 
9 & $10^3$ & 10 & 15, 15, 15, 15 \\ 
10 & $10^3$ & 100 & 15, 15, 15, 15 \\ 
\hline
\hline
11 & $3 \cdot 10^3$ & 10 & -  \\ 
12 & $3 \cdot 10^3$ & 100 & -  \\ 
13 & $3 \cdot 10^3$ & 10 & 15 \\ 
14 & $3 \cdot 10^3$ & 100 & 15 \\ 
15 & $3 \cdot 10^3$ & 10 & 15, 15 \\ 
16 & $3 \cdot 10^3$ & 100 & 15, 15 \\ 
17 & $3 \cdot 10^3$ & 10 & 15, 10 \\ 
18 & $3 \cdot 10^3$ & 100 & 15, 10 \\ 
19 & $3 \cdot 10^3$ & 10 & 15, 15, 15, 15 \\ 
20 & $3 \cdot 10^3$ & 100 & 15, 15, 15, 15 \\ 
\hline
\hline
21 & $5 \cdot 10^3$ & 10 & -  \\ 
22 & $5 \cdot 10^3$ & 100 & -  \\ 
23 & $5 \cdot 10^3$ & 10 & 15 \\ 
24 & $5 \cdot 10^3$ & 100 & 15 \\ 
25 & $5 \cdot 10^3$ & 10 & 15, 15 \\ 
26 & $5 \cdot 10^3$ & 100 & 15, 15 \\ 
27 & $5 \cdot 10^3$ & 10 & 15, 10 \\ 
28 & $5 \cdot 10^3$ & 100 & 15, 10 \\ 
29 & $5 \cdot 10^3$ & 10 & 15, 15, 15, 15 \\ 
30 & $5 \cdot 10^3$ & 100 & 15, 15, 15, 15 \\ 
\hline
\hline
31 & $10^4$ & 10 & -  \\ 
32 & $10^4$ & 100 & -  \\ 
33 & $10^4$ & 10 & 15 \\ 
34 & $10^4$ & 100 & 15 \\ 
35 & $10^4$ & 10 & 15, 15 \\ 
36 & $10^4$ & 100 & 15, 15 \\ 
37 & $10^4$ & 10 & 15, 10 \\ 
38 & $10^4$ & 100 & 15, 10 \\ 
39 & $10^4$ & 10 & 15, 15, 15, 15 \\ 
40 & $10^4$ & 100 & 15, 15, 15, 15 \\ 
\hline
\caption{Scenarios simulated for small sample sizes.} 
\label{scenarios_sim}
\end{longtable}



\newpage

\begin{longtable}{rrrl} 
scenario\_id & sample\_size & n\_dimensions & value\_primary\_dimensions \\ 
\hline
41 & $10^5$ & 10 &  -\\ 
42 & $10^5$ & 100 & - \\ 
43 & $10^5$ & 10 & 15 \\ 
44 & $10^5$ & 100 & 15 \\ 
45 & $10^5$ & 10 & 15, 15 \\ 
46 & $10^5$ & 100 & 15, 15 \\ 
47 & $10^5$ & 10 & 15, 10 \\ 
48 & $10^5$ & 100 & 15, 10 \\ 
49 & $10^5$ & 10 & 15, 15, 15, 15 \\ 
50 & $10^5$ & 100 & 15, 15, 15, 15 \\ 
\hline
\hline
51 & $10^6$ & 10 &  -\\ 
52 & $10^6$ & 100 &  -\\ 
53 & $10^6$ & 10 & 15 \\ 
54 & $10^6$ & 100 & 15 \\ 
55 & $10^6$ & 10 & 15, 15 \\ 
56 & $10^6$ & 100 & 15, 15 \\ 
57 & $10^6$ & 10 & 15, 10 \\ 
58 & $10^6$ & 100 & 15, 10 \\ 
59 & $10^6$ & 10 & 15, 15, 15, 15 \\ 
60 & $10^6$ & 100 & 15, 15, 15, 15 \\ 
\hline
\caption{Scenarios simulated for large sample sizes.} 
\label{scenarios_sim_big}
\end{longtable}

\newpage

\indent Note that scenarios 1, 2, 11, 12, 21, 22, 31, 32, 41,
42, 51, 52 are pure noise. We refer to them as \textit{noisy 
scenarios}.


\indent Given a scenario, the steps done to calculate  and to
store all the data needed are:

\begin{enumerate}

\item Generate the dataset \textbf{X} according to the scenario. 

\item For each algorithm, we do the following steps:

\begin{enumerate}

\item Run the algorithm and get MDS configuration for the algorithm
($\mathbf{MDS_{alg}}$).

\item Get the elapsed time to compute MDS configuration and store it.

\item Get \textit{normalized eigenvalues} and store them.

\item Align $\mathbf{MDS_{alg}}$ and \textbf{X} using Procrustes.

\item Get the correlation coefficients between the main dimensions of 
$\mathbf{MDS_{alg}}$ and \textbf{X} and store them.

\end{enumerate}

\end{enumerate}

\indent There are some important details that affect the results of the 
simulations, which are:

\begin{itemize}

\item When running the algorithms, we ask for as many columns as the original 
data has, i.e, $k$. Therefore, the low-dimensional space has the same 
dimension as the original dataset.

\item For the \textit{normalized eigenvalues}, we just store 6 eigenvalues 
instead of the full list of eigenvalues (otherwise we would store $n$ 
eigenvalues, which is memory consuming). 

\item For Procrustes we dot not allow dilations, otherwise distance could not
be preserved. In addition, we do not use all the columns 
to do the alignment, we select the main dimensions. If there is not any
main dimension, i.e it is one of the \textit{noisy scenarios}, we just select 4 
columns. 

\item To avoid memory problems with the alignment when $n$ is greater or 
equal to $10^5$, Procrustes is done in the following way:

\begin{enumerate}

\item Create $p$ partitions of \textbf{X} and the result of a given MDS 
algorithm ($\mathbf{MDS_{alg}}$). Both sets of partitions contain exactly the
same observations.


\item For each partition get Procrustes parameters without dilations.

\item Accumulate the parameters iteration after iteration. So, at the end, 
we obtain $\mathbf{R} = \sum_{i = 1}^p \mathbf{R_i}/p$ and 
$\mathbf{t} = \sum_{i = 1}^p \mathbf{t_i}/p$.

\item Apply these parameters to $\mathbf{MDS_{alg}}$ so that 
\textbf{X} and $\mathbf{MDS_{alg}}$ are in the same coordinate system and
they can be compared, i.e

\[
\mathbf{X_{Procrustes}} = \mathbf{X} \mathbf{R} + \mathbf{1t'}.
\]


\end{enumerate}
\end{itemize}


\indent Note that the original dataset, \textbf{X}, is always available and
it is already the MDS configuration, since we simulate independent columns with
mean value equals to 0. Therefore, even though $n$ is so large that MDS can 
not be calculated using classical methods, we always have the solution that 
we would obtain if running classical MDS were possible. Therefore, we can 
always compare the MDS provided by the algorithms ($\mathbf{MDS_{alg}}$) 
with the original dataset (\textbf{X}).


\indent In order to test the results quality of the algorithms as well as 
the time needed to compute the MDS configuration, some metrics are calculated. 
These metrics are the following ones:

\begin{itemize}
\item Performance of results quality: two metrics are calculated, which are:
\begin{itemize}


\item Correlation between the main dimensions of the data and the
main dimensions after applying the algorithms. We get the diagonal of the 
correlation matrix, i.e, the correlation between dimension $i$ of the data 
and the dimension $i$ of the algorithm. 

\item \textit{Normalized eigenvalues} as an approximation of the standard 
deviation of the variables of \textbf{X}.
\end{itemize}

\item Elapsed Time to get the MDS configuration: Given an algorithm, we compute 
and store the elapsed time to get the corresponding MDS configuration.

\end{itemize}

\indent We do it in this way because we want to check some hypothesis. We 
expect the three algorithms to behave ``correctly". By ``correctly" we mean 
that the behavior should be similar to those obtained as if classical MDS 
were run. Therefore, we expect that the correlation between the main 
dimensions of the data and the main dimensions of the MDS of each algorithm 
is close to 1.

\indent In addition, the variance of the original data should be captured. 
So, given the highest \textit{normalized eigenvalues}, we expect that 
its square root is approximately 15 or 10 when the scenarios are not the 
\textit{noisy scenarios}. 

\indent For the time of the algorithms, we have done some analysis and it seems 
that \textit{MDS based on Gower interpolation} is the fastest one.
So, proper tests will be done.


\indent The algorithms have as input values a set of variables. The input matrix 
is already explained, but there is another parameter that has been used in 
the description of the algorithms (see \Cref{alg_mds}): $l$. The meaning of 
$l$ is a little bit different in each algorithm, but for simplicity we 
set this value equals to 500. 

\indent \textit{Fast MDS} has an extra parameter: the amplification parameter.
\citeN{Yang06afast} use a value of 3 to test the algorithm. 
We use the same value. So, for each partition, it is taken 30 
(when the original matrix has 10 columns) or 300 (when the original matrix 
has 100 columns) points for every partition to build $\mathbf{M_{align}}$.

\indent Since a total of 6000 simulations are performed and some of them 
include large datasets, we use \textit{Amazon Web Services} (AWS) to carry out 
the simulations. 10 servers of the same type are used: \textit{c5n.4xlarge}. It 
has 16 cores and 42 GB of RAM memory.

\section{Correlation coefficients}
In this section we provide the simulations results for the correlation 
coefficients. Given a scenario and its dataset \textbf{X} of size $n \times k$, 
the correlation matrix between the main dimensions of \textbf{X} and 
the main dimensions of $\mathbf{MDS_{alg}}$ is computed. We are 
interested in the diagonal of the correlation matrix, expecting that 
the values are close to 1. 

\indent The length of the diagonal correlation matrix depends on the scenarios.
It can be:

\begin{itemize}
\item 1, when  $\lambda_1 = 15$ and $\lambda_2 = \cdots = \lambda_k = 1$.

\item 2, when$\lambda_1  = \lambda_2 = 15$ and 
$\lambda_3 = \cdots = \lambda_k = 1$
or $\lambda_1  = 15$, $\lambda_2 =10$ and $\lambda_3 = \cdots \lambda_k = 1$.

\item 4, when $\lambda_1  = \lambda_2 = \lambda_3 = \lambda_4 = 15$ and 
$\lambda_5 = \cdots = \lambda_k = 1$.

\item 0, when $\lambda_1 = \cdots = \lambda_k = 1$, i.e, 
\textit{noisy scenarios}.

\end{itemize}


\indent When the scenario is not a \textit{noisy one}, the correlation obtained
is greater than 0.9999, which indicates that the algorithms are able to 
capture the main dimensions of the data. 

\indent To provide a visual result, Figure \ref{divide_correlation_scenario_60} 
shows a boxplot of the correlation coefficients between the four main 
dimensions of the original dataset \textbf{X} and the four main dimensions 
of the MDS obtained with \textit{Divide and Conquer MDS}, $\mathbf{MDS_{Div}}$, 
for \textit{scenario\_id = 60}.\footnote{\textit{scenario\_id} = 60 
has the following configuration: $n=10^6$, 100 columns and 4 main dimensions 
with $\lambda_i = 15$, $i \in \{1, 2, 3, 4 \}$.\label{s60}} 
As we can see, all the values are close to 1. What shows Figure 
\ref{divide_correlation_scenario_60} happens for all the 
\textit{not-noisy scenarios} and all the algorithms. 

\begin{figure}[!h]
\centering
    \includegraphics[scale = 1.2]{./images/example_corr_divide.pdf}
    \caption{
    Boxplot for correlation coefficients between the main dimensions of \textbf{X} 
    and the main dimensions of $\mathbf{MDS_{Div}}$ for
    \textit{scenario\_id } = 60\textsuperscript{\ref{s60}}. \\
    The correlation is close to 1, indicating that the algorithm captures 
    the main dimensions of the original dataset \textbf{X}.
    }
    \label{divide_correlation_scenario_60}
\end{figure}


\indent Note that before calculating the correlation matrix, Procrustes 
transformation is performed (dilations are not allowed) so that both coordinate 
systems are the same.


\indent For the \textit{noisy scenarios}, Figure \ref{noisy_corr_divide} 
shows the boxplot for \textit{scenario\_id} = 51\footnote{\textit{scenario\_id} = 51 
has the following configuration: $n=10^6$, 10 columns and without any main 
dimensions.\label{s51}}  and 
\textit{scenario\_id} = 52\footnote{\textit{scenario\_id} = 52 
has the following configuration: $n=10^6$, 100 columns and without any main 
dimensions.\label{s52}}. These Figures have been generated using  
\textit{Divide and Conquer MDS}. Since these scenarios are pure noise, 
the correlation is low. 

\indent We do not observe any negative value since Procrustes alignment 
is done before calculating the correlation coefficients. Therefore, \textbf{X} 
and $\mathbf{MDS_{Div}}$ have the same orientation. 

\indent Again, what shows Figure \ref{noisy_corr_divide} also happens for 
the remaining \textit{noisy scenarios} and for the other two algorithms.


\begin{figure}
\centering
    \includegraphics[scale = 2.5]{./images/noisy_corr_divide.pdf}
    \caption{
    Boxplot for correlation coefficients between \textbf{X} and 
    $\mathbf{MDS_{Div}}$ for two different scenarios: 
    \textit{scenario\_id} = 51\textsuperscript{\ref{s51}} 
    and \textit{scenario\_id} = 52\textsuperscript{\ref{s52}}. \\ 
    Since all of them are noise, the correlation is low.\\ 
    \textit{scenario\_id} is on the top of each boxplot. 
    } 
    \label{noisy_corr_divide}
\end{figure}

\FloatBarrier


\section{Eigenvalues}
In this section we analyse how the eigenvalues approximate the standard 
deviation of the original variables. 

\indent Since the original dataset, \textbf{X}, is postmultiplied by a diagonal 
matrix $k \times k$ that contains $\lambda_1, \dots, \lambda_k$, then 
$\mbox{var}(X_i) = \lambda_i^2$ and $\mbox{sd}(X_i) = \lambda_i$, where
$X_i$ is the column $i$ from \textbf{X}.

\indent MDS should be able to capture the variance of the main dimensions 
through the eigenvalues. Let $\phi_1, \dots, \phi_t$ be the 
\textit{normalized eigenvalues} of the MDS configuration such that 
$\phi_1 > \phi_2 > \cdots > \phi_t$. The first highest
\textit{normalized eigenvalues} have to verify $\sqrt{\phi_j} \approx \lambda_j$.


\indent To check how the algorithms approximate the variance of the original 
data, we compute the bias and the Mean Square Error (MSE) for each scenario. 
We do not include the noisy ones. Remember that bias and MSE are calculated as 
follows:

$$
\widehat{\mbox{bias}} = \frac{1}{m}\sum_{i=1}^m \sqrt{\phi_{ij}} - \lambda_j = \overline{\sqrt{\phi_j}} - \lambda_j,
$$

$$
\widehat{\mbox{MSE}} = \frac{1}{m} \sum_{i = 1} ^m (\lambda_j - \sqrt{\phi_{ij}})^2.
$$


\indent Since we perform 100 simulations, $m = 100$. Depending on the 
scenario, there can be 1, 2 or 4 estimators. 

\indent Table \ref{mse_divide_one_dimensions} shows the $\widehat{\mbox{bias}}$
and the $\widehat{\mbox{MSE}}$ for \textit{Divide and Conquer MDS} and scenarios 
with one main dimension $\lambda = 15$. As we can see, the 
$\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ are ``low" for these 
scenarios and this algorithm.

\indent The remaining cases are in \Cref{chap:mes}. As long as number of 
dimensions increases, the $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ 
do the same. However, they seem to be in an acceptable range.

\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
 scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ \\ 
  \hline
  3 & 14.98 & -0.02 & 0.03 \\ 
  4 & 15.03 & 0.03 & 0.11 \\ 
  13 & 15.00 & -0.00 & 0.00 \\ 
  14 & 14.96 & -0.04 & 0.16 \\ 
  23 & 14.99 & -0.01 & 0.02 \\ 
  24 & 14.99 & -0.01 & 0.01 \\ 
  33 & 14.99 & -0.01 & 0.01 \\ 
  34 & 14.99 & -0.01 & 0.00 \\ 
  43 & 14.99 & -0.01 & 0.01 \\ 
  44 & 14.99 & -0.01 & 0.01 \\ 
  53 & 14.98 & -0.02 & 0.03 \\ 
  54 & 14.99 & -0.01 & 0.01 \\ 
   \hline
\end{tabular}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with one main dimension $\lambda_1 = 15$ for \textit{Divide and Conquer MDS}.}
\label{mse_divide_one_dimensions}
\end{table}

\indent Table \ref{mse_fast_one_dimensions} shows the $\widehat{\mbox{bias}}$ 
and $\widehat{\mbox{MSE}}$ for \textit{Fast MDS} and scenarios with one main 
dimension $\lambda = 15$. For this algorithm, the $\widehat{\mbox{MSE}}$ is
higher than for \textit{Divide and Conquer}, even in some cases 
(for instance \textit{scenario\_id}  = 33 \footnote{\textit{scenario\_id} = 33 
has the following configuration: $n=10^6$, 10 columns and 1 main dimension 
with $\lambda_1 = 15$.})
$\widehat{\mbox{MSE}}$ is high compared with the previous algorithm. 



\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
 scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ \\
  \hline
  3 & 14.85 & -0.15 & 2.27 \\ 
  4 & 15.01 & 0.01 & 0.01 \\ 
  13 & 14.91 & -0.09 & 0.76 \\ 
  14 & 15.10 & 0.10 & 0.93 \\ 
  23 & 14.96 & -0.04 & 0.14 \\ 
  24 & 15.03 & 0.03 & 0.07 \\ 
  33 & 14.33 & -0.67 & 44.82 \\ 
  34 & 15.09 & 0.09 & 0.76 \\ 
  43 & 15.00 & -0.00 & 0.00 \\ 
  44 & 15.00 & 0.00 & 0.00 \\ 
  53 & 14.86 & -0.14 & 1.88 \\ 
  54 & 14.90 & -0.10 & 1.02 \\ 
   \hline
\end{tabular}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with one main dimension $\lambda_1 = 15$ for \textit{Fast MDS}.}
\label{mse_fast_one_dimensions}
\end{table}


\indent One possible reason is the fact that the number of points used to do the 
Procrustes alignment are not enough. For this case, \textit{scenario\_id} = 33,
there are  30 points per partition, being 17 partitions in total. So, 
the alignment is done with 510 points out of $10^4$ (i.e, $5\%$ of the points). 
We have repeated the simulations for \textit{scenario\_id} = 33 with 60 
points per partition and the $\widehat{\mbox{MSE}}$ is lower (0.10). 

\indent The remaining cases for \textit{Fast MDS} are in \Cref{mse_fast}. Again,
some of the scenarios have a high $\widehat{\mbox{MSE}}$ compared with
\textit{Divide and Conquer}.

\indent Although \textit{Fast MDS} have higher $\widehat{\mbox{MSE}}$ values 
than the previous algorithm, we consider that they are acceptable. 

\indent Table \ref{mse_gower_one_dimensions} shows the $\widehat{\mbox{bias}}$ 
and $\widehat{\mbox{MSE}}$ for \textit{MDS based on Gower interpolation} 
and scenarios with one main dimension $\lambda = 15$. The remaining ones are in
\Cref{mse_fast}. The comments given to \textit{Divide and Conquer MDS} apply 
also to these cases. 

\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
 scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$\\
  \hline
  3 & 15.05 & 0.05 & 0.22 \\ 
  4 & 15.02 & 0.02 & 0.04 \\ 
  13 & 14.94 & -0.06 & 0.36 \\ 
  14 & 15.04 & 0.04 & 0.20 \\ 
  23 & 14.98 & -0.02 & 0.04 \\ 
  24 & 15.02 & 0.02 & 0.05 \\ 
  33 & 14.99 & -0.01 & 0.01 \\ 
  34 & 15.06 & 0.06 & 0.31 \\ 
  43 & 15.04 & 0.04 & 0.19 \\ 
  44 & 14.97 & -0.03 & 0.07 \\ 
  53 & 14.98 & -0.02 & 0.06 \\ 
  54 & 14.90 & -0.10 & 1.07 \\ 
   \hline
\end{tabular}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with one main dimension $\lambda_1 = 15$ for \textit{MDS based on Gower interpolation}.}
\label{mse_gower_one_dimensions}
\end{table}

\indent The algorithm that has the lowest error is the 
\textit{Divide and Conquer MDS}. Even though the other ones have higher errors,
especially \textit{Fast MDS}, we consider that they are good enough.

\indent Note that, we do not consider the \textit{noisy scenarios}, since all 
the directions have the same variance.






\FloatBarrier

\section{Time to compute MDS}
In this section we investigate if there exists an algorithm that is faster than the
other ones. Given the results of Table \ref{mean_elapsed_time}, it seems that
\textit{MDS based on Gower interpolation} has the lowest time. Table 
\ref{mean_elapsed_time} provides a rank between the methods: 
it seems that \textit{MDS based on Gower interpolation} is the fastest one. 
In second position it would be \textit{Fast MDS} and finally 
\textit{Divide and Conquer}.

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
 sample\_size & n\_dimensions & mean\_divide\_conquer & mean\_fast & mean\_gower \\ 
  \hline
  $10^3$ & 10 & 0.27 & 0.14 & 0.10 \\ 
  $10^3$ & 100 & 0.78 & 0.69 & 0.28 \\ 
  $3 \cdot 10^3$ & 10 & 0.78 & 0.32 & 0.16 \\ 
  $3 \cdot 10^3$ & 100 & 2.50 & 3.14 & 0.52 \\ 
  $5 \cdot 10^3$ & 10 & 1.37 & 0.54 & 0.20 \\ 
  $5 \cdot 10^3$ & 100 & 4.25 & 5.69 & 0.84 \\ 
  $10^4$ & 10 & 2.60 & 1.81 & 0.31 \\ 
  $10^4$ & 100 & 8.85 & 11.79 & 1.37 \\ 
  $10^5$ & 10 & 28.10 & 11.46 & 2.44 \\ 
  $10^5$ & 100 & 106.30 & 116.46 & 18.02 \\ 
  $10^6$ & 10 & 420.29 & 106.59 & 53.15 \\ 
  $10^6$ & 100 & 2365.46 & 1070.19 & 813.15 \\ 
   \hline
\end{tabular}
\caption{Mean of elapsed time (in seconds) to compute each algorithm.} 
\label{mean_elapsed_time}
\end{table}



\indent We do an \textsf{ANOVA} test using three factors: the sample 
size (which has 6 levels), the number of dimensions (which has 2 levels) 
and the algorithm (which has 3 levels). Instead of using the 
\textit{elapsed time} variable, we use its logarithm.

\indent Given the results of the  \textsf{ANOVA} test, which are in 
Table \ref{anova_elapsed_3_levels}, we can reject the null hypothesis that 
the three algorithms have the same expected elapsed time.

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
  algorithm    & 2 & 9283.73 & 4641.86 & 32143.99 & $<2e-16$ \\ 
  sample\_size  & 5 & 108572.93 & 21714.59 & 150369.26 & $<2e-16$ \\ 
  n\_dimensions & 1 & 12868.36 & 12868.36 & 89110.86 &  $<2e-16$ \\ 
  Residuals    & 17991 & 2598.05 & 0.14 &  &  \\ 
   \hline
\end{tabular}
\caption{Results for \textsf{ANOVA} test  for differences in $\log(\mbox{elapsed\_time})$ using algorithms, sample size and  num. dimensions as factors.} 
\label{anova_elapsed_3_levels}
\end{table}

\indent We fit a linear regression with these
variables and see the magnitude of the coefficients. In addition, we plot the
distribution of $\log(\mbox{elapsed\_time})$ for all the algorithms.

\indent Table \ref{lm_all_variables} contains the value of the coefficients.
As long as either the sample size or the data dimensions increase the 
coefficients do the same (and so the time needed). Looking at the 
values for the \textit{algorithm} variable, it seems that 
\textit{MDS base on Gower interpolation} is the fastest. On the other hand, 
\textit{Divide and Conquer} is the slowest one.

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -1.4058 & 0.0085 & -165.44 & $<2e-16$ \\ 
  algorithmfast & -0.4313 & 0.0069 & -62.17 & $<2e-16$ \\ 
  algorithmgower & -1.6926 & 0.0069 & -243.96 & $<2e-16$ \\ 
  sample\_size3000 & 0.9473 & 0.0098 & 96.54 & $<2e-16$ \\ 
  sample\_size5000 & 1.4434 & 0.0098 & 147.10 & $<2e-16$ \\ 
  sample\_size10000 & 2.1505 & 0.0098 & 219.17 & $<2e-16$ \\ 
  sample\_size1e+05 & 4.4286 & 0.0098 & 451.35 & $<2e-16$ \\ 
  sample\_size1e+06 & 7.2782 & 0.0098 & 741.78 & $<2e-16$ \\ 
  n\_dimensions100 & 1.6910 & 0.0057 & 298.51 & $<2e-16$ \\ 
   \hline
\end{tabular}
\caption{Linear model for response $\log(\mbox{elapsed\_time})$.} 
\label{lm_all_variables}
\end{table}

\indent Figure \ref{elapsed_time_1000_part1} and
Figure \ref{elapsed_time_1000_part2} show the estimated density of the
elapsed time for each algorithm and each \textit{scenario\_id} for $n = 10^3$. 
As we can see, \textit{MDS based on Gower interpolation} is the fastest 
algorithm, especially when 100 dimensions are required. The remaining figures, 
i.e $n>10^3$,  are in \Cref{elapsed_time_alg}. 

\indent One interesting thing that we can observe 
from \Cref{elapsed_time_alg} is the fact that the elapsed time grows as long 
as the sample size does, especially form \textit{Divide and Conquer MDS}.
However, \textit{MDS based on Gower interpolation} and \textit{Fast MDS} 
provide a really good time even though the sample size is large, being
\textit{MDS based on Gower interpolation} the fastest one. So, we can consider
both algorithms efficient, since they are able to compute MDS is a reasonable
amount of time.


\begin{figure}[h]
\centering
    \includegraphics[scale=2]{./images/elapsed_time_1000_part1.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and 
    the first five \textit{scenario\_id} of $n=10^3$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
    \label{elapsed_time_1000_part1}
\end{figure}



\begin{figure}[h]
\centering
    \includegraphics[scale=2]{./images/elapsed_time_1000_part2.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and 
    the last five \textit{scenario\_id} of $n=10^3$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
    \label{elapsed_time_1000_part2}
\end{figure}


\chapter{Conclusions}
The goal of this master's thesis was to find an algorithm able to compute a MDS
configuration when dealing with large datasets in an ``efficient" way, i.e, 
such an algorithm should be fast enough to get the configuration.

\indent Even though our first method, \textit{Divide and Conquer MDS}, is
the slowest one, it is able to obtain a low-dimensional configuration. In 
addition, it has a really good property that the other two algorithms do not 
have: it is able to capture the variance of the original data quite well. 

\indent \textit{Fast MDS} provides an improvement of timing, being 
faster than \textit{Divide and Conquer MDS}. However,  it is based on 
recursive programming. The problem with these kind of algorithms is that they
can consume a lot of memory.

\indent In addition, a carefully selection of the number of points to perform 
Procrustes alignment has to be done, since the $\widehat{\mbox{MSE}}$
can be high, as it happen with our simulation study.

\indent A really good algorithm is \textit{MDS based on Gower interpolation},
since it provides a MDS configuration in a short amount of time with low errors 
and its implementation is easy. Apart from this, it does not need to do 
any Procrustes transformation, which save time and memory. Therefore, this 
is the algorithm to obtain MDS with large datasets.

\indent Observe that this algorithm does essentially what classical Statistics 
advises: when your population is too large, take a sample of it. \\

\noindent{\textbf{Problem encountered during the development of the thesis}}

\indent Basically we have faced two kind of problems, which are:

\begin{itemize}
\item Computational problems: when working with large datasets, we suffered from
consuming all RAM of the servers. Especially when doing Procrustes for aligning
the original data and the MDS for $n$ large. The solution to it was to partition
the process into pieces that the servers could manage without consuming all RAM.

\item Procrustes packages: even though \textsf{R} has a lot of packages that
allows to compute Procrustes, they did not fulfilled our goals. The reasons are
either because the output is not well specified or because some of the 
transformations (mainly dilations or translations) were not included. We 
recommend to use \textsf{MCMCpack} package.

\end{itemize}

\noindent{\textbf{Future research}}

\indent The algorithms that we have developed are implemented in \textsf{R},
which is a good language to do prototypes. However, this is not the best
programming language to be used. So, we recommend to implement them in a robust 
programming language such as \textsf{C}, \textsf{C++} or \textsf{Java}.

\indent Tuning of parameters: as we have seen in \textit{Fast MDS}, the 
$\widehat{\mbox{MSE}}$ depends on the number of points to perform Procrustes
alignment. So, a (simulation) study on this parameter should be done in order 
to obtain the ``optimal" value for which the $\widehat{\mbox{MSE}}$ is as good
as the one for \textit{Divide and Conquer}.

\indent On the other hand, it might be that \textit{Divide and Conquer} uses
more than necessary points to perform Procrustes alignment. Reducing the
number of points would preserve the $\widehat{\mbox{MSE}}$ while improving the
time needed to get the MDS configuration.

\indent Given that the world is talking about Big Data, it is a good 
opportunity to challenge the algorithms and use them with Spark/Hadoop. 
Actually, the initial idea of this thesis was to use them with a Spark DB that 
contains millions of chess games. Due to lack of time, we could not do so. 

\indent In the same line of Spark/Hadoop, an implementation based on 
\textit{map-reduce} would speed up the algorithms, reducing the time needed
to get the low-dimensional configuration. \\

\noindent{\textbf{Acknowledgments}}

I would like to express my gratitude to \textit{Pedro Delicado}, since he helped
me to do this thesis. There have been some difficult issues in which his help
allowed me to move on and solve them. Also, I would like to thank
\textit{Roger Devesa}, since he has helped me with reviewing recursive 
programming and with implementing \textit{Fast MDS}.



\bibliographystyle{chicago}
\bibliography{bibliography.bib}


\appendix 




\chapter{Bias and MSE for eigenvalues}
\section{Divide and Conquer MDS}
\label{chap:mes}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
 scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ & $\overline{\sqrt{\phi_2}}$ & $\widehat{\mbox{bias}_2}$ & $\widehat{\mbox{MSE}_2}$ \\ 
  \hline
  5 & 15.41 & 0.41 & 17.12 & 14.59 & 0.41 & 16.99 \\ 
  6 & 15.41 & 0.41 & 17.08 & 14.54 & 0.41 & 20.88 \\ 
  15 & 15.40 & 0.40 & 15.99 & 14.55 & 0.40 & 19.89 \\ 
  16 & 15.39 & 0.39 & 15.03 & 14.54 & 0.39 & 21.59 \\ 
  25 & 15.41 & 0.41 & 16.71 & 14.55 & 0.41 & 20.01 \\ 
  26 & 15.41 & 0.41 & 16.94 & 14.57 & 0.41 & 18.79 \\ 
  35 & 15.39 & 0.39 & 15.57 & 14.54 & 0.39 & 21.13 \\ 
  36 & 15.42 & 0.42 & 17.60 & 14.57 & 0.42 & 18.22 \\ 
  45 & 15.40 & 0.40 & 15.77 & 14.56 & 0.40 & 19.42 \\ 
  46 & 15.41 & 0.41 & 16.85 & 14.56 & 0.41 & 19.15 \\ 
  55 & 15.40 & 0.40 & 16.00 & 14.56 & 0.40 & 19.52 \\ 
  56 & 15.41 & 0.41 & 16.73 & 14.56 & 0.41 & 18.96 \\ 
   \hline
\end{tabular}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with two main dimensions $\lambda_1 = 15$ and $\lambda_2 = 15$ for \textit{Divide and Conquer MDS}.}
\end{table}

\FloatBarrier

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
 scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ & $\overline{\sqrt{\phi_2}}$ & $\widehat{\mbox{bias}_2}$ & $\widehat{\mbox{MSE}_2}$ \\ 
  \hline
 7 & 14.96 & -0.04 & 0.15 & 9.95 & -0.05 & 0.27 \\ 
 8 & 14.98 & -0.02 & 0.02 & 9.92 & -0.08 & 0.57 \\ 
 17 & 15.01 & 0.01 & 0.02 & 9.98 & -0.02 & 0.05 \\ 
 18 & 15.03 & 0.03 & 0.08 & 9.98 & -0.02 & 0.04 \\ 
 27 & 15.00 & 0.00 & 0.00 & 9.99 & -0.01 & 0.02 \\ 
 28 & 15.01 & 0.01 & 0.01 & 9.97 & -0.03 & 0.11 \\ 
 37 & 15.01 & 0.01 & 0.01 & 9.97 & -0.03 & 0.06 \\ 
 38 & 15.01 & 0.01 & 0.00 & 10.00 & -0.00 & 0.00 \\ 
 47 & 15.00 & 0.00 & 0.00 & 9.98 & -0.02 & 0.05 \\ 
 48 & 15.01 & 0.01 & 0.00 & 9.98 & -0.02 & 0.04 \\ 
 57 & 15.00 & -0.00 & 0.00 & 9.97 & -0.03 & 0.08 \\ 
 58 & 15.00 & 0.00 & 0.00 & 9.98 & -0.02 & 0.03 \\ 
   \hline
\end{tabular}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with two main dimensions $\lambda_1 = 15$ and $\lambda_2 = 10$ for \textit{Divide and Conquer MDS}.}
\end{table}

\FloatBarrier

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{rrrrrrrrrrrrr}
scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ & $\overline{\sqrt{\phi_2}}$ & $\widehat{\mbox{bias}_2}$ & $\widehat{\mbox{MSE}_2}$ & $\overline{\sqrt{\phi_3}}$ & $\widehat{\mbox{bias}_3}$ & $\widehat{\mbox{MSE}_3}$ & $\overline{\sqrt{\phi_4}}$ & $\widehat{\mbox{bias}_4}$ & $\widehat{\mbox{MSE}_4}$\\   
  \hline
  9 & 15.88 & 0.88 & 77.09 & 15.26 & 0.26 & 6.94 & 14.71 & -0.29 & 8.40 & 14.08 & -0.92 & 84.94 \\ 
  10 & 15.89 & 0.89 & 80.08 & 15.26 & 0.26 & 6.52 & 14.70 & -0.30 & 8.82 & 14.09 & -0.91 & 83.59 \\ 
  19 & 15.90 & 0.90 & 80.98 & 15.26 & 0.26 & 6.87 & 14.68 & -0.32 & 10.37 & 14.04 & -0.96 & 91.72 \\ 
  20 & 15.88 & 0.88 & 76.61 & 15.25 & 0.25 & 6.02 & 14.70 & -0.30 & 9.07 & 14.04 & -0.96 & 91.55 \\ 
  29 & 15.87 & 0.87 & 76.05 & 15.26 & 0.26 & 6.58 & 14.68 & -0.32 & 9.99 & 14.06 & -0.94 & 87.55 \\ 
  30 & 15.87 & 0.87 & 75.86 & 15.24 & 0.24 & 5.83 & 14.68 & -0.32 & 10.21 & 14.07 & -0.93 & 86.76 \\ 
  39 & 15.89 & 0.89 & 79.01 & 15.25 & 0.25 & 6.07 & 14.69 & -0.31 & 9.80 & 14.06 & -0.94 & 87.95 \\ 
  40 & 15.91 & 0.91 & 82.54 & 15.25 & 0.25 & 6.32 & 14.69 & -0.31 & 9.64 & 14.06 & -0.94 & 87.92 \\ 
  49 & 15.89 & 0.89 & 78.58 & 15.25 & 0.25 & 6.10 & 14.68 & -0.32 & 9.98 & 14.06 & -0.94 & 87.77 \\ 
  50 & 15.89 & 0.89 & 79.78 & 15.25 & 0.25 & 6.30 & 14.69 & -0.31 & 9.69 & 14.07 & -0.93 & 86.87 \\ 
  59 & 15.89 & 0.89 & 78.65 & 15.25 & 0.25 & 6.09 & 14.68 & -0.32 & 9.95 & 14.06 & -0.94 & 88.05 \\ 
  60 & 15.89 & 0.89 & 79.73 & 15.25 & 0.25 & 6.40 & 14.69 & -0.31 & 9.57 & 14.07 & -0.93 & 86.68 \\ 
   \hline
\end{tabular}}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with four main dimensions $\lambda_i = 15$ $i \in \{1,2,3,4\}$ for \textit{Divide and Conquer MDS}.}
\end{table}

\FloatBarrier

\newpage
\section{Fast MDS}
\label{mse_fast}


\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ & $\overline{\sqrt{\phi_2}}$ & $\widehat{\mbox{bias}_2}$ & $\widehat{\mbox{MSE}_2}$ \\ 
  \hline
  5 & 16.06 & 1.06 & 112.12 & 13.79 & -1.21 & 145.86 \\ 
  6 & 15.41 & 0.41 & 17.21 & 14.56 & -0.44 & 19.44 \\ 
  15 & 15.71 & 0.71 & 49.84 & 14.31 & -0.69 & 46.93 \\ 
  16 & 15.40 & 0.40 & 15.93 & 14.39 & -0.61 & 37.06 \\ 
  25 & 15.50 & 0.50 & 24.80 & 14.43 & -0.57 & 32.32 \\ 
  26 & 15.45 & 0.45 & 20.39 & 14.46 & -0.54 & 29.07 \\ 
  35 & 16.52 & 1.52 & 231.64 & 13.35 & -1.65 & 271.33 \\ 
  36 & 15.46 & 0.46 & 21.30 & 14.49 & -0.51 & 25.90 \\ 
  45 & 15.45 & 0.45 & 20.62 & 14.37 & -0.63 & 40.08 \\ 
  46 & 15.41 & 0.41 & 17.03 & 14.55 & -0.45 & 20.34 \\ 
  55 & 15.67 & 0.67 & 45.00 & 14.27 & -0.73 & 52.78 \\ 
  56 & 15.35 & 0.35 & 12.11 & 14.57 & -0.43 & 18.11 \\ 
   \hline
\end{tabular}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with two main dimensions $\lambda_1 = 15$ and $\lambda_2 = 15$ for \textit{Fast MDS}.}
\end{table}

\FloatBarrier

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ & $\overline{\sqrt{\phi_2}}$ & $\widehat{\mbox{bias}_2}$ & $\widehat{\mbox{MSE}_2}$ \\  
  \hline
  7 & 15.00 & 0.00 & 0.00 & 9.82 & -0.18 & 3.22 \\ 
  8 & 14.96 & -0.04 & 0.17 & 9.93 & -0.07 & 0.56 \\ 
  17 & 15.14 & 0.14 & 2.00 & 9.88 & -0.12 & 1.37 \\ 
  18 & 14.99 & -0.01 & 0.00 & 9.98 & -0.02 & 0.04 \\ 
  27 & 14.91 & -0.09 & 0.75 & 9.95 & -0.05 & 0.21 \\ 
  28 & 14.89 & -0.11 & 1.29 & 9.90 & -0.10 & 0.92 \\ 
  37 & 15.10 & 0.10 & 0.93 & 9.64 & -0.36 & 13.12 \\ 
  38 & 14.97 & -0.03 & 0.10 & 9.96 & -0.04 & 0.12 \\ 
  47 & 15.02 & 0.02 & 0.05 & 9.93 & -0.07 & 0.47 \\ 
  48 & 14.99 & -0.01 & 0.02 & 10.02 & 0.02 & 0.03 \\ 
  57 & 14.88 & -0.12 & 1.38 & 9.91 & -0.09 & 0.74 \\ 
  58 & 15.03 & 0.03 & 0.09 & 9.99 & -0.01 & 0.02 \\ 
   \hline
\end{tabular}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with two main dimensions $\lambda_1 = 15$ and $\lambda_2 = 10$ for \textit{Fast MDS}.}
\end{table}

\FloatBarrier

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{rrrrrrrrrrrrr}
scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ & $\overline{\sqrt{\phi_2}}$ & $\widehat{\mbox{bias}_2}$ & $\widehat{\mbox{MSE}_2}$ & $\overline{\sqrt{\phi_3}}$ & $\widehat{\mbox{bias}_3}$ & $\widehat{\mbox{MSE}_3}$ & $\overline{\sqrt{\phi_4}}$ & $\widehat{\mbox{bias}_4}$ & $\widehat{\mbox{MSE}_4}$\\ 
  \hline
  9 & 17.43 & 2.43 & 590.46 & 15.58 & 0.58 & 33.32 & 13.74 & -1.26 & 158.12 & 11.99 & -3.01 & 903.29 \\ 
  10 & 15.90 & 0.90 & 80.54 & 15.25 & 0.25 & 6.49 & 14.70 & -0.30 & 8.79 & 14.08 & -0.92 & 84.90 \\ 
  19 & 16.40 & 1.40 & 195.78 & 15.27 & 0.27 & 7.32 & 14.31 & -0.69 & 47.00 & 13.33 & -1.67 & 280.08 \\ 
  20 & 16.01 & 1.01 & 102.73 & 15.32 & 0.32 & 10.08 & 14.65 & -0.35 & 11.97 & 13.87 & -1.13 & 127.87 \\ 
  29 & 16.14 & 1.14 & 130.55 & 15.29 & 0.29 & 8.64 & 14.53 & -0.47 & 22.40 & 13.81 & -1.19 & 141.75 \\ 
  30 & 16.02 & 1.02 & 104.48 & 15.32 & 0.32 & 10.27 & 14.63 & -0.37 & 13.72 & 14.00 & -1.00 & 99.58 \\ 
  39 & 17.96 & 2.96 & 875.12 & 15.53 & 0.53 & 27.71 & 13.70 & -1.30 & 169.28 & 11.42 & -3.58 & 1281.54 \\ 
  40 & 16.01 & 1.01 & 102.49 & 15.29 & 0.29 & 8.26 & 14.69 & -0.31 & 9.35 & 13.93 & -1.07 & 114.86 \\ 
  49 & 16.14 & 1.14 & 129.03 & 15.39 & 0.39 & 15.54 & 14.63 & -0.37 & 13.90 & 13.94 & -1.06 & 112.16 \\ 
  50 & 15.98 & 0.98 & 96.04 & 15.25 & 0.25 & 6.28 & 14.61 & -0.39 & 15.18 & 13.97 & -1.03 & 106.71 \\ 
  59 & 16.29 & 1.29 & 165.16 & 15.22 & 0.22 & 4.73 & 14.37 & -0.63 & 39.55 & 13.44 & -1.56 & 243.94 \\ 
  60 & 15.98 & 0.98 & 95.75 & 15.33 & 0.33 & 10.87 & 14.70 & -0.30 & 9.21 & 14.06 & -0.94 & 87.68 \\ 
   \hline
\end{tabular}}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with four main dimensions $\lambda_i = 15$ $i \in \{1,2,3,4\}$ for \textit{Fast MDS}.}
\end{table}


\FloatBarrier

\section{MDS based on Gower interpolation}
\label{mse_gower}
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ & $\overline{\sqrt{\phi_2}}$ & $\widehat{\mbox{bias}_2}$ & $\widehat{\mbox{MSE}_2}$ \\ 
  \hline
  5 & 15.45 & 0.45 & 20.49 & 14.63 & -0.37 & 13.52 \\ 
  6 & 15.41 & 0.41 & 17.04 & 14.56 & -0.44 & 19.36 \\ 
  15 & 15.42 & 0.42 & 17.23 & 14.59 & -0.41 & 16.71 \\ 
  16 & 15.35 & 0.35 & 12.08 & 14.47 & -0.53 & 28.25 \\ 
  25 & 15.45 & 0.45 & 19.82 & 14.55 & -0.45 & 19.91 \\ 
  26 & 15.40 & 0.40 & 16.06 & 14.52 & -0.48 & 23.04 \\ 
  35 & 15.36 & 0.36 & 13.23 & 14.53 & -0.47 & 22.56 \\ 
  36 & 15.40 & 0.40 & 15.75 & 14.53 & -0.47 & 22.29 \\ 
  45 & 15.36 & 0.36 & 12.93 & 14.47 & -0.53 & 27.88 \\ 
  46 & 15.38 & 0.38 & 14.27 & 14.60 & -0.40 & 16.20 \\ 
  55 & 15.37 & 0.37 & 13.86 & 14.52 & -0.48 & 22.97 \\ 
  56 & 15.33 & 0.33 & 11.15 & 14.57 & -0.43 & 18.74 \\ 
   \hline
\end{tabular}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with two main dimensions $\lambda_1 = 15$ and $\lambda_2 = 15$ for \textit{MDS based on Gower interpolation}.}
\end{table}


\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ & $\overline{\sqrt{\phi_2}}$ & $\widehat{\mbox{bias}_2}$ & $\widehat{\mbox{MSE}_2}$ \\ 
  \hline
7 & 14.95 & -0.05 & 0.25 & 9.95 & -0.05 & 0.24 \\ 
8 & 14.96 & -0.04 & 0.18 & 9.93 & -0.07 & 0.51 \\ 
17 & 15.05 & 0.05 & 0.27 & 9.98 & -0.02 & 0.06 \\ 
18 & 15.02 & 0.02 & 0.04 & 9.99 & -0.01 & 0.01 \\ 
27 & 14.98 & -0.02 & 0.05 & 9.98 & -0.02 & 0.03 \\ 
28 & 14.92 & -0.08 & 0.63 & 9.90 & -0.10 & 1.05 \\ 
37 & 14.98 & -0.02 & 0.02 & 9.98 & -0.02 & 0.04 \\ 
38 & 14.97 & -0.03 & 0.07 & 9.97 & -0.03 & 0.11 \\ 
47 & 15.05 & 0.05 & 0.28 & 9.97 & -0.03 & 0.07 \\ 
48 & 15.00 & -0.00 & 0.00 & 10.00 & -0.00 & 0.00 \\ 
57 & 14.89 & -0.11 & 1.19 & 9.98 & -0.02 & 0.05 \\ 
58 & 15.03 & 0.03 & 0.09 & 9.98 & -0.02 & 0.06 \\ 
   \hline
\end{tabular}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with two main dimensions $\lambda_1 = 15$ and $\lambda_2 = 10$ for \textit{MDS based on Gower interpolation}.}
\end{table}



\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{rrrrrrrrrrrrr}
scenario\_id & $\overline{\sqrt{\phi_1}}$ & $\widehat{\mbox{bias}_1}$ & $\widehat{\mbox{MSE}_1}$ & $\overline{\sqrt{\phi_2}}$ & $\widehat{\mbox{bias}_2}$ & $\widehat{\mbox{MSE}_2}$ & $\overline{\sqrt{\phi_3}}$ & $\widehat{\mbox{bias}_3}$ & $\widehat{\mbox{MSE}_3}$ & $\overline{\sqrt{\phi_4}}$ & $\widehat{\mbox{bias}_4}$ & $\widehat{\mbox{MSE}_4}$\\  
  \hline
  9 & 15.89 & 0.89 & 79.83 & 15.28 & 0.28 & 7.68 & 14.73 & -0.27 & 7.38 & 14.08 & -0.92 & 84.17 \\ 
  10 & 15.90 & 0.90 & 80.72 & 15.26 & 0.26 & 7.00 & 14.70 & -0.30 & 9.07 & 14.10 & -0.90 & 81.70 \\ 
  19 & 15.83 & 0.83 & 68.94 & 15.20 & 0.20 & 4.00 & 14.63 & -0.37 & 13.55 & 14.03 & -0.97 & 94.05 \\ 
  20 & 15.86 & 0.86 & 73.48 & 15.24 & 0.24 & 5.66 & 14.67 & -0.33 & 10.78 & 14.02 & -0.98 & 96.87 \\ 
  29 & 15.88 & 0.88 & 76.87 & 15.24 & 0.24 & 5.91 & 14.66 & -0.34 & 11.64 & 14.08 & -0.92 & 84.24 \\ 
  30 & 15.93 & 0.93 & 86.05 & 15.27 & 0.27 & 7.03 & 14.67 & -0.33 & 10.97 & 14.12 & -0.88 & 77.32 \\ 
  39 & 15.87 & 0.87 & 75.93 & 15.23 & 0.23 & 5.30 & 14.67 & -0.33 & 10.59 & 14.05 & -0.95 & 91.13 \\ 
  40 & 15.90 & 0.90 & 80.39 & 15.26 & 0.26 & 6.61 & 14.68 & -0.32 & 10.22 & 14.07 & -0.93 & 86.34 \\ 
  49 & 15.97 & 0.97 & 94.74 & 15.30 & 0.30 & 9.03 & 14.70 & -0.30 & 8.94 & 14.13 & -0.87 & 74.89 \\ 
  50 & 15.88 & 0.88 & 77.34 & 15.22 & 0.22 & 5.03 & 14.66 & -0.34 & 11.33 & 14.10 & -0.90 & 81.27 \\ 
  59 & 15.88 & 0.88 & 77.43 & 15.20 & 0.20 & 4.12 & 14.63 & -0.37 & 13.67 & 14.03 & -0.97 & 94.66 \\ 
  60 & 15.97 & 0.97 & 94.19 & 15.33 & 0.33 & 10.85 & 14.70 & -0.30 & 9.29 & 14.07 & -0.93 & 86.59 \\ 
   \hline
\end{tabular}}
\caption{Estimator, $\widehat{\mbox{bias}}$ and $\widehat{\mbox{MSE}}$ for scenarios with four main dimensions $\lambda_i = 15$ $i \in \{1,2,3,4\}$ for \textit{Fast MDS}.}
\end{table}

\chapter{Time required to compute MDS configuration}
\label{elapsed_time_alg}

\begin{figure}[h]
\centering
    \includegraphics[scale=1]{./images/elapsed_time_3000_part1.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and the five 
    first \textit{scenario\_id} of $n=3 \cdot 10^3$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
\end{figure}

\FloatBarrier


\begin{figure}[h]
\centering
    \includegraphics[scale=1]{./images/elapsed_time_3000_part2.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and the five 
    last \textit{scenario\_id} of $n=3 \cdot 10^3$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
    \label{elapsed_time_3000}
\end{figure}

\FloatBarrier



\begin{figure}[h]
\centering
    \includegraphics[scale=2]{./images/elapsed_time_5000_part1.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and the five 
    first \textit{scenario\_id} of $n=5 \cdot 10^3$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
\end{figure}

\FloatBarrier


\begin{figure}[h]
\centering
    \includegraphics[scale=2]{./images/elapsed_time_5000_part2.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and the five 
    last \textit{scenario\_id} of $n=5 \cdot 10^3$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
\end{figure}

\FloatBarrier


\begin{figure}[h]
\centering
    \includegraphics[scale=2]{./images/elapsed_time_10000_part1.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and the five 
    first \textit{scenario\_id} of $n=10^4$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
\end{figure}

\FloatBarrier

\begin{figure}[h]
\centering
    \includegraphics[scale=2]{./images/elapsed_time_10000_part2.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and the five 
    last \textit{scenario\_id} of $n=10^4$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
\end{figure}

\FloatBarrier

\begin{figure}[h]
\centering
    \includegraphics[scale=2]{./images/elapsed_time_100000_part1.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and the five 
    first \textit{scenario\_id} of $n=10^5$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
\end{figure}

\FloatBarrier

\begin{figure}[h]
\centering
    \includegraphics[scale=2]{./images/elapsed_time_100000_part2.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and the five 
    last \textit{scenario\_id} of $n=10^5$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
\end{figure}

\FloatBarrier

\begin{figure}[h]
\centering
    \includegraphics{./images/elapsed_time_1000000_part1.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and the five 
    first \textit{scenario\_id} of $n=10^6$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
\end{figure}

\FloatBarrier


\begin{figure}[h]
\centering
    \includegraphics{./images/elapsed_time_1000000_part2.pdf}
    \caption{
    Estimated density of elapsed time (in sec.) for each algorithm and the five 
    last \textit{scenario\_id} of $n=10^6$.\\
    \textit{scenario\_id} is on the top of each plot.
    }
\end{figure}

\FloatBarrier

\chapter{Code}
\label{chap:code}

\section{Divide and Conquer MDS}
\begin{lstlisting}
divide_conquer.mds <- function(
  x,
  l,
  s,
  metric
){
  
  # List positions
  ls_positions = list()
  list_eigenvalues = list()
  i_eigen = 1
  
  # Initial parameters
  p = ceiling(2*nrow(x)/l)
  groups = sample(x = p, size = nrow(x), replace = TRUE)
  groups = sort(groups)
  unique_group = unique(groups)
  total_groups = length(unique_group)
  
  for(k in 1:total_groups){
    # Getting the group that is being processed
    current_group = unique_group[k]
    x_positions_current_group = which(groups == current_group)
    ls_positions[[k]] = x_positions_current_group
    
    # Take the data in the following way:
    #   If it is the first iteration, take the data from first group
    #   else, take the data from k-1 and k groups
    if(k == 1){
      filter_rows_by_position = x_positions_current_group
      rows_processed = x_positions_current_group
    }else{
      
      rows_processed = c(
        rows_processed,
        x_positions_current_group
      )
      previous_group = unique_group[k-1]
      x_positions_previous_group =  which(groups == previous_group)
      
      # Rows to be filtered
      filter_rows_by_position = c(
        x_positions_previous_group,
        x_positions_current_group
      )
      
    }
    
    # Matrix to apply MDS
    submatrix_data = x[filter_rows_by_position, ]
    
    # Calculate distance
    distance_matrix = cluster::daisy(
      x = submatrix_data,
      metric = metric
    )
    
    # Applying MDS to the submatrix of data
    cmd_eig = stats::cmdscale(
      d = distance_matrix, 
      k = s,
      eig = TRUE
    )
    
    mds_iteration = cmd_eig$points
    if(p%%2 == 0){
      if(k%%2 == 0){
        list_eigenvalues[[i_eigen]] = cmd_eig$eig/nrow(submatrix_data)
        i_eigen = i_eigen + 1
      }
    }else{
      if(k %% 2 == 1){
        list_eigenvalues[[i_eigen]] = cmd_eig$eig/nrow(submatrix_data)
        i_eigen = i_eigen + 1
      }
    }

        row.names(mds_iteration) = row.names(submatrix_data)
    
    if(k == 1){
      # Define cum-MDS as MDS(1)
      cum_mds = mds_iteration
      
    }else{
      # Take the result of MDS(k-1) obtained with k-1 and k
      rn_prev = row.names(x)[x_positions_previous_group]  
      rn_cur = row.names(x)[x_positions_current_group] 
      pos_previous_group_current_mds = row.names(mds_iteration) %in% rn_prev 
      pos_current_group_current_mds = row.names(mds_iteration) %in% rn_cur
      mds_previous = mds_iteration[pos_previous_group_current_mds,]  
      mds_current = mds_iteration[pos_current_group_current_mds,]
      
      # From cum-MDS take the result of group k-1
      positions_cum_sum_previous = which(row.names(cum_mds) %in% rn_prev
      cum_mds_previous = cum_mds[positions_cum_sum_previous, ] 
      
      
      # Apply Procrustes transformation
      procrustes_result =  MCMCpack::procrustes(
        X = mds_previous, #The matrix to be transformed
        Xstar = cum_mds_previous, # target matrix
        translation = TRUE, 
        dilation = TRUE
      )
      
      rotation_matrix = procrustes_result$R
      dilation = procrustes_result$s
      translation = procrustes_result$tt
      ones_vector = rep(1, nrow(mds_current)) 
      translation_matrix = ones_vector %*% t(translation)
      
      # Transform the data for the k-th group  
      cum_mds_current = dilation * mds_current %*% rotation_matrix + 
        translation_matrix
      
      cum_mds = rbind(
        cum_mds,
        cum_mds_current
      )
      
    }
    
  }
  
  # Reordering
  reording_permutation = match(1:nrow(x), rows_processed)
  cum_mds = cum_mds[reording_permutation, ]
  
  
  return(
    list(
      points = cum_mds,
      eig = list_eigenvalues 
    )
  )
}
\end{lstlisting}

\newpage
\section{Fast MDS}
\begin{lstlisting}
fast.mds <- function(
  x,
  l,
  s,
  k,
  metric
){
  # Initial Parametres
  list_matrix = list()
  list_index = list()
  list_mds = list()
  list_mds_align = list()
  list_number_dimensions = list()
  list_eigenvalues = list()
  
  sub_sample_size = k * s
  n = nrow(x)
  
  # Partition into p matrices
  # When doing the partitions it can happen that there are so many matrices
  # that s*k < nrow(x_i). In this case, we do a sampling again, otherwise
  # cmdscale have problems
  
  p = ceiling(l/sub_sample_size)
  observations_division = sample(x = p, size = nrow(x), replace = TRUE)
  observations_division = sort(observations_division)
  min_sample_size = min(table(observations_division))
  
  while( min_sample_size < sub_sample_size && p > 1){
    p = p - 1 
    observations_division = sample(x = p, size = nrow(x), replace = TRUE)
    observations_division = sort(observations_division)
    min_sample_size = min(table(observations_division))
  }
  
  # Partition into p submatrices
  for(i_group in 1:p){
    ind = which(observations_division == i_group)
    list_matrix[[i_group]] = x[ind, ]
  }
  
  able_to_do_mds = n/p <= l | p == 1
  
  
  # We can do MDS
  if(able_to_do_mds == TRUE){
    for (i_group in 1:p) {
      matrix_filter = list_matrix[[i_group]]
      
      # MDS for each submatrix
      distance_matrix = daisy(
        x = matrix_filter,
        metric = metric
      )
      
      cmd_eig = stats::cmdscale(
        d = distance_matrix, 
        k = s,
        eig = TRUE
      )
      
      # Storing the eigenvalues
      list_mds[[i_group]] = cmd_eig$points
      list_eigenvalues[[i_group]] = cmd_eig$eig
      
      # Subsample
      sample_size = sub_sample_size
      if(sample_size > length( row.names(matrix_filter ) ) ){
        sample_size = length( row.names(matrix_filter ) )
      }
      
      
      list_index[[i_group]] = sample(
        x = row.names(matrix_filter), 
        size = sample_size, 
        replace = FALSE
      )
      
      
      # Building x_M_align
      ind_M = which(row.names(x) %in% list_index[[i_group]])
      if(i_group == 1){
        x_M_align = x[ind_M, ]
      }else{
        x_M_align = rbind(
          x_M_align,
          x[ind_M, ]
        )
      }
      
    }
    
    # M_align: MDS over x_M_align
    distance_matrix_M = distance_matrix = daisy(
      x = x_M_align,
      metric = metric
    )
    
    M_align = stats::cmdscale(
      d = distance_matrix_M, 
      k = s
    )
    
    # Global alignment
    for(i_group in 1:p){
      row_names = list_index[[i_group]]
      
      ind_M = which(row.names(M_align) %in% row_names)
      M_align_filter =  M_align[ind_M, ]
      
      di = list_mds[[i_group]]
      ind_mds = which(row.names( di ) %in% row_names)
      di_filter = di[ind_mds, ]
      
      # Alignment
      procrustes_result =  MCMCpack::procrustes(
        X = di_filter, #The matrix to be transformed
        Xstar = M_align_filter, # target matrix
        translation = TRUE, 
        dilation = TRUE
      )
      
      rotation_matrix = procrustes_result$R
      dilation = procrustes_result$s
      translation = procrustes_result$tt
      ones_vector = rep(1, nrow(di)) 
      translation_matrix = ones_vector %*% t(translation)
      
      
      tranformation_di = dilation * di %*% rotation_matrix + translation_matrix
      
      
      # Append
      if(i_group == 1){
        Z = tranformation_di
      } else{
        Z = rbind(
          Z,
          tranformation_di
        )
        
      }
    }
    
    row.names(Z) = row.names(x)
    
  }else{
    list_zi <- list()
    list_index <- list()
    list_number_dimensions = list()
    list_eigenvalues = list()
    for(i_group in 1:p){
      
      # Apply the algorithm
      cmd_eig = fast.mds(
        x = list_matrix[[i_group]],
        l = l,
        s = s,
        k = k,
        metric = metric
      )
      
      # Storing MDS and eigenvalues
      list_zi[[i_group]] = cmd_eig$points
      list_eigenvalues[[i_group]] = cmd_eig$eig

      # Take a subsample
      list_index[[i_group]] = sample(
        x = row.names( list_zi[[i_group]] ), 
        size = k * s, 
        replace = FALSE
      )
      
      ind = which( row.names( list_zi[[i_group]] ) %in% list_index[[i_group]])
      submatrix = list_matrix[[i_group]][ind, ] 
      
      
      if(i_group == 1){
        x_M_align = submatrix 
      } else{
        x_M_align = rbind(
          x_M_align,
          submatrix
        )
      }
    }
    
    
    distance_matrix_M  = daisy(
      x = x_M_align,
      metric = metric
    )
    
    M_align = stats::cmdscale(
      d = distance_matrix_M, 
      k = s
    )
    
    
    # Global alignment
    for(i_group in 1:p){
      row_names = list_index[[i_group]]
      
      ind_M = which(row.names(x_M_align) %in% row_names)
      M_align_filter =  M_align[ind_M, ]
      
      di = list_zi[[i_group]]
      ind_mds = which(row.names( di ) %in% row_names)
      di_filter = di[ind_mds, ]
      
      # Alignment
      procrustes_result =  MCMCpack::procrustes(
        X = di_filter, #The matrix to be transformed
        Xstar = M_align_filter, # target matrix
        translation = TRUE, 
        dilation = TRUE
      )
      
      rotation_matrix = procrustes_result$R
      dilation = procrustes_result$s
      translation = procrustes_result$tt
      ones_vector = rep(1, nrow(di)) 
      translation_matrix = ones_vector %*% t(translation)
      
      
      tranformation_di = dilation * di %*% rotation_matrix + translation_matrix
      
      
      # Append
      if(i_group == 1){
        Z = tranformation_di
      } else{
        Z = rbind(
          Z,
          tranformation_di
        )
        
      }
    }
    
    row.names(Z) = row.names(x)
    
  }
  
  return(
    list(
      points = Z,
      eig_normal = list_eigenvalues
    )
  )
}

\end{lstlisting}


\section{MDS based on Gower interpolation}
\begin{lstlisting}
gower.interpolation.mds <- function(
  x,
  l,
  s
){
  
  nrow_x = nrow(x)
  p = ceiling(nrow_x/l)
  if(p<1) p = 1
  
  if( p>1 ){
    # Do MDS with the first group and then use the Gower interpolation formula
    sample_distribution = sample(x = p, size = nrow_x, replace = TRUE)
    sample_distribution = sort(sample_distribution)
    
    # Get the first group 
    ind_1 = which(sample_distribution == 1)
    n_1 = length(ind_1)
    
    # Do MDS with the first group
    submatrix_data = x[ind_1, ]
    
    distance_matrix = cluster::daisy(
      x = submatrix_data,
      metric = "euclidean"
    )
    
    distance_matrix = as.matrix(distance_matrix)
    
    # MDS for the first group
    cmd_eig = stats::cmdscale(
      d = distance_matrix, 
      k = s,
      eig = TRUE
    )
    
    M = cmd_eig$points
    eigen = cmd_eig$eig/nrow(M)
    cum_mds = M
    
    # Calculations needed to do Gower interpolation
    delta_matrix = distance_matrix^2 
    In = diag(n_1)
    ones_vector = rep(1, n_1)
    J = In - 1/n_1*ones_vector %*% t(ones_vector)
    G = -1/2 * J %*% delta_matrix %*% t(J) 
    g_vector = diag(G)
    # S = cov(M)
    S = 1/(nrow(M)-1)*t(M) %*% M
    S_inv = solve(S)
    
    # For the rest of the groups, do the interpolation
    for(i_group in 2:p){
      # Filtering the data
      ind_i_group = which(sample_distribution == i_group)
      submatrix_data = x[ind_i_group, ]
      
      
      # A matrix
      distance_matrix_filter = pdist::pdist(
        X = submatrix_data,
        Y = x[ind_1, ]
      )
      
      distance_matrix_filter = as.matrix(distance_matrix_filter)
      A = distance_matrix_filter^2
      ones_vector = rep(1, length(ind_i_group))
      MDS_i_group = 1/(2*n_1)*(ones_vector %*%t(g_vector) - A) %*% M %*% S_inv
      cum_mds = rbind(
        cum_mds,
        MDS_i_group
      )
    }
  }else{
    # It is possible to run MDS directly
    distance_matrix = cluster::daisy(
      x = x,
      metric = "euclidean"
    )
    
    distance_matrix = as.matrix(distance_matrix)
    
    # MDS for the first groups
    cmd_eig = stats::cmdscale(
      d = distance_matrix, 
      k = s,
      eig = TRUE
    )
    
    cum_mds = cmd_eig$points
    eigen = cmd_eig$eig/nrow_x
  }
  
  return(
    list(
      points = cum_mds,
      eig = eigen
    )
  )
}

\end{lstlisting}
\end{document}
